{"type":"session","version":3,"id":"9f05f876-81ce-4b1a-a270-944c34247913","timestamp":"2026-02-14T00:31:07.289Z","cwd":"/job"}
{"type":"model_change","id":"fcb0d057","parentId":null,"timestamp":"2026-02-14T00:31:07.290Z","provider":"anthropic","modelId":"claude-sonnet-4-5-20250929"}
{"type":"thinking_level_change","id":"744794bb","parentId":"fcb0d057","timestamp":"2026-02-14T00:31:07.290Z","thinkingLevel":"medium"}
{"type":"message","id":"2f5e77c0","parentId":"744794bb","timestamp":"2026-02-14T00:31:07.303Z","message":{"role":"user","content":[{"type":"text","text":"\n\n# Your Job\n\nRead the WAT factory instructions in CLAUDE.md and factory/workflow.md.\n\nGenerate a PRP (Product Requirements Prompt) for a system called \"weekly-instagram-content-publisher\".\n\nProblem description:\nA weekly social media system that generates an Instagram-ready content pack (captions, hashtags, creative briefs, and optional image prompts) based on a brand voice and weekly theme, runs quality review checks, and then either publishes via the Instagram Graph API or prepares a \"ready-to-post\" pack for manual upload.\n\nInputs:\n- brand_profile (JSON): Brand voice + guardrails (tone, do/don't list, target audience, products/services, banned topics, preferred CTA, emoji style)\n- weekly_theme (string): The focus/topic for this week's posts (e.g., \"spring launch\", \"behind the scenes\", \"customer stories\")\n- post_plan (JSON): Number and types of posts to create (e.g., 3 reels concepts + 2 carousels + 1 single image), plus any required series/templates\n- reference_links (JSON array, optional): URLs to recent blog posts, announcements, or product pages to pull accurate details from\n- publishing_mode (choice): \"auto_publish\" or \"content_pack_only\"\n\nDesired Outputs:\n- output/instagram/content_pack_{date}.md: A formatted weekly pack with post-by-post details (hook, caption, CTA, hashtags, alt text, posting time)\n- output/instagram/content_pack_{date}.json: Structured data for each post\n- output/instagram/review_report_{date}.md: Quality review results (brand-voice match, compliance checks, claims verification)\n- If auto_publish: output/instagram/publish_log_{date}.json with IG media IDs or failure reasons\n- If content_pack_only: output/instagram/upload_checklist_{date}.md for manual upload\n\nIntegrations: Firecrawl (reference content), Anthropic/OpenAI (generation), Instagram Graph API (publishing), Filesystem (storage)\n\nExecution: Weekly cron (Monday 09:00 UTC), manual workflow_dispatch, Agent HQ issue-driven\n\nSpecial Requirements:\n- Two-pass process: Generate > Review > Publish\n- Quality gates: brand voice alignment, prohibited claims check, banned topics scan, hashtag hygiene, IG format compliance\n- Auto-publish gating with fallback to manual content packs\n- Rolling archive with latest.md index\n- Rate limiting and retry logic\n- Suggested subagents: content-strategist, copywriter, hashtag-specialist, reviewer-specialist\n\nFollow the PRP generation process:\n1. Read library/patterns.md and library/tool_catalog.md for reusable patterns\n2. Read config/mcp_registry.md for available MCPs\n3. Read PRPs/templates/prp_base.md for the PRP template\n4. Generate the PRP and save it to PRPs/weekly-instagram-content-publisher.md\n5. Log the confidence score and any ambiguity flags\n\nIMPORTANT: End your final commit message with exactly this line:\nCONFIDENCE_SCORE: X/10\nwhere X is the PRP's confidence score. This is used for automated pipeline decisions."}],"timestamp":1771029067297}}
{"type":"message","id":"29eba2f4","parentId":"2f5e77c0","timestamp":"2026-02-14T00:31:13.199Z","message":{"role":"assistant","content":[{"type":"thinking","thinking":"Alright, I need to generate a PRP (Product Requirements Prompt) for a weekly Instagram content publishing system. Let me break down what I need to do:\n\n1. Read the relevant library files to understand patterns and tools\n2. Read the MCP registry to see what integrations are available\n3. Read the PRP template to understand the structure\n4. Generate a comprehensive PRP following the structure\n5. Save it to the correct location\n6. Include confidence score in the commit message\n\nLet me start by reading the necessary files.","thinkingSignature":"ErIFCkYICxgCKkC27KsLM1zUBmaVFg8p/hY5MzrgbWrZlYWrDY6r1w2BMTZJhqEEKeEpB0Fh6PNFI9Z6QENSo7AT1wn+u+uFDjIZEgx7+rFsZVyAzCorrpAaDD+Kg411F1jLVRlazSIwe3js7xfkUw7Is6deZqqeGKLeyCIRC+i/YPCpsQijam6GF9kVsdFfTAMmnzxBclGfKpkE9CKNLy/ZeA5pNlZ9qjn4P0KFKAxpOxqutCR4L9o2AXPxOjZpCZC/81+5+mW2MOabCZAdwF8yahqHS1+10sk9ZVBsETJFY3g9XRXuNYUu8ggIVmVxmLKN0jnotkwgduQNsJ5mPrSzBdK8sorswXHYlePEQ0tlhmhdxOHYYgot1V6A4nnXua/2lp1Ccf+HngLMc/5gIWayO5z75kGvPxKO8sHVQAebHsGC3xDGuR9jxrD020BktXZ5i2Hs4a01dzWJFXC14ekRJJTHY/j7MF7ipOVXbA1iCG9VER7zOycXcWecs63sY6o8zUnUY5PzngLCHIqSSEBFabIQV9Sz1OVBj1LgAZbHqQINIysFxSoh5cPB9aOlIhYQ169mrlSlWAwD/phZbKdRWd9UyDflmDS7wUMqlhqxk3FiZJT8YK39ioufW7zIvgS7PadrLapWXm81q546xOmOvaGfY3y3zaNtDA4ElaP8i/LG7C1xd7XzwJLK18ejnCb5xM1ALQLxQvpejUBF2Tdf1X8R6t7phkx7Wn8dz02oEQKu/Gzcj069jbBqv8tzbZaRJEN8fO8JbD1ko16fUHAtniVbgKQdyf1011kvb1jOShq4HfY0Go96V0AFA110K+0QKh0UzN7vU8Hbl3WiMBdLN07wdxYnkdIf2E62ysMdgsM2rzL2/bFYcfWzWWJqc93eJOBpDjENI8NNUVa6c04K0KgLGAE="},{"type":"text","text":"I'll generate a comprehensive PRP for the weekly Instagram content publisher system. Let me start by reading the necessary reference files."},{"type":"toolCall","id":"toolu_01LJnKdcn29jC8SZjThtLf9V","name":"read","arguments":{"path":"library/patterns.md"}},{"type":"toolCall","id":"toolu_015TKX84qaUsgknBqmRVAeXa","name":"read","arguments":{"path":"library/tool_catalog.md"}},{"type":"toolCall","id":"toolu_01VYGpzD1oWuytyeHpm27Fqg","name":"read","arguments":{"path":"config/mcp_registry.md"}},{"type":"toolCall","id":"toolu_01UVV25vgWmnPjK6E1B8vzaa","name":"read","arguments":{"path":"PRPs/templates/prp_base.md"}}],"api":"anthropic-messages","provider":"anthropic","model":"claude-sonnet-4-5-20250929","usage":{"input":10,"output":329,"cacheRead":0,"cacheWrite":9365,"totalTokens":9704,"cost":{"input":0.00003,"output":0.004935,"cacheRead":0,"cacheWrite":0.035118750000000004,"total":0.04008375}},"stopReason":"toolUse","timestamp":1771029067299}}
{"type":"message","id":"c585d566","parentId":"29eba2f4","timestamp":"2026-02-14T00:31:13.206Z","message":{"role":"toolResult","toolCallId":"toolu_01LJnKdcn29jC8SZjThtLf9V","toolName":"read","content":[{"type":"text","text":"# WAT Systems Factory -- Workflow Pattern Catalog\n\n> Proven, reusable workflow patterns for building GitHub-native autonomous systems with Claude Code and MCP tools.\n\nEach pattern below is a composable building block. Patterns can be combined: a **Monitor > Detect > Alert** system might trigger an **Issue > Execute > PR** workflow, which internally uses **Fan-Out > Process > Merge** for the heavy lifting.\n\n---\n\n## 1. Scrape > Process > Output\n\n**Summary:** A web scraping pipeline that fetches raw content from one or more URLs via a web-access MCP, processes and structures the data with Python (or Claude itself), and writes clean output to a target destination.\n\n### When to Use\n\n- You need structured data extracted from websites that lack APIs.\n- You want to convert messy HTML into clean JSON, CSV, or Markdown on a recurring schedule.\n- You are building a dataset from publicly available web content.\n\n### Steps\n\n1. **Define targets** -- Specify URLs, selectors, or search queries in a config file or workflow input.\n2. **Fetch raw content** -- Use a web MCP (Fetch, Puppeteer, or Playwright) to retrieve page content. Handle pagination if needed.\n3. **Parse and extract** -- Strip irrelevant markup. Pull target fields using CSS selectors, XPath, or Claude-driven semantic extraction.\n4. **Transform** -- Normalize data types, deduplicate, validate schema. Python scripts or inline Claude processing.\n5. **Output** -- Write results to a file in the repo (`data/`, `output/`), post to an API, or commit as a structured artifact.\n\n### Key Tools / MCPs\n\n- **Fetch MCP** -- lightweight HTTP retrieval, returns Markdown-converted content.\n- **Puppeteer / Playwright MCP** -- for JavaScript-rendered pages requiring a headless browser.\n- **Python** -- pandas, BeautifulSoup, or custom scripts for heavy transformation.\n- **filesystem MCP** -- write results to disk.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  schedule:\n    - cron: '0 6 * * *'   # daily at 06:00 UTC\n  workflow_dispatch:        # manual trigger\n```\n\n### Example Use Cases\n\n- Scrape competitor pricing pages nightly; output a CSV diff showing changes.\n- Collect job postings from three sites; normalize into a single JSON feed committed to the repo.\n- Pull public government data tables and convert to machine-readable Parquet files.\n\n### Failure Handling\n\n- **Retry with backoff** on transient HTTP errors (429, 503).\n- **Schema validation** after extraction -- if required fields are missing, flag the record rather than silently dropping it.\n- **Stale-data guard** -- if fetch returns identical content to last run, skip processing and log \"no change.\"\n- **Alert on structural change** -- if selectors return zero matches (site redesign), open a GitHub Issue for human review.\n\n---\n\n## 2. Research > Analyze > Report\n\n**Summary:** A multi-source research workflow that gathers information from diverse sources (web, APIs, local files), synthesizes findings with Claude, and produces a formatted report committed to the repository.\n\n### When to Use\n\n- You need to compile information from multiple sources into a single coherent document.\n- Periodic market research, competitive analysis, or literature reviews.\n- Any task where a human would spend hours reading and summarizing.\n\n### Steps\n\n1. **Define research brief** -- Specify the topic, questions to answer, source priorities, and output format.\n2. **Gather sources** -- Query web search, fetch specific URLs, read local reference files, call APIs.\n3. **Extract key findings** -- For each source, pull relevant facts, quotes, data points. Tag each with provenance.\n4. **Analyze and synthesize** -- Claude cross-references findings, identifies patterns, contradictions, and gaps.\n5. **Generate report** -- Produce a structured Markdown document with sections, citations, and an executive summary.\n6. **Commit and notify** -- Commit the report to the repo. Optionally open a PR or send a notification.\n\n### Key Tools / MCPs\n\n- **Web Search MCP / Fetch MCP** -- gather online sources.\n- **filesystem MCP** -- read local reference documents.\n- **GitHub MCP** -- commit output, open issues or PRs.\n- **Claude (extended thinking)** -- deep analysis and synthesis.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  schedule:\n    - cron: '0 9 * * 1'   # weekly on Monday at 09:00 UTC\n  workflow_dispatch:\n    inputs:\n      topic:\n        description: 'Research topic'\n        required: true\n```\n\n### Example Use Cases\n\n- Weekly competitive landscape report that tracks new features announced by three competitors.\n- Monthly technology radar summarizing emerging tools relevant to the team's stack.\n- Ad-hoc deep-dive report triggered manually via `workflow_dispatch` with a topic input.\n\n### Failure Handling\n\n- **Source fallback** -- if a primary source is unreachable, log it and proceed with remaining sources. Note gaps in the report.\n- **Confidence scoring** -- Claude flags low-confidence claims; the report includes a \"Caveats\" section.\n- **Minimum-source threshold** -- if fewer than N sources are successfully retrieved, abort and open an issue rather than publishing a thin report.\n\n---\n\n## 3. Monitor > Detect > Alert\n\n**Summary:** A continuous monitoring system that periodically checks a target (website, API, dataset, repo) for changes, applies detection logic, and fires alerts or triggers downstream workflows when significant changes are found.\n\n### When to Use\n\n- You want to be notified when something changes: a webpage, an API response, a competitor's pricing, a regulatory filing.\n- You need a lightweight, Git-native alternative to commercial monitoring SaaS.\n- Ideal for cron-triggered GitHub Actions that run on a schedule.\n\n### Steps\n\n1. **Fetch current state** -- Retrieve the current snapshot of the monitored target.\n2. **Load previous state** -- Read the last-known state from a file committed in the repo (e.g., `state/last_snapshot.json`).\n3. **Diff** -- Compare current vs. previous. Use structural diff for JSON, text diff for prose, hash comparison for binary.\n4. **Evaluate significance** -- Apply rules or Claude-based judgment to determine if the change is meaningful (filter noise).\n5. **Alert** -- If significant: open a GitHub Issue, send a Slack message, trigger a downstream workflow via `repository_dispatch`, or all three.\n6. **Persist new state** -- Commit the current snapshot as the new baseline.\n\n### Key Tools / MCPs\n\n- **Fetch MCP / Puppeteer MCP** -- retrieve current state from web targets.\n- **filesystem MCP** -- read/write state files.\n- **GitHub MCP** -- open issues, trigger dispatches.\n- **Slack MCP** -- send alert messages.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  schedule:\n    - cron: '*/30 * * * *'  # every 30 minutes\n```\n\n### Example Use Cases\n\n- Monitor a government regulatory page for new filings; alert the legal team via Slack when a new document appears.\n- Watch a competitor's changelog page; open a GitHub Issue summarizing each detected update.\n- Track an API's response schema for breaking changes; alert the engineering team.\n\n### Failure Handling\n\n- **Transient failure tolerance** -- if the fetch fails, do NOT overwrite previous state. Log the failure. After N consecutive failures, escalate to an alert.\n- **False-positive suppression** -- use Claude to evaluate whether a detected \"change\" is meaningful (e.g., ignore timestamp-only diffs).\n- **State corruption recovery** -- if state file is missing or unparseable, re-initialize from scratch and log a warning instead of crashing.\n\n---\n\n## 4. Intake > Enrich > Deliver\n\n**Summary:** A data enrichment pipeline that receives raw input (from a form, webhook, file drop, or API), augments it with additional data from external sources, and delivers the enriched result to its destination.\n\n### When to Use\n\n- Incoming data is incomplete and needs augmentation before it is useful.\n- You are building a lead enrichment, content tagging, or metadata enhancement system.\n- Raw data arrives in one format and must be transformed and enriched before storage or delivery.\n\n### Steps\n\n1. **Intake** -- Receive raw data. Sources: webhook payload, new file in repo, manual trigger with JSON input, or a watched directory.\n2. **Validate** -- Check that required fields exist and data types are correct. Reject malformed input early.\n3. **Enrich** -- For each record, call enrichment sources: web search for context, APIs for supplementary data, Claude for classification or summarization.\n4. **Merge** -- Combine original data with enrichment results into a unified schema.\n5. **Deliver** -- Write enriched data to its destination: commit to repo, POST to API, append to a dataset file, or open a PR.\n\n### Key Tools / MCPs\n\n- **Web Search MCP** -- find supplementary information.\n- **Fetch MCP** -- call enrichment APIs.\n- **Claude** -- classify, tag, summarize, or score records.\n- **filesystem MCP** -- read input, write output.\n- **GitHub MCP** -- commit results or open PRs.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  push:\n    paths:\n      - 'inbox/**'          # new file dropped in inbox\n  workflow_dispatch:\n    inputs:\n      payload:\n        description: 'JSON data to enrich'\n        required: true\n```\n\n### Example Use Cases\n\n- A sales team drops a CSV of company names into `inbox/`; the pipeline enriches each with industry, size, website, and LinkedIn URL, then writes to `data/enriched_leads.csv`.\n- Incoming support tickets are enriched with customer history and sentiment score before routing.\n- Raw RSS feed items are enriched with summaries, topic tags, and relevance scores.\n\n### Failure Handling\n\n- **Per-record error isolation** -- if enrichment fails for one record, mark it as `enrichment_failed` and continue processing the rest. Do not let one bad record kill the batch.\n- **Rate-limit awareness** -- track API call counts; pause and retry when rate-limited rather than failing fast.\n- **Partial delivery** -- deliver successfully enriched records on schedule; queue failed records for retry in the next run.\n\n---\n\n## 5. Generate > Review > Publish\n\n**Summary:** A content generation pipeline with built-in quality gates. Claude generates draft content, a review step validates quality and compliance, and approved content is published to its destination.\n\n### When to Use\n\n- Automated content creation (blog posts, documentation, social media, release notes) where quality must be verified before publishing.\n- Any workflow where you want a \"human-in-the-loop\" approval step or an automated quality check before output goes live.\n\n### Steps\n\n1. **Brief** -- Define the content requirements: topic, audience, tone, length, format, and any reference material.\n2. **Generate** -- Claude produces a first draft using the brief and any provided reference material.\n3. **Automated review** -- Run quality checks: spelling/grammar, factual consistency against source material, tone analysis, compliance rules (e.g., no prohibited claims).\n4. **Gate decision** -- If automated review passes, proceed. If it fails, loop back to step 2 with feedback (max N retries). Optionally, require human approval via a PR review.\n5. **Publish** -- Commit final content to the repo, deploy to a CMS via API, or post to a platform.\n\n### Key Tools / MCPs\n\n- **Claude** -- generation and review (use separate system prompts for generator vs. reviewer to create genuine separation).\n- **filesystem MCP** -- read briefs, write drafts and final output.\n- **GitHub MCP** -- open a draft PR for human review; merge on approval.\n- **Fetch MCP** -- publish to external CMS or platform APIs.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  issues:\n    types: [labeled]        # label \"content-request\" triggers generation\n  pull_request_review:\n    types: [submitted]      # human approval triggers publish\n  workflow_dispatch:\n```\n\n### Example Use Cases\n\n- Weekly changelog: gather merged PRs, generate a formatted changelog, open a PR for team review, publish on approval.\n- Blog post pipeline: issue labeled \"draft-request\" triggers generation; editor reviews the PR; merge triggers deployment.\n- Social media batch: generate a week's worth of posts, run brand-voice review, output approved posts to a scheduling queue.\n\n### Failure Handling\n\n- **Retry budget** -- if automated review rejects the draft, regenerate with targeted feedback up to 3 times. After that, escalate to a human by opening an issue.\n- **Rollback on publish failure** -- if the publish step fails (API error), keep the approved content in the repo and log the failure for manual retry.\n- **Diff guard** -- before publishing, diff against previously published content to prevent accidental overwrite of manual edits.\n\n---\n\n## 6. Listen > Decide > Act\n\n**Summary:** An event-driven agent that listens for incoming events (webhooks, GitHub events, messages), applies decision logic to determine the appropriate response, and takes autonomous action.\n\n### When to Use\n\n- You need a system that reacts to external events in near-real-time.\n- Webhook-triggered GitHub Actions are the execution engine.\n- The appropriate response depends on the content of the event (routing, triage, conditional logic).\n\n### Steps\n\n1. **Receive event** -- GitHub Actions triggers on the event (webhook, issue creation, PR comment, `repository_dispatch`).\n2. **Parse and classify** -- Extract relevant data from the event payload. Use Claude to classify intent or severity if the input is unstructured (e.g., natural language).\n3. **Decide** -- Apply decision logic: rule-based routing, Claude-driven triage, or a lookup table. Determine which action(s) to take.\n4. **Act** -- Execute the chosen action: respond to the issue, trigger another workflow, call an API, update a file, or escalate to a human.\n5. **Log** -- Record the event, decision, and action taken for auditability.\n\n### Key Tools / MCPs\n\n- **GitHub MCP** -- read event payloads, respond to issues/PRs, trigger dispatches.\n- **Claude** -- classify, triage, and decide.\n- **Slack MCP** -- notify humans when escalation is needed.\n- **Fetch MCP** -- call external APIs as part of the action.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  issues:\n    types: [opened, labeled]\n  issue_comment:\n    types: [created]\n  repository_dispatch:\n    types: [incoming_webhook]\n```\n\n### Example Use Cases\n\n- Triage bot: new issues are classified by Claude (bug, feature, question) and auto-labeled, assigned, and replied to with relevant templates.\n- Deployment gatekeeper: a PR comment like \"/deploy staging\" triggers a deployment workflow after validating the commenter's permissions.\n- Customer feedback router: webhook delivers feedback; Claude scores sentiment and routes critical items to Slack.\n\n### Failure Handling\n\n- **Idempotency** -- design actions to be safe to retry. Use event IDs to detect and skip duplicate deliveries.\n- **Decision logging** -- log the full decision chain (event > classification > decision > action) so failures can be diagnosed.\n- **Safe defaults** -- if classification confidence is low, default to escalation (notify a human) rather than taking an autonomous action that might be wrong.\n\n---\n\n## 7. Collect > Transform > Store\n\n**Summary:** A classic ETL (Extract, Transform, Load) pipeline where Git serves as the data store. Data is collected from external sources, transformed into a canonical format, and committed to the repository as versioned, queryable files.\n\n### When to Use\n\n- You want a version-controlled data lake without standing up a database.\n- Small-to-medium datasets (tens of MB) that benefit from Git's diff, history, and branching.\n- Data pipelines where auditability and reproducibility matter more than query speed.\n\n### Steps\n\n1. **Collect** -- Pull data from source(s): APIs, web scrapes, file uploads, other repos.\n2. **Validate** -- Check data integrity: schema validation, null checks, type enforcement.\n3. **Transform** -- Normalize, clean, reshape. Convert to the canonical schema. Deduplicate against existing data in the store.\n4. **Diff** -- Compare transformed data against the current version in the repo. Compute what is new, changed, or deleted.\n5. **Store** -- Commit the updated dataset files to the repo. Use meaningful commit messages describing what changed (e.g., \"Add 14 new records from source X\").\n\n### Key Tools / MCPs\n\n- **Fetch MCP** -- pull data from APIs and web sources.\n- **Python** -- pandas or custom scripts for transformation.\n- **filesystem MCP** -- read/write data files.\n- **GitHub MCP / Git CLI** -- commit, push, branch as needed.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  schedule:\n    - cron: '0 */6 * * *'  # every 6 hours\n  workflow_dispatch:\n```\n\n### Example Use Cases\n\n- Collect cryptocurrency prices from three APIs every 6 hours; store as daily CSV files in `data/prices/`.\n- Pull open government datasets weekly; transform into a normalized schema; maintain a living dataset in the repo.\n- Aggregate internal metrics from multiple microservice health endpoints; store as JSON for a dashboard.\n\n### Failure Handling\n\n- **Atomic commits** -- only commit if the entire pipeline succeeds. Never commit partial or corrupted data.\n- **Schema drift detection** -- if source data no longer matches the expected schema, abort and alert rather than storing malformed data.\n- **Rollback via Git** -- if bad data is committed, revert the commit. The previous state is always recoverable.\n- **Dedup safety** -- idempotent writes ensure that re-running the pipeline does not create duplicate records.\n\n---\n\n## 8. Fan-Out > Process > Merge\n\n**Summary:** The Agent Teams pattern. A coordinator agent splits a large task into independent sub-tasks, dispatches them to parallel sub-agents (or sequential iterations), collects all results, and merges them into a unified output.\n\n### When to Use\n\n- A task is naturally decomposable into independent units of work (e.g., process each file, research each topic, analyze each competitor).\n- You want to reduce wall-clock time by parallelizing work.\n- The final deliverable requires combining results from multiple sub-tasks.\n\n### Steps\n\n1. **Plan** -- The coordinator analyzes the task and determines how to decompose it. Produces a work manifest: a list of sub-tasks with their inputs.\n2. **Fan-Out** -- Dispatch each sub-task. Options: parallel GitHub Actions jobs via a matrix strategy, sequential Claude tool calls, or `repository_dispatch` events to separate workflows.\n3. **Process** -- Each sub-agent executes its sub-task independently. Results are written to a known location (e.g., `temp/results/task_01.json`).\n4. **Collect** -- The coordinator waits for all sub-tasks to complete and gathers their outputs.\n5. **Merge** -- Combine results: concatenate, deduplicate, reconcile conflicts, and produce the final unified output.\n6. **Deliver** -- Commit the merged output or pass it to the next stage in a larger pipeline.\n\n### Key Tools / MCPs\n\n- **Claude** -- coordinator logic, sub-task execution, merge logic.\n- **GitHub Actions matrix** -- parallel job execution.\n- **filesystem MCP** -- read/write intermediate and final results.\n- **GitHub MCP** -- coordinate via issues, artifacts, or dispatch events.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  workflow_dispatch:\n    inputs:\n      task_manifest:\n        description: 'JSON array of sub-tasks'\n        required: true\n  repository_dispatch:\n    types: [fan_out_request]\n```\n\n### Example Use Cases\n\n- Analyze 20 competitor websites: fan out one sub-agent per competitor, each produces a profile, merge into a single competitive analysis report.\n- Process a batch of 50 documents: each sub-agent summarizes one document, merge into a master summary with cross-references.\n- Multi-language translation: fan out one sub-agent per target language, merge all translations into a localized content bundle.\n\n### Failure Handling\n\n- **Partial success tolerance** -- if some sub-tasks fail, merge the successful results and report failures. Do not discard all work because of one failure.\n- **Retry failed sub-tasks** -- the coordinator can re-dispatch failed sub-tasks up to N times before giving up.\n- **Timeout enforcement** -- set a maximum wall-clock time for the fan-out phase. Collect whatever is done when the timer expires.\n- **Conflict resolution** -- if sub-task results conflict, the merge step uses Claude to reconcile or flags the conflict for human review.\n\n---\n\n## 9. Issue > Execute > PR\n\n**Summary:** The GitHub Agent HQ pattern. An agent receives a task specification via a GitHub Issue, autonomously executes the required workflow, and delivers the results as a draft Pull Request for human review.\n\n### When to Use\n\n- You want a human-in-the-loop task queue where work is requested via Issues and delivered via PRs.\n- The agent should be fully autonomous in execution but the output requires human approval before merging.\n- Building an internal tool where non-technical stakeholders can request work by opening an Issue.\n\n### Steps\n\n1. **Receive** -- A new Issue is opened (or an existing Issue is labeled with a trigger label like `agent-task`). GitHub Actions fires.\n2. **Parse task** -- Claude reads the Issue body and extracts the task specification: what to do, inputs, constraints, expected output.\n3. **Create branch** -- Create a feature branch named after the Issue (e.g., `agent/issue-42`).\n4. **Execute** -- Run the appropriate workflow or pattern to complete the task. This might invoke any other pattern in this catalog.\n5. **Commit results** -- Commit all output files to the feature branch with clear commit messages.\n6. **Open draft PR** -- Open a draft PR linked to the original Issue. The PR body includes a summary of what was done, key decisions made, and anything that needs human attention.\n7. **Comment on Issue** -- Post a comment on the Issue linking to the PR and summarizing the outcome.\n\n### Key Tools / MCPs\n\n- **GitHub MCP** -- read issues, create branches, commit, open PRs, post comments.\n- **Claude** -- parse task, execute logic, generate summaries.\n- **Any other MCP** -- depending on the task being executed.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  issues:\n    types: [opened, labeled]\n  issue_comment:\n    types: [created]       # support \"/execute\" command in comments\n```\n\n### Example Use Cases\n\n- A product manager opens an Issue: \"Research top 5 CRM tools and compare features.\" The agent researches, builds a comparison table, and opens a PR with the report.\n- An engineer opens an Issue: \"Add unit tests for the auth module.\" The agent generates tests and delivers them as a PR.\n- A stakeholder opens an Issue: \"Update the pricing page data for Q1.\" The agent pulls new data, updates the files, and opens a PR for review.\n\n### Failure Handling\n\n- **Status updates** -- the agent comments on the Issue at each major step (\"Starting research...\", \"Generating report...\", \"Opening PR...\") so progress is visible.\n- **Graceful failure** -- if the agent cannot complete the task, it comments on the Issue with a clear explanation of what went wrong and what it tried, then labels the Issue `agent-failed` for human triage.\n- **Scope guard** -- if Claude determines the task is ambiguous or out of scope, it asks a clarifying question in the Issue comments rather than guessing.\n- **Draft-only PRs** -- all PRs are opened as drafts. The agent never merges its own work.\n\n---\n\n## 10. n8n Import > Translate > Deploy\n\n**Summary:** A conversion pattern that ingests an n8n workflow JSON export, translates it into a WAT-native system (Claude Code + MCP + GitHub Actions), and deploys it as a fully functional GitHub-native workflow.\n\n### When to Use\n\n- You are migrating from n8n (or a similar visual workflow tool) to a GitHub-native architecture.\n- You want to leverage existing workflow logic without manually rewriting it.\n- You are evaluating whether an n8n workflow can run as a WAT system to reduce infrastructure dependencies.\n\n### Steps\n\n1. **Import** -- Read the n8n workflow JSON file. This contains nodes, connections, credentials references, and configuration.\n2. **Parse and map** -- Walk the node graph. For each n8n node type, identify the equivalent WAT component:\n   - HTTP Request node -> Fetch MCP call\n   - Code (JavaScript/Python) node -> inline script or Python file\n   - IF/Switch node -> Claude decision logic or conditional in workflow\n   - Webhook trigger -> `repository_dispatch` or `workflow_dispatch`\n   - Cron trigger -> `schedule` trigger in GitHub Actions\n   - Set/Merge nodes -> data transformation in Claude or Python\n3. **Translate** -- Generate the WAT system files:\n   - `.github/workflows/*.yml` -- GitHub Actions workflow definition.\n   - `prompts/` -- Claude system prompts for nodes that require AI logic.\n   - `scripts/` -- Python or shell scripts for code nodes.\n   - `config/` -- Configuration extracted from the n8n workflow.\n4. **Validate** -- Dry-run the generated workflow. Check that all node types were mapped, no credentials are hardcoded, and the control flow is correct.\n5. **Deploy** -- Commit the generated files to the repo. Open a PR with a detailed summary of the translation, noting any nodes that required manual review or could not be auto-translated.\n\n### Key Tools / MCPs\n\n- **filesystem MCP** -- read n8n JSON, write generated files.\n- **Claude** -- interpret node logic, generate prompts, translate code, produce workflow YAML.\n- **GitHub MCP** -- commit files, open PR.\n- **Python** -- parse n8n JSON, generate boilerplate.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  push:\n    paths:\n      - 'imports/n8n/*.json'   # drop an n8n export to trigger conversion\n  workflow_dispatch:\n    inputs:\n      n8n_file:\n        description: 'Path to n8n JSON file in the repo'\n        required: true\n```\n\n### Example Use Cases\n\n- Migrate a 15-node n8n lead-scoring workflow to a Git-versioned, GitHub Actions-powered system.\n- Convert an n8n social media posting pipeline into a WAT **Generate > Review > Publish** system.\n- Translate an n8n monitoring workflow into a WAT **Monitor > Detect > Alert** pattern, eliminating the need for a running n8n instance.\n\n### Failure Handling\n\n- **Unsupported node catalog** -- maintain a list of n8n node types that cannot be auto-translated. When encountered, flag them in the PR description with a `TODO` and a suggested manual implementation.\n- **Credential safety** -- never translate credentials. Replace all credential references with GitHub Secrets placeholders (`${{ secrets.SERVICE_API_KEY }}`) and document which secrets need to be configured.\n- **Fidelity check** -- generate a side-by-side comparison of the n8n flow and the translated WAT flow so a human reviewer can verify correctness.\n- **Incremental migration** -- support translating a subset of the n8n workflow, allowing teams to migrate node-by-node rather than all-at-once.\n\n---\n\n## Pattern Composition Quick Reference\n\nPatterns are building blocks. Here are common compositions:\n\n| Composition | How It Works |\n|---|---|\n| **Monitor > Detect > Alert** + **Issue > Execute > PR** | Monitor detects a change, opens an Issue, which triggers the agent to act and deliver a PR. |\n| **Issue > Execute > PR** using **Fan-Out > Process > Merge** | A complex Issue task is decomposed and parallelized internally before the PR is opened. |\n| **Scrape > Process > Output** feeding **Collect > Transform > Store** | Scraping output is piped into the ETL pattern for versioned storage. |\n| **Listen > Decide > Act** routing to any pattern | An event-driven dispatcher that selects and triggers the appropriate pattern based on the event. |\n| **n8n Import > Translate > Deploy** producing any pattern | The translator identifies which WAT pattern best fits the n8n workflow and generates accordingly. |\n\n---\n\n## Choosing a Pattern\n\n| Question | Recommended Pattern |\n|---|---|\n| Do I need to pull data from websites? | **Scrape > Process > Output** |\n| Do I need to synthesize information from many sources? | **Research > Analyze > Report** |\n| Do I need to watch for changes over time? | **Monitor > Detect > Alert** |\n| Do I need to augment incoming data? | **Intake > Enrich > Deliver** |\n| Do I need AI-generated content with quality control? | **Generate > Review > Publish** |\n| Do I need to react to external events? | **Listen > Decide > Act** |\n| Do I need versioned data storage without a database? | **Collect > Transform > Store** |\n| Do I need to parallelize a large task? | **Fan-Out > Process > Merge** |\n| Do I need a task queue with human approval? | **Issue > Execute > PR** |\n| Do I need to migrate an n8n workflow? | **n8n Import > Translate > Deploy** |\n| Do I need B2B lead generation from web data? | **Intake > Enrich > Deliver** (lead-gen variant) |\n\n---\n\n## Proven Compositions (from builds)\n\n### Lead Generation Pipeline (lead-gen-machine)\n- **Pattern**: Intake > Enrich > Deliver + Scrape > Process > Output\n- **Flow**: Parse profile > Search web > Scrape sites > Extract contacts (Claude) > Score leads > Output CSV\n- **Key insight**: The Intake > Enrich > Deliver pattern works well when the \"enrichment\" involves multi-step web scraping + AI extraction. Each company is an independent unit of work, making this a strong candidate for Fan-Out > Process > Merge parallelization.\n- **Scoring pattern**: Decompose match criteria into independent dimensions (industry, size, location, keywords) with weighted point values. This makes the scoring transparent and tunable.\n- **Fallback chains**: Firecrawl > HTTP+BeautifulSoup for scraping, Claude > regex for extraction. Always have a degraded-but-functional fallback.\n\n### Marketing Pipeline (marketing-pipeline)\n- **Pattern**: Intake > Enrich > Deliver + Generate > Review > Publish\n- **Flow**: Ingest CSV > Enrich via Apollo/Hunter/Brave/Claude > Deep-score (5 dimensions) > Segment (Hot/Warm/Cold) > Generate outreach (Hot) > Generate nurture (Warm) > Output CSVs + emails\n- **Key insight**: Multi-API enrichment with graceful degradation. Apollo (primary) > Hunter (email verification) > Brave (research) > Claude (pain analysis). Each API layer adds value but the system works with only Anthropic API.\n- **Scoring pattern**: 5-dimension scoring (size fit, tech stack compatibility, budget signals, accessibility, pain signals) with transparent point breakdown. More dimensions = more nuanced than lead-gen-machine's 4-dimension approach.\n- **Content generation pattern**: Outreach and nurture are independent post-segmentation, making them natural Agent Teams candidates. Outreach is per-lead personalized; nurture is aggregate-informed.\n- **API fallback chain**: Apollo > Hunter > Brave > Claude > HTTP scraping. Four-level fallback with clear degradation at each level.\n- **CSV-in/CSV-out**: Accepts CSV input (compatible with lead-gen-machine output), produces segmented CSVs + markdown email sequences. Easy to chain systems.\n\n### Website Uptime Monitor (website-uptime-monitor)\n- **Pattern**: Monitor > Log (simplified from Monitor > Detect > Alert)\n- **Flow**: Check URL via HTTP GET > Measure response time > Determine up/down status > Append to CSV > Commit to Git\n- **Key insight**: Simplest possible monitoring system - no detection logic, no alerting, just raw data collection. Every check produces a log entry regardless of status. Git history IS the audit trail.\n- **No MCPs needed**: Direct Python `requests` library is simpler and more reliable than Fetch MCP for a single GET request. CSV append uses stdlib. Zero external dependencies beyond GitHub Actions.\n- **Exit code signaling**: Tool exits 0 if up, 1 if down - GitHub Actions UI shows workflow as \"failed\" when site is down, providing at-a-glance status without additional alerting logic.\n- **Concurrency handling**: GitHub Actions `concurrency` setting prevents simultaneous runs. Git push includes retry with rebase to handle rare concurrent commits.\n- **CSV as database**: Append-only CSV committed to Git provides versioned, auditable, queryable dataset. No external database needed. Git handles MB-scale data easily.\n- **Cron variance accepted**: GitHub Actions cron has ±5 minute variance - documented as expected behavior rather than a bug to fix.\n- **Three execution paths**: (1) Scheduled cron (primary), (2) Manual dispatch (testing), (3) Local CLI (development). Agent HQ (4) optional for issue-driven tasks.\n- **Cost awareness**: ~1,440 GitHub Actions minutes/month for 5-minute checks. Fits within free tier (2,000/month private, unlimited public).\n- **Extensibility points**: Add alerting by extending monitor.yml (Slack webhook, GitHub Issue). Add multiple URLs via matrix strategy or separate workflows. Add authentication via tool arguments + GitHub Secrets.\n\n### CSV-to-JSON Converter (csv-to-json-converter)\n- **Pattern**: Collect > Transform > Store\n- **Flow**: Collect CSV files > Analyze structure > Parse CSV > Infer types > Validate data > Convert to JSON/JSONL > Generate reports > Commit\n- **Key insight**: Multi-phase transformation with specialist subagents. Each phase (analyze, infer, validate, write) has a dedicated subagent with focused responsibilities. Main agent orchestrates the pipeline.\n- **Subagent architecture**: Four specialist subagents (csv-analyzer, type-inference-specialist, data-validator, json-writer) with minimal tool access. Subagents are the DEFAULT delegation mechanism, not Agent Teams.\n- **Type inference pattern**: Statistical analysis with confidence scoring (80% threshold). Detects int, float, boolean, datetime, string. Falls back to string for ambiguous columns. Handles 10+ null value representations.\n- **Encoding detection**: chardet for automatic encoding detection with UTF-8 fallback. Strips BOM (byte order mark) automatically. Supports UTF-8, Latin-1, Windows-1252, etc.\n- **Ragged row handling**: Pad short rows with null, truncate long rows with warning. Log all issues in validation report but continue processing (graceful degradation).\n- **Validation without failure**: Validator NEVER raises exceptions. Always returns a report. Strict mode controls whether issues halt the pipeline, but validation itself is fail-safe.\n- **JSONL streaming**: For large files (1M+ rows), JSONL format writes one record at a time. No array wrapper. Memory-efficient for multi-GB datasets.\n- **Batch processing modes**: Sequential (1-2 files) or parallel (3+ files with Agent Teams). Parallel is 5x faster wall time but same token cost. Always has sequential fallback.\n- **Three execution paths**: (1) CLI for local development, (2) GitHub Actions for production, (3) Agent HQ for issue-driven tasks. All three produce identical output.\n- **Metadata reporting**: run_summary.json (machine-readable) + validation_report.md (human-readable). Includes per-file stats, type inference results, validation issues, recommendations.\n- **No secrets required**: Uses only stdlib (csv, json, pathlib) + chardet + python-dateutil. No external APIs. Pure data transformation pipeline.\n- **Git commit discipline**: Stage ONLY output files (never `git add -A`). Commit message includes file count, row count, timestamp. Output directory is parameterized.\n\n### RSS Digest Monitor (rss-digest-monitor)\n- **Pattern**: Monitor > Collect > Transform > Deliver (with state persistence)\n- **Flow**: Load state > Fetch RSS feeds > Filter new posts > Generate HTML digest > Send email > Update state > Commit\n- **Key insight**: State persistence in Git prevents post duplication across runs. Composite GUID keys (feed_url::guid) prevent false deduplication. State is ONLY updated after successful email delivery, ensuring retry on failure.\n- **Subagent architecture**: Three specialist subagents (rss-fetcher-specialist, digest-generator-specialist, state-manager-specialist). Subagents are the DEFAULT delegation mechanism. Sequential execution is correct due to clear data dependencies.\n- **Per-feed error isolation**: One failed feed does NOT kill the entire run. Each feed is fetched independently with try/except. Failed feeds are logged and skipped. Graceful degradation ensures partial success.\n- **Composite GUID pattern**: Use `feed_url::entry_guid` as deduplication key instead of guid alone. Prevents false positives when different feeds use the same GUID format.\n- **State file size management**: Enforce maximum 10,000 GUIDs by keeping most recent entries. Prevents unbounded state growth. Log warning if limit is hit.\n- **Email with fallback**: MIME multipart with HTML primary and plain-text fallback. Ensures compatibility with all email clients. HTML is styled with inline CSS for maximum rendering support.\n- **Conditional email send**: Skip email generation if no new posts found. Silent run with state update only. Saves API calls and prevents empty emails.\n- **SMTP retry logic**: Retry once with 30s delay on transient failures (auth, network, timeout). Do NOT update state if email send fails -- posts retry on next run.\n- **First-run initialization**: Missing or corrupted state file initializes empty state. All posts are processed on first run. Log warning but continue (better to duplicate once than crash).\n- **Date parsing with fallback**: Use python-dateutil for robust parsing of varied date formats. If unparseable, use current timestamp. Include post anyway (better to duplicate than miss).\n- **Three execution paths**: (1) Scheduled cron at 8 AM UTC (primary), (2) Manual dispatch with force_send (testing), (3) Local CLI (development). All produce identical output.\n- **Run summary logging**: Each run produces `logs/YYYY-MM-DD_run.json` with feeds_checked, feeds_failed, new_posts, email_sent, failed_feeds array. Machine-readable audit trail.\n- **Git commit discipline**: Stage ONLY state/rss_state.json and logs/*.json. NEVER `git add -A`. Commit message includes date and post count for easy history review.\n- **No MCP dependencies**: Uses standard Python libraries (feedparser, smtplib, json, pathlib). Works out-of-the-box with only Python and requirements.txt. No external MCPs required.\n- **Cost awareness**: ~3-5 minutes per day = ~90-150 GitHub Actions minutes/month. Well within free tier (2,000/month private, unlimited public). Minimal token cost for state management only.\n\n## 11. Content Transformation with Tone Matching\n\n**Summary:** An AI-powered content repurposing pipeline that extracts content from a source URL, analyzes its tone and style, and generates platform-optimized variants (social media, email, etc.) that match the source tone automatically.\n\n### When to Use\n\n- You need to publish the same content across multiple platforms with different format requirements\n- Content must maintain consistent voice/tone across all platforms\n- Manual reformatting for each platform is time-consuming and error-prone\n- You want AI to handle platform-specific best practices (character limits, hashtags, formatting)\n\n### Steps\n\n1. **Fetch source content** -- Scrape the source URL (blog post, article) and extract clean text/markdown. Use Firecrawl with HTTP fallback for reliability.\n2. **Analyze tone** -- Use LLM (Claude) with structured output to analyze writing style across multiple dimensions: formality, technical level, humor, primary emotion. Return confidence score.\n3. **Generate platform variants (parallel)** -- For each target platform (Twitter, LinkedIn, email, Instagram):\n   - Send source content + tone analysis to LLM\n   - Request platform-optimized output matching source tone\n   - Enforce platform-specific constraints (char limits, hashtag counts, formatting rules)\n   - Validate character counts before accepting output\n4. **Merge and output** -- Combine all variants into single JSON file with metadata, tone analysis, and all platform content\n\n### Key Tools / MCPs\n\n- **Firecrawl MCP** -- reliable web scraping with JS rendering support\n- **Anthropic MCP** -- Claude for tone analysis and content generation\n- **HTTP + BeautifulSoup** -- fallback scraping if Firecrawl unavailable\n- **Python stdlib** -- JSON manipulation, filename slugification, output assembly\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  workflow_dispatch:\n    inputs:\n      blog_url:\n        description: 'Blog post URL to repurpose'\n        required: true\n  # Can also be triggered on new blog post detection via RSS or webhook\n```\n\n### Example Use Cases\n\n- Marketing team publishes a blog post, wants Twitter thread, LinkedIn post, email newsletter section, and Instagram caption\n- Content creator wants to maximize reach by publishing same content on all major platforms\n- Agency needs to repurpose client blog posts for multi-channel campaigns\n- Developer advocates want to turn technical blog posts into social-friendly content\n\n### Subagent Architecture\n\n- **content-scraper-specialist** -- Handles web scraping with fallback strategies\n- **tone-analyzer-specialist** -- Analyzes source content tone/style\n- **content-generator-specialist** -- Coordinates multi-platform generation (can use Agent Teams for parallelization)\n- **output-assembler-specialist** -- Merges all content into final JSON\n\n### Agent Teams Parallelization\n\nPlatform generation is ideal for Agent Teams:\n- 4 independent platform generation tasks (Twitter, LinkedIn, email, Instagram)\n- Each takes 10-15 seconds sequentially = 40-55s total\n- Parallel execution with 4 teammates = 12-18s\n- **2x speedup** with identical results and same token cost\n\n### Platform-Specific Constraints\n\n| Platform | Char Limit | Hashtags | Format Notes |\n|----------|------------|----------|--------------|\n| Twitter | 280 per tweet | 2-3 per thread | Numbered thread (1/N format), hook in first tweet, CTA in last |\n| LinkedIn | 3000 max, 1300 target | 3-5 | Professional tone, hook before \"see more\" fold, line breaks |\n| Email | 500-800 words | N/A | Subject line (40-60 chars), scannable format, HTML + plain text |\n| Instagram | 2200 max | 10-15 | Casual tone, emojis (3-5), line breaks, hook before \"more\" button |\n\n### Tone Matching Pattern\n\n1. LLM analyzes source content with structured JSON schema\n2. Returns tone profile: {formality, technical_level, humor_level, primary_emotion, confidence}\n3. Platform generators receive tone profile in their prompts\n4. Each generator adjusts output style to match source tone (formal → formal, casual → casual)\n5. Result: All platforms sound like they were written by the same person\n\n### Failure Handling\n\n- **Scraping failure** -- Halt workflow immediately (no content = can't proceed). Log clear error about paywall/404/timeout.\n- **Tone analysis failure** -- Return default neutral profile, continue with warning. Quality slightly reduced but system remains functional.\n- **Single platform failure** -- Continue with other platforms. Include error structure for failed platform in output. Partial success is acceptable.\n- **Character limit exceeded** -- Tools automatically truncate with \"...\" and log warning. Better than crashing.\n\n### Performance & Cost\n\n**Execution time**:\n- Sequential: ~52-74 seconds\n- With Agent Teams: ~25-37 seconds (2x faster)\n\n**API cost per run** (Claude Sonnet 4):\n- Scraping: $0.01-0.02 (Firecrawl)\n- LLM calls: ~$0.09 (tone + 4 platforms)\n- **Total: ~$0.10-0.11 per blog post**\n\n### Success Criteria\n\n✅ Source content extracted (≥ 100 chars)\n✅ Tone analysis returns confidence ≥ 0.5\n✅ At least 2/4 platforms generate successfully\n✅ All character counts within platform limits\n✅ Output JSON file written to repo\n\n### Key Learnings\n\n- **Tone matching is powerful** -- AI can accurately replicate writing style when given structured tone analysis\n- **Parallel generation scales well** -- 4 independent LLM calls are perfect for Agent Teams\n- **Character validation is critical** -- Must validate before returning, not after\n- **Partial success is acceptable** -- 3/4 platforms working is better than all-or-nothing\n- **Firecrawl + fallback pattern works** -- Primary API with HTTP backup maximizes reliability\n- **Subagent specialization reduces complexity** -- Each subagent has clear, scoped responsibility\n\n### Related Patterns\n\nCombines elements of:\n- **Scrape > Process > Output** (content extraction)\n- **Fan-Out > Process > Merge** (parallel platform generation via Agent Teams)\n- **Generate > Review > Publish** (content creation with validation)\n\n## 12. Sequential State Management Pipeline\n\n**Summary:** A data transformation pipeline that maintains persistent state across runs using atomic file operations. Each execution reads current state, transforms data, generates output, updates state, and commits results -- all while preventing race conditions.\n\n### When to Use\n\n- You need sequential numbering or versioning that persists across workflow runs\n- Output depends on previous runs (counters, IDs, sequence numbers)\n- Concurrent execution must not create duplicates\n- State must survive container/server restarts\n- Git-native state storage is preferred over external databases\n\n### Steps\n\n1. **Read State** -- Load previous state from committed file\n2. **Lock State** -- Acquire file lock to prevent concurrent modifications\n3. **Transform Data** -- Process input using current state\n4. **Update State** -- Increment counter, add ID, update version\n5. **Generate Output** -- Produce artifacts using updated state\n6. **Commit State** -- Write state file and outputs to repo\n7. **Release Lock** -- Allow next execution to proceed\n\n### Key Tools / MCPs\n\n- **filelock** (Python) -- Atomic file locking with timeout\n- **Git** -- State file versioning and recovery\n- **filesystem MCP** -- Read/write state files\n- **JSON** (stdlib) -- State serialization\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  workflow_dispatch:\n    inputs:\n      data:\n        description: 'Input data to process'\n        required: true\n  push:\n    paths:\n      - 'input/**'\n```\n\n### Example Use Cases\n\n- Invoice generator with sequential invoice numbers (INV-1001, INV-1002...)\n- Document versioning system (v1.0, v1.1, v2.0...)\n- Sequential ID assignment for records\n- Incrementing batch job numbers\n- Auto-incrementing PR numbers in custom systems\n\n### State File Format\n\n```json\n{\n  \"last_value\": 1042,\n  \"prefix\": \"INV-\",\n  \"padding\": 4,\n  \"metadata\": {\n    \"last_updated\": \"2026-02-11T14:30:00Z\",\n    \"total_count\": 1042\n  }\n}\n```\n\n### Atomic Operations Pattern\n\n```python\nfrom filelock import FileLock, Timeout\n\nlock_path = f\"{state_file}.lock\"\ntry:\n    with FileLock(lock_path, timeout=5):\n        # Read current state\n        state = json.loads(Path(state_file).read_text())\n        \n        # Modify state\n        state[\"last_value\"] += 1\n        \n        # Write updated state\n        Path(state_file).write_text(json.dumps(state, indent=2))\n        \n        # Return new value\n        return state[\"last_value\"]\nexcept Timeout:\n    # Fallback: use timestamp or other unique value\n    return generate_fallback_value()\n```\n\n### Failure Handling\n\n**Lock timeout (5 seconds)**\n- **Cause**: Concurrent execution or filesystem issue\n- **Action**: Use fallback mechanism (timestamp, UUID, or queue for retry)\n- **Log**: Warning with details for investigation\n- **Continue**: Workflow proceeds with fallback value\n\n**State file missing**\n- **Cause**: First run or file deleted\n- **Action**: Initialize to default starting value\n- **Log**: Info message about initialization\n- **Continue**: Normal operation from initial state\n\n**State file corrupted**\n- **Cause**: Incomplete write, manual edit, merge conflict\n- **Action**: Restore from Git history or reinitialize\n- **Log**: Error with corrupted content\n- **Halt**: Require manual intervention (prevents data loss)\n\n**Concurrent modifications**\n- **Prevented by**: File locking ensures only one process modifies at a time\n- **If lock fails**: Fallback prevents workflow failure\n- **Trade-off**: Fallback may break sequential ordering but ensures system continues\n\n### Git-Native State Management\n\nState files are committed to the repository:\n\n**Benefits:**\n- Version history (full audit trail)\n- Rollback capability (`git checkout HEAD~5 state.json`)\n- Merge conflict detection\n- No external database needed\n- Free hosting on GitHub\n\n**Considerations:**\n- Not suitable for high-frequency updates (>100/min)\n- Commit overhead (~1-2 seconds per update)\n- Repo size impact (minimal for small JSON files)\n\n### Fallback Strategies\n\nWhen atomic state update fails, use one of these fallbacks:\n\n| Strategy | When to Use | Trade-off |\n|----------|-------------|-----------|\n| **Timestamp** | Unique value needed, order not critical | Breaks sequential numbering |\n| **UUID** | Unique value needed, no pattern required | No human-readable sequence |\n| **Queue for retry** | Sequential order is critical | Delays processing |\n| **Range allocation** | High concurrency expected | More complex state management |\n\n### Integration Points\n\n**GitHub Actions:**\n```yaml\nsteps:\n  - name: Get next value\n    id: state\n    run: |\n      python tools/manage_state.py state.json get_next > output.json\n      VALUE=$(jq -r '.value' output.json)\n      echo \"VALUE=$VALUE\" >> $GITHUB_OUTPUT\n  \n  - name: Use value\n    run: |\n      process_data.py --id ${{ steps.state.outputs.VALUE }}\n  \n  - name: Commit state\n    run: |\n      git add state.json output/*\n      git commit -m \"Process ID ${{ steps.state.outputs.VALUE }}\"\n      git push\n```\n\n### Success Criteria\n\n- State file exists and is valid JSON\n- Lock acquired within timeout (5 seconds)\n- State incremented correctly\n- Output files reference correct state value\n- State file committed (or queued for commit)\n- Audit trail complete (log entry written)\n\n### Performance Characteristics\n\n- **Lock acquisition**: < 100ms (no contention) to 5s (with contention)\n- **State read/write**: < 50ms (small JSON files)\n- **Git commit**: 1-2 seconds\n- **Total overhead**: 2-7 seconds per execution\n\n**Bottleneck:** Git commit (unavoidable for state persistence)\n\n### Key Learnings (from invoice-generator build)\n\n- **Lock timeout must have fallback** -- Never let locking failure halt the workflow\n- **Initialize on missing file** -- Don't fail on first run\n- **Timestamp fallback works** -- Breaks sequence but ensures uniqueness\n- **Audit log is separate** -- State update and audit logging are independent\n- **State-before-output** -- Update state BEFORE generating output (prevents duplicate detection)\n- **Decimal for currency** -- Use Decimal type when state affects financial calculations\n\n### Anti-Patterns to Avoid\n\n- **No locking** -- Race conditions create duplicates\n- **Hardcoded initial value** -- Make it configurable\n- **Fail without fallback** -- Lock timeout should not crash the system\n- **Update state after output** -- Output first, then state (wrong order leads to duplicates on failure)\n- **Manual state edits** -- Always use the tool (manual edits bypass validation)\n- **Ignore lock timeouts** -- Log and investigate when locks time out frequently\n\n### Related Patterns\n\nCombines elements of:\n- **Collect > Transform > Store** (base pipeline)\n- **Monitor > Detect > Alert** (state changes can trigger alerts)\n- **Generate > Review > Publish** (state affects generated content)\n\nOften used with:\n- **Intake > Enrich > Deliver** (state provides IDs or version numbers)\n- **Fan-Out > Process > Merge** (each parallel task gets unique ID from state)\n\n## 13. Multi-Source Weekly Monitor with Snapshot Comparison\n\n\n[Showing lines 1-963 of 1208 (50.0KB limit). Use offset=964 to continue.]"}],"details":{"truncation":{"content":"# WAT Systems Factory -- Workflow Pattern Catalog\n\n> Proven, reusable workflow patterns for building GitHub-native autonomous systems with Claude Code and MCP tools.\n\nEach pattern below is a composable building block. Patterns can be combined: a **Monitor > Detect > Alert** system might trigger an **Issue > Execute > PR** workflow, which internally uses **Fan-Out > Process > Merge** for the heavy lifting.\n\n---\n\n## 1. Scrape > Process > Output\n\n**Summary:** A web scraping pipeline that fetches raw content from one or more URLs via a web-access MCP, processes and structures the data with Python (or Claude itself), and writes clean output to a target destination.\n\n### When to Use\n\n- You need structured data extracted from websites that lack APIs.\n- You want to convert messy HTML into clean JSON, CSV, or Markdown on a recurring schedule.\n- You are building a dataset from publicly available web content.\n\n### Steps\n\n1. **Define targets** -- Specify URLs, selectors, or search queries in a config file or workflow input.\n2. **Fetch raw content** -- Use a web MCP (Fetch, Puppeteer, or Playwright) to retrieve page content. Handle pagination if needed.\n3. **Parse and extract** -- Strip irrelevant markup. Pull target fields using CSS selectors, XPath, or Claude-driven semantic extraction.\n4. **Transform** -- Normalize data types, deduplicate, validate schema. Python scripts or inline Claude processing.\n5. **Output** -- Write results to a file in the repo (`data/`, `output/`), post to an API, or commit as a structured artifact.\n\n### Key Tools / MCPs\n\n- **Fetch MCP** -- lightweight HTTP retrieval, returns Markdown-converted content.\n- **Puppeteer / Playwright MCP** -- for JavaScript-rendered pages requiring a headless browser.\n- **Python** -- pandas, BeautifulSoup, or custom scripts for heavy transformation.\n- **filesystem MCP** -- write results to disk.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  schedule:\n    - cron: '0 6 * * *'   # daily at 06:00 UTC\n  workflow_dispatch:        # manual trigger\n```\n\n### Example Use Cases\n\n- Scrape competitor pricing pages nightly; output a CSV diff showing changes.\n- Collect job postings from three sites; normalize into a single JSON feed committed to the repo.\n- Pull public government data tables and convert to machine-readable Parquet files.\n\n### Failure Handling\n\n- **Retry with backoff** on transient HTTP errors (429, 503).\n- **Schema validation** after extraction -- if required fields are missing, flag the record rather than silently dropping it.\n- **Stale-data guard** -- if fetch returns identical content to last run, skip processing and log \"no change.\"\n- **Alert on structural change** -- if selectors return zero matches (site redesign), open a GitHub Issue for human review.\n\n---\n\n## 2. Research > Analyze > Report\n\n**Summary:** A multi-source research workflow that gathers information from diverse sources (web, APIs, local files), synthesizes findings with Claude, and produces a formatted report committed to the repository.\n\n### When to Use\n\n- You need to compile information from multiple sources into a single coherent document.\n- Periodic market research, competitive analysis, or literature reviews.\n- Any task where a human would spend hours reading and summarizing.\n\n### Steps\n\n1. **Define research brief** -- Specify the topic, questions to answer, source priorities, and output format.\n2. **Gather sources** -- Query web search, fetch specific URLs, read local reference files, call APIs.\n3. **Extract key findings** -- For each source, pull relevant facts, quotes, data points. Tag each with provenance.\n4. **Analyze and synthesize** -- Claude cross-references findings, identifies patterns, contradictions, and gaps.\n5. **Generate report** -- Produce a structured Markdown document with sections, citations, and an executive summary.\n6. **Commit and notify** -- Commit the report to the repo. Optionally open a PR or send a notification.\n\n### Key Tools / MCPs\n\n- **Web Search MCP / Fetch MCP** -- gather online sources.\n- **filesystem MCP** -- read local reference documents.\n- **GitHub MCP** -- commit output, open issues or PRs.\n- **Claude (extended thinking)** -- deep analysis and synthesis.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  schedule:\n    - cron: '0 9 * * 1'   # weekly on Monday at 09:00 UTC\n  workflow_dispatch:\n    inputs:\n      topic:\n        description: 'Research topic'\n        required: true\n```\n\n### Example Use Cases\n\n- Weekly competitive landscape report that tracks new features announced by three competitors.\n- Monthly technology radar summarizing emerging tools relevant to the team's stack.\n- Ad-hoc deep-dive report triggered manually via `workflow_dispatch` with a topic input.\n\n### Failure Handling\n\n- **Source fallback** -- if a primary source is unreachable, log it and proceed with remaining sources. Note gaps in the report.\n- **Confidence scoring** -- Claude flags low-confidence claims; the report includes a \"Caveats\" section.\n- **Minimum-source threshold** -- if fewer than N sources are successfully retrieved, abort and open an issue rather than publishing a thin report.\n\n---\n\n## 3. Monitor > Detect > Alert\n\n**Summary:** A continuous monitoring system that periodically checks a target (website, API, dataset, repo) for changes, applies detection logic, and fires alerts or triggers downstream workflows when significant changes are found.\n\n### When to Use\n\n- You want to be notified when something changes: a webpage, an API response, a competitor's pricing, a regulatory filing.\n- You need a lightweight, Git-native alternative to commercial monitoring SaaS.\n- Ideal for cron-triggered GitHub Actions that run on a schedule.\n\n### Steps\n\n1. **Fetch current state** -- Retrieve the current snapshot of the monitored target.\n2. **Load previous state** -- Read the last-known state from a file committed in the repo (e.g., `state/last_snapshot.json`).\n3. **Diff** -- Compare current vs. previous. Use structural diff for JSON, text diff for prose, hash comparison for binary.\n4. **Evaluate significance** -- Apply rules or Claude-based judgment to determine if the change is meaningful (filter noise).\n5. **Alert** -- If significant: open a GitHub Issue, send a Slack message, trigger a downstream workflow via `repository_dispatch`, or all three.\n6. **Persist new state** -- Commit the current snapshot as the new baseline.\n\n### Key Tools / MCPs\n\n- **Fetch MCP / Puppeteer MCP** -- retrieve current state from web targets.\n- **filesystem MCP** -- read/write state files.\n- **GitHub MCP** -- open issues, trigger dispatches.\n- **Slack MCP** -- send alert messages.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  schedule:\n    - cron: '*/30 * * * *'  # every 30 minutes\n```\n\n### Example Use Cases\n\n- Monitor a government regulatory page for new filings; alert the legal team via Slack when a new document appears.\n- Watch a competitor's changelog page; open a GitHub Issue summarizing each detected update.\n- Track an API's response schema for breaking changes; alert the engineering team.\n\n### Failure Handling\n\n- **Transient failure tolerance** -- if the fetch fails, do NOT overwrite previous state. Log the failure. After N consecutive failures, escalate to an alert.\n- **False-positive suppression** -- use Claude to evaluate whether a detected \"change\" is meaningful (e.g., ignore timestamp-only diffs).\n- **State corruption recovery** -- if state file is missing or unparseable, re-initialize from scratch and log a warning instead of crashing.\n\n---\n\n## 4. Intake > Enrich > Deliver\n\n**Summary:** A data enrichment pipeline that receives raw input (from a form, webhook, file drop, or API), augments it with additional data from external sources, and delivers the enriched result to its destination.\n\n### When to Use\n\n- Incoming data is incomplete and needs augmentation before it is useful.\n- You are building a lead enrichment, content tagging, or metadata enhancement system.\n- Raw data arrives in one format and must be transformed and enriched before storage or delivery.\n\n### Steps\n\n1. **Intake** -- Receive raw data. Sources: webhook payload, new file in repo, manual trigger with JSON input, or a watched directory.\n2. **Validate** -- Check that required fields exist and data types are correct. Reject malformed input early.\n3. **Enrich** -- For each record, call enrichment sources: web search for context, APIs for supplementary data, Claude for classification or summarization.\n4. **Merge** -- Combine original data with enrichment results into a unified schema.\n5. **Deliver** -- Write enriched data to its destination: commit to repo, POST to API, append to a dataset file, or open a PR.\n\n### Key Tools / MCPs\n\n- **Web Search MCP** -- find supplementary information.\n- **Fetch MCP** -- call enrichment APIs.\n- **Claude** -- classify, tag, summarize, or score records.\n- **filesystem MCP** -- read input, write output.\n- **GitHub MCP** -- commit results or open PRs.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  push:\n    paths:\n      - 'inbox/**'          # new file dropped in inbox\n  workflow_dispatch:\n    inputs:\n      payload:\n        description: 'JSON data to enrich'\n        required: true\n```\n\n### Example Use Cases\n\n- A sales team drops a CSV of company names into `inbox/`; the pipeline enriches each with industry, size, website, and LinkedIn URL, then writes to `data/enriched_leads.csv`.\n- Incoming support tickets are enriched with customer history and sentiment score before routing.\n- Raw RSS feed items are enriched with summaries, topic tags, and relevance scores.\n\n### Failure Handling\n\n- **Per-record error isolation** -- if enrichment fails for one record, mark it as `enrichment_failed` and continue processing the rest. Do not let one bad record kill the batch.\n- **Rate-limit awareness** -- track API call counts; pause and retry when rate-limited rather than failing fast.\n- **Partial delivery** -- deliver successfully enriched records on schedule; queue failed records for retry in the next run.\n\n---\n\n## 5. Generate > Review > Publish\n\n**Summary:** A content generation pipeline with built-in quality gates. Claude generates draft content, a review step validates quality and compliance, and approved content is published to its destination.\n\n### When to Use\n\n- Automated content creation (blog posts, documentation, social media, release notes) where quality must be verified before publishing.\n- Any workflow where you want a \"human-in-the-loop\" approval step or an automated quality check before output goes live.\n\n### Steps\n\n1. **Brief** -- Define the content requirements: topic, audience, tone, length, format, and any reference material.\n2. **Generate** -- Claude produces a first draft using the brief and any provided reference material.\n3. **Automated review** -- Run quality checks: spelling/grammar, factual consistency against source material, tone analysis, compliance rules (e.g., no prohibited claims).\n4. **Gate decision** -- If automated review passes, proceed. If it fails, loop back to step 2 with feedback (max N retries). Optionally, require human approval via a PR review.\n5. **Publish** -- Commit final content to the repo, deploy to a CMS via API, or post to a platform.\n\n### Key Tools / MCPs\n\n- **Claude** -- generation and review (use separate system prompts for generator vs. reviewer to create genuine separation).\n- **filesystem MCP** -- read briefs, write drafts and final output.\n- **GitHub MCP** -- open a draft PR for human review; merge on approval.\n- **Fetch MCP** -- publish to external CMS or platform APIs.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  issues:\n    types: [labeled]        # label \"content-request\" triggers generation\n  pull_request_review:\n    types: [submitted]      # human approval triggers publish\n  workflow_dispatch:\n```\n\n### Example Use Cases\n\n- Weekly changelog: gather merged PRs, generate a formatted changelog, open a PR for team review, publish on approval.\n- Blog post pipeline: issue labeled \"draft-request\" triggers generation; editor reviews the PR; merge triggers deployment.\n- Social media batch: generate a week's worth of posts, run brand-voice review, output approved posts to a scheduling queue.\n\n### Failure Handling\n\n- **Retry budget** -- if automated review rejects the draft, regenerate with targeted feedback up to 3 times. After that, escalate to a human by opening an issue.\n- **Rollback on publish failure** -- if the publish step fails (API error), keep the approved content in the repo and log the failure for manual retry.\n- **Diff guard** -- before publishing, diff against previously published content to prevent accidental overwrite of manual edits.\n\n---\n\n## 6. Listen > Decide > Act\n\n**Summary:** An event-driven agent that listens for incoming events (webhooks, GitHub events, messages), applies decision logic to determine the appropriate response, and takes autonomous action.\n\n### When to Use\n\n- You need a system that reacts to external events in near-real-time.\n- Webhook-triggered GitHub Actions are the execution engine.\n- The appropriate response depends on the content of the event (routing, triage, conditional logic).\n\n### Steps\n\n1. **Receive event** -- GitHub Actions triggers on the event (webhook, issue creation, PR comment, `repository_dispatch`).\n2. **Parse and classify** -- Extract relevant data from the event payload. Use Claude to classify intent or severity if the input is unstructured (e.g., natural language).\n3. **Decide** -- Apply decision logic: rule-based routing, Claude-driven triage, or a lookup table. Determine which action(s) to take.\n4. **Act** -- Execute the chosen action: respond to the issue, trigger another workflow, call an API, update a file, or escalate to a human.\n5. **Log** -- Record the event, decision, and action taken for auditability.\n\n### Key Tools / MCPs\n\n- **GitHub MCP** -- read event payloads, respond to issues/PRs, trigger dispatches.\n- **Claude** -- classify, triage, and decide.\n- **Slack MCP** -- notify humans when escalation is needed.\n- **Fetch MCP** -- call external APIs as part of the action.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  issues:\n    types: [opened, labeled]\n  issue_comment:\n    types: [created]\n  repository_dispatch:\n    types: [incoming_webhook]\n```\n\n### Example Use Cases\n\n- Triage bot: new issues are classified by Claude (bug, feature, question) and auto-labeled, assigned, and replied to with relevant templates.\n- Deployment gatekeeper: a PR comment like \"/deploy staging\" triggers a deployment workflow after validating the commenter's permissions.\n- Customer feedback router: webhook delivers feedback; Claude scores sentiment and routes critical items to Slack.\n\n### Failure Handling\n\n- **Idempotency** -- design actions to be safe to retry. Use event IDs to detect and skip duplicate deliveries.\n- **Decision logging** -- log the full decision chain (event > classification > decision > action) so failures can be diagnosed.\n- **Safe defaults** -- if classification confidence is low, default to escalation (notify a human) rather than taking an autonomous action that might be wrong.\n\n---\n\n## 7. Collect > Transform > Store\n\n**Summary:** A classic ETL (Extract, Transform, Load) pipeline where Git serves as the data store. Data is collected from external sources, transformed into a canonical format, and committed to the repository as versioned, queryable files.\n\n### When to Use\n\n- You want a version-controlled data lake without standing up a database.\n- Small-to-medium datasets (tens of MB) that benefit from Git's diff, history, and branching.\n- Data pipelines where auditability and reproducibility matter more than query speed.\n\n### Steps\n\n1. **Collect** -- Pull data from source(s): APIs, web scrapes, file uploads, other repos.\n2. **Validate** -- Check data integrity: schema validation, null checks, type enforcement.\n3. **Transform** -- Normalize, clean, reshape. Convert to the canonical schema. Deduplicate against existing data in the store.\n4. **Diff** -- Compare transformed data against the current version in the repo. Compute what is new, changed, or deleted.\n5. **Store** -- Commit the updated dataset files to the repo. Use meaningful commit messages describing what changed (e.g., \"Add 14 new records from source X\").\n\n### Key Tools / MCPs\n\n- **Fetch MCP** -- pull data from APIs and web sources.\n- **Python** -- pandas or custom scripts for transformation.\n- **filesystem MCP** -- read/write data files.\n- **GitHub MCP / Git CLI** -- commit, push, branch as needed.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  schedule:\n    - cron: '0 */6 * * *'  # every 6 hours\n  workflow_dispatch:\n```\n\n### Example Use Cases\n\n- Collect cryptocurrency prices from three APIs every 6 hours; store as daily CSV files in `data/prices/`.\n- Pull open government datasets weekly; transform into a normalized schema; maintain a living dataset in the repo.\n- Aggregate internal metrics from multiple microservice health endpoints; store as JSON for a dashboard.\n\n### Failure Handling\n\n- **Atomic commits** -- only commit if the entire pipeline succeeds. Never commit partial or corrupted data.\n- **Schema drift detection** -- if source data no longer matches the expected schema, abort and alert rather than storing malformed data.\n- **Rollback via Git** -- if bad data is committed, revert the commit. The previous state is always recoverable.\n- **Dedup safety** -- idempotent writes ensure that re-running the pipeline does not create duplicate records.\n\n---\n\n## 8. Fan-Out > Process > Merge\n\n**Summary:** The Agent Teams pattern. A coordinator agent splits a large task into independent sub-tasks, dispatches them to parallel sub-agents (or sequential iterations), collects all results, and merges them into a unified output.\n\n### When to Use\n\n- A task is naturally decomposable into independent units of work (e.g., process each file, research each topic, analyze each competitor).\n- You want to reduce wall-clock time by parallelizing work.\n- The final deliverable requires combining results from multiple sub-tasks.\n\n### Steps\n\n1. **Plan** -- The coordinator analyzes the task and determines how to decompose it. Produces a work manifest: a list of sub-tasks with their inputs.\n2. **Fan-Out** -- Dispatch each sub-task. Options: parallel GitHub Actions jobs via a matrix strategy, sequential Claude tool calls, or `repository_dispatch` events to separate workflows.\n3. **Process** -- Each sub-agent executes its sub-task independently. Results are written to a known location (e.g., `temp/results/task_01.json`).\n4. **Collect** -- The coordinator waits for all sub-tasks to complete and gathers their outputs.\n5. **Merge** -- Combine results: concatenate, deduplicate, reconcile conflicts, and produce the final unified output.\n6. **Deliver** -- Commit the merged output or pass it to the next stage in a larger pipeline.\n\n### Key Tools / MCPs\n\n- **Claude** -- coordinator logic, sub-task execution, merge logic.\n- **GitHub Actions matrix** -- parallel job execution.\n- **filesystem MCP** -- read/write intermediate and final results.\n- **GitHub MCP** -- coordinate via issues, artifacts, or dispatch events.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  workflow_dispatch:\n    inputs:\n      task_manifest:\n        description: 'JSON array of sub-tasks'\n        required: true\n  repository_dispatch:\n    types: [fan_out_request]\n```\n\n### Example Use Cases\n\n- Analyze 20 competitor websites: fan out one sub-agent per competitor, each produces a profile, merge into a single competitive analysis report.\n- Process a batch of 50 documents: each sub-agent summarizes one document, merge into a master summary with cross-references.\n- Multi-language translation: fan out one sub-agent per target language, merge all translations into a localized content bundle.\n\n### Failure Handling\n\n- **Partial success tolerance** -- if some sub-tasks fail, merge the successful results and report failures. Do not discard all work because of one failure.\n- **Retry failed sub-tasks** -- the coordinator can re-dispatch failed sub-tasks up to N times before giving up.\n- **Timeout enforcement** -- set a maximum wall-clock time for the fan-out phase. Collect whatever is done when the timer expires.\n- **Conflict resolution** -- if sub-task results conflict, the merge step uses Claude to reconcile or flags the conflict for human review.\n\n---\n\n## 9. Issue > Execute > PR\n\n**Summary:** The GitHub Agent HQ pattern. An agent receives a task specification via a GitHub Issue, autonomously executes the required workflow, and delivers the results as a draft Pull Request for human review.\n\n### When to Use\n\n- You want a human-in-the-loop task queue where work is requested via Issues and delivered via PRs.\n- The agent should be fully autonomous in execution but the output requires human approval before merging.\n- Building an internal tool where non-technical stakeholders can request work by opening an Issue.\n\n### Steps\n\n1. **Receive** -- A new Issue is opened (or an existing Issue is labeled with a trigger label like `agent-task`). GitHub Actions fires.\n2. **Parse task** -- Claude reads the Issue body and extracts the task specification: what to do, inputs, constraints, expected output.\n3. **Create branch** -- Create a feature branch named after the Issue (e.g., `agent/issue-42`).\n4. **Execute** -- Run the appropriate workflow or pattern to complete the task. This might invoke any other pattern in this catalog.\n5. **Commit results** -- Commit all output files to the feature branch with clear commit messages.\n6. **Open draft PR** -- Open a draft PR linked to the original Issue. The PR body includes a summary of what was done, key decisions made, and anything that needs human attention.\n7. **Comment on Issue** -- Post a comment on the Issue linking to the PR and summarizing the outcome.\n\n### Key Tools / MCPs\n\n- **GitHub MCP** -- read issues, create branches, commit, open PRs, post comments.\n- **Claude** -- parse task, execute logic, generate summaries.\n- **Any other MCP** -- depending on the task being executed.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  issues:\n    types: [opened, labeled]\n  issue_comment:\n    types: [created]       # support \"/execute\" command in comments\n```\n\n### Example Use Cases\n\n- A product manager opens an Issue: \"Research top 5 CRM tools and compare features.\" The agent researches, builds a comparison table, and opens a PR with the report.\n- An engineer opens an Issue: \"Add unit tests for the auth module.\" The agent generates tests and delivers them as a PR.\n- A stakeholder opens an Issue: \"Update the pricing page data for Q1.\" The agent pulls new data, updates the files, and opens a PR for review.\n\n### Failure Handling\n\n- **Status updates** -- the agent comments on the Issue at each major step (\"Starting research...\", \"Generating report...\", \"Opening PR...\") so progress is visible.\n- **Graceful failure** -- if the agent cannot complete the task, it comments on the Issue with a clear explanation of what went wrong and what it tried, then labels the Issue `agent-failed` for human triage.\n- **Scope guard** -- if Claude determines the task is ambiguous or out of scope, it asks a clarifying question in the Issue comments rather than guessing.\n- **Draft-only PRs** -- all PRs are opened as drafts. The agent never merges its own work.\n\n---\n\n## 10. n8n Import > Translate > Deploy\n\n**Summary:** A conversion pattern that ingests an n8n workflow JSON export, translates it into a WAT-native system (Claude Code + MCP + GitHub Actions), and deploys it as a fully functional GitHub-native workflow.\n\n### When to Use\n\n- You are migrating from n8n (or a similar visual workflow tool) to a GitHub-native architecture.\n- You want to leverage existing workflow logic without manually rewriting it.\n- You are evaluating whether an n8n workflow can run as a WAT system to reduce infrastructure dependencies.\n\n### Steps\n\n1. **Import** -- Read the n8n workflow JSON file. This contains nodes, connections, credentials references, and configuration.\n2. **Parse and map** -- Walk the node graph. For each n8n node type, identify the equivalent WAT component:\n   - HTTP Request node -> Fetch MCP call\n   - Code (JavaScript/Python) node -> inline script or Python file\n   - IF/Switch node -> Claude decision logic or conditional in workflow\n   - Webhook trigger -> `repository_dispatch` or `workflow_dispatch`\n   - Cron trigger -> `schedule` trigger in GitHub Actions\n   - Set/Merge nodes -> data transformation in Claude or Python\n3. **Translate** -- Generate the WAT system files:\n   - `.github/workflows/*.yml` -- GitHub Actions workflow definition.\n   - `prompts/` -- Claude system prompts for nodes that require AI logic.\n   - `scripts/` -- Python or shell scripts for code nodes.\n   - `config/` -- Configuration extracted from the n8n workflow.\n4. **Validate** -- Dry-run the generated workflow. Check that all node types were mapped, no credentials are hardcoded, and the control flow is correct.\n5. **Deploy** -- Commit the generated files to the repo. Open a PR with a detailed summary of the translation, noting any nodes that required manual review or could not be auto-translated.\n\n### Key Tools / MCPs\n\n- **filesystem MCP** -- read n8n JSON, write generated files.\n- **Claude** -- interpret node logic, generate prompts, translate code, produce workflow YAML.\n- **GitHub MCP** -- commit files, open PR.\n- **Python** -- parse n8n JSON, generate boilerplate.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  push:\n    paths:\n      - 'imports/n8n/*.json'   # drop an n8n export to trigger conversion\n  workflow_dispatch:\n    inputs:\n      n8n_file:\n        description: 'Path to n8n JSON file in the repo'\n        required: true\n```\n\n### Example Use Cases\n\n- Migrate a 15-node n8n lead-scoring workflow to a Git-versioned, GitHub Actions-powered system.\n- Convert an n8n social media posting pipeline into a WAT **Generate > Review > Publish** system.\n- Translate an n8n monitoring workflow into a WAT **Monitor > Detect > Alert** pattern, eliminating the need for a running n8n instance.\n\n### Failure Handling\n\n- **Unsupported node catalog** -- maintain a list of n8n node types that cannot be auto-translated. When encountered, flag them in the PR description with a `TODO` and a suggested manual implementation.\n- **Credential safety** -- never translate credentials. Replace all credential references with GitHub Secrets placeholders (`${{ secrets.SERVICE_API_KEY }}`) and document which secrets need to be configured.\n- **Fidelity check** -- generate a side-by-side comparison of the n8n flow and the translated WAT flow so a human reviewer can verify correctness.\n- **Incremental migration** -- support translating a subset of the n8n workflow, allowing teams to migrate node-by-node rather than all-at-once.\n\n---\n\n## Pattern Composition Quick Reference\n\nPatterns are building blocks. Here are common compositions:\n\n| Composition | How It Works |\n|---|---|\n| **Monitor > Detect > Alert** + **Issue > Execute > PR** | Monitor detects a change, opens an Issue, which triggers the agent to act and deliver a PR. |\n| **Issue > Execute > PR** using **Fan-Out > Process > Merge** | A complex Issue task is decomposed and parallelized internally before the PR is opened. |\n| **Scrape > Process > Output** feeding **Collect > Transform > Store** | Scraping output is piped into the ETL pattern for versioned storage. |\n| **Listen > Decide > Act** routing to any pattern | An event-driven dispatcher that selects and triggers the appropriate pattern based on the event. |\n| **n8n Import > Translate > Deploy** producing any pattern | The translator identifies which WAT pattern best fits the n8n workflow and generates accordingly. |\n\n---\n\n## Choosing a Pattern\n\n| Question | Recommended Pattern |\n|---|---|\n| Do I need to pull data from websites? | **Scrape > Process > Output** |\n| Do I need to synthesize information from many sources? | **Research > Analyze > Report** |\n| Do I need to watch for changes over time? | **Monitor > Detect > Alert** |\n| Do I need to augment incoming data? | **Intake > Enrich > Deliver** |\n| Do I need AI-generated content with quality control? | **Generate > Review > Publish** |\n| Do I need to react to external events? | **Listen > Decide > Act** |\n| Do I need versioned data storage without a database? | **Collect > Transform > Store** |\n| Do I need to parallelize a large task? | **Fan-Out > Process > Merge** |\n| Do I need a task queue with human approval? | **Issue > Execute > PR** |\n| Do I need to migrate an n8n workflow? | **n8n Import > Translate > Deploy** |\n| Do I need B2B lead generation from web data? | **Intake > Enrich > Deliver** (lead-gen variant) |\n\n---\n\n## Proven Compositions (from builds)\n\n### Lead Generation Pipeline (lead-gen-machine)\n- **Pattern**: Intake > Enrich > Deliver + Scrape > Process > Output\n- **Flow**: Parse profile > Search web > Scrape sites > Extract contacts (Claude) > Score leads > Output CSV\n- **Key insight**: The Intake > Enrich > Deliver pattern works well when the \"enrichment\" involves multi-step web scraping + AI extraction. Each company is an independent unit of work, making this a strong candidate for Fan-Out > Process > Merge parallelization.\n- **Scoring pattern**: Decompose match criteria into independent dimensions (industry, size, location, keywords) with weighted point values. This makes the scoring transparent and tunable.\n- **Fallback chains**: Firecrawl > HTTP+BeautifulSoup for scraping, Claude > regex for extraction. Always have a degraded-but-functional fallback.\n\n### Marketing Pipeline (marketing-pipeline)\n- **Pattern**: Intake > Enrich > Deliver + Generate > Review > Publish\n- **Flow**: Ingest CSV > Enrich via Apollo/Hunter/Brave/Claude > Deep-score (5 dimensions) > Segment (Hot/Warm/Cold) > Generate outreach (Hot) > Generate nurture (Warm) > Output CSVs + emails\n- **Key insight**: Multi-API enrichment with graceful degradation. Apollo (primary) > Hunter (email verification) > Brave (research) > Claude (pain analysis). Each API layer adds value but the system works with only Anthropic API.\n- **Scoring pattern**: 5-dimension scoring (size fit, tech stack compatibility, budget signals, accessibility, pain signals) with transparent point breakdown. More dimensions = more nuanced than lead-gen-machine's 4-dimension approach.\n- **Content generation pattern**: Outreach and nurture are independent post-segmentation, making them natural Agent Teams candidates. Outreach is per-lead personalized; nurture is aggregate-informed.\n- **API fallback chain**: Apollo > Hunter > Brave > Claude > HTTP scraping. Four-level fallback with clear degradation at each level.\n- **CSV-in/CSV-out**: Accepts CSV input (compatible with lead-gen-machine output), produces segmented CSVs + markdown email sequences. Easy to chain systems.\n\n### Website Uptime Monitor (website-uptime-monitor)\n- **Pattern**: Monitor > Log (simplified from Monitor > Detect > Alert)\n- **Flow**: Check URL via HTTP GET > Measure response time > Determine up/down status > Append to CSV > Commit to Git\n- **Key insight**: Simplest possible monitoring system - no detection logic, no alerting, just raw data collection. Every check produces a log entry regardless of status. Git history IS the audit trail.\n- **No MCPs needed**: Direct Python `requests` library is simpler and more reliable than Fetch MCP for a single GET request. CSV append uses stdlib. Zero external dependencies beyond GitHub Actions.\n- **Exit code signaling**: Tool exits 0 if up, 1 if down - GitHub Actions UI shows workflow as \"failed\" when site is down, providing at-a-glance status without additional alerting logic.\n- **Concurrency handling**: GitHub Actions `concurrency` setting prevents simultaneous runs. Git push includes retry with rebase to handle rare concurrent commits.\n- **CSV as database**: Append-only CSV committed to Git provides versioned, auditable, queryable dataset. No external database needed. Git handles MB-scale data easily.\n- **Cron variance accepted**: GitHub Actions cron has ±5 minute variance - documented as expected behavior rather than a bug to fix.\n- **Three execution paths**: (1) Scheduled cron (primary), (2) Manual dispatch (testing), (3) Local CLI (development). Agent HQ (4) optional for issue-driven tasks.\n- **Cost awareness**: ~1,440 GitHub Actions minutes/month for 5-minute checks. Fits within free tier (2,000/month private, unlimited public).\n- **Extensibility points**: Add alerting by extending monitor.yml (Slack webhook, GitHub Issue). Add multiple URLs via matrix strategy or separate workflows. Add authentication via tool arguments + GitHub Secrets.\n\n### CSV-to-JSON Converter (csv-to-json-converter)\n- **Pattern**: Collect > Transform > Store\n- **Flow**: Collect CSV files > Analyze structure > Parse CSV > Infer types > Validate data > Convert to JSON/JSONL > Generate reports > Commit\n- **Key insight**: Multi-phase transformation with specialist subagents. Each phase (analyze, infer, validate, write) has a dedicated subagent with focused responsibilities. Main agent orchestrates the pipeline.\n- **Subagent architecture**: Four specialist subagents (csv-analyzer, type-inference-specialist, data-validator, json-writer) with minimal tool access. Subagents are the DEFAULT delegation mechanism, not Agent Teams.\n- **Type inference pattern**: Statistical analysis with confidence scoring (80% threshold). Detects int, float, boolean, datetime, string. Falls back to string for ambiguous columns. Handles 10+ null value representations.\n- **Encoding detection**: chardet for automatic encoding detection with UTF-8 fallback. Strips BOM (byte order mark) automatically. Supports UTF-8, Latin-1, Windows-1252, etc.\n- **Ragged row handling**: Pad short rows with null, truncate long rows with warning. Log all issues in validation report but continue processing (graceful degradation).\n- **Validation without failure**: Validator NEVER raises exceptions. Always returns a report. Strict mode controls whether issues halt the pipeline, but validation itself is fail-safe.\n- **JSONL streaming**: For large files (1M+ rows), JSONL format writes one record at a time. No array wrapper. Memory-efficient for multi-GB datasets.\n- **Batch processing modes**: Sequential (1-2 files) or parallel (3+ files with Agent Teams). Parallel is 5x faster wall time but same token cost. Always has sequential fallback.\n- **Three execution paths**: (1) CLI for local development, (2) GitHub Actions for production, (3) Agent HQ for issue-driven tasks. All three produce identical output.\n- **Metadata reporting**: run_summary.json (machine-readable) + validation_report.md (human-readable). Includes per-file stats, type inference results, validation issues, recommendations.\n- **No secrets required**: Uses only stdlib (csv, json, pathlib) + chardet + python-dateutil. No external APIs. Pure data transformation pipeline.\n- **Git commit discipline**: Stage ONLY output files (never `git add -A`). Commit message includes file count, row count, timestamp. Output directory is parameterized.\n\n### RSS Digest Monitor (rss-digest-monitor)\n- **Pattern**: Monitor > Collect > Transform > Deliver (with state persistence)\n- **Flow**: Load state > Fetch RSS feeds > Filter new posts > Generate HTML digest > Send email > Update state > Commit\n- **Key insight**: State persistence in Git prevents post duplication across runs. Composite GUID keys (feed_url::guid) prevent false deduplication. State is ONLY updated after successful email delivery, ensuring retry on failure.\n- **Subagent architecture**: Three specialist subagents (rss-fetcher-specialist, digest-generator-specialist, state-manager-specialist). Subagents are the DEFAULT delegation mechanism. Sequential execution is correct due to clear data dependencies.\n- **Per-feed error isolation**: One failed feed does NOT kill the entire run. Each feed is fetched independently with try/except. Failed feeds are logged and skipped. Graceful degradation ensures partial success.\n- **Composite GUID pattern**: Use `feed_url::entry_guid` as deduplication key instead of guid alone. Prevents false positives when different feeds use the same GUID format.\n- **State file size management**: Enforce maximum 10,000 GUIDs by keeping most recent entries. Prevents unbounded state growth. Log warning if limit is hit.\n- **Email with fallback**: MIME multipart with HTML primary and plain-text fallback. Ensures compatibility with all email clients. HTML is styled with inline CSS for maximum rendering support.\n- **Conditional email send**: Skip email generation if no new posts found. Silent run with state update only. Saves API calls and prevents empty emails.\n- **SMTP retry logic**: Retry once with 30s delay on transient failures (auth, network, timeout). Do NOT update state if email send fails -- posts retry on next run.\n- **First-run initialization**: Missing or corrupted state file initializes empty state. All posts are processed on first run. Log warning but continue (better to duplicate once than crash).\n- **Date parsing with fallback**: Use python-dateutil for robust parsing of varied date formats. If unparseable, use current timestamp. Include post anyway (better to duplicate than miss).\n- **Three execution paths**: (1) Scheduled cron at 8 AM UTC (primary), (2) Manual dispatch with force_send (testing), (3) Local CLI (development). All produce identical output.\n- **Run summary logging**: Each run produces `logs/YYYY-MM-DD_run.json` with feeds_checked, feeds_failed, new_posts, email_sent, failed_feeds array. Machine-readable audit trail.\n- **Git commit discipline**: Stage ONLY state/rss_state.json and logs/*.json. NEVER `git add -A`. Commit message includes date and post count for easy history review.\n- **No MCP dependencies**: Uses standard Python libraries (feedparser, smtplib, json, pathlib). Works out-of-the-box with only Python and requirements.txt. No external MCPs required.\n- **Cost awareness**: ~3-5 minutes per day = ~90-150 GitHub Actions minutes/month. Well within free tier (2,000/month private, unlimited public). Minimal token cost for state management only.\n\n## 11. Content Transformation with Tone Matching\n\n**Summary:** An AI-powered content repurposing pipeline that extracts content from a source URL, analyzes its tone and style, and generates platform-optimized variants (social media, email, etc.) that match the source tone automatically.\n\n### When to Use\n\n- You need to publish the same content across multiple platforms with different format requirements\n- Content must maintain consistent voice/tone across all platforms\n- Manual reformatting for each platform is time-consuming and error-prone\n- You want AI to handle platform-specific best practices (character limits, hashtags, formatting)\n\n### Steps\n\n1. **Fetch source content** -- Scrape the source URL (blog post, article) and extract clean text/markdown. Use Firecrawl with HTTP fallback for reliability.\n2. **Analyze tone** -- Use LLM (Claude) with structured output to analyze writing style across multiple dimensions: formality, technical level, humor, primary emotion. Return confidence score.\n3. **Generate platform variants (parallel)** -- For each target platform (Twitter, LinkedIn, email, Instagram):\n   - Send source content + tone analysis to LLM\n   - Request platform-optimized output matching source tone\n   - Enforce platform-specific constraints (char limits, hashtag counts, formatting rules)\n   - Validate character counts before accepting output\n4. **Merge and output** -- Combine all variants into single JSON file with metadata, tone analysis, and all platform content\n\n### Key Tools / MCPs\n\n- **Firecrawl MCP** -- reliable web scraping with JS rendering support\n- **Anthropic MCP** -- Claude for tone analysis and content generation\n- **HTTP + BeautifulSoup** -- fallback scraping if Firecrawl unavailable\n- **Python stdlib** -- JSON manipulation, filename slugification, output assembly\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  workflow_dispatch:\n    inputs:\n      blog_url:\n        description: 'Blog post URL to repurpose'\n        required: true\n  # Can also be triggered on new blog post detection via RSS or webhook\n```\n\n### Example Use Cases\n\n- Marketing team publishes a blog post, wants Twitter thread, LinkedIn post, email newsletter section, and Instagram caption\n- Content creator wants to maximize reach by publishing same content on all major platforms\n- Agency needs to repurpose client blog posts for multi-channel campaigns\n- Developer advocates want to turn technical blog posts into social-friendly content\n\n### Subagent Architecture\n\n- **content-scraper-specialist** -- Handles web scraping with fallback strategies\n- **tone-analyzer-specialist** -- Analyzes source content tone/style\n- **content-generator-specialist** -- Coordinates multi-platform generation (can use Agent Teams for parallelization)\n- **output-assembler-specialist** -- Merges all content into final JSON\n\n### Agent Teams Parallelization\n\nPlatform generation is ideal for Agent Teams:\n- 4 independent platform generation tasks (Twitter, LinkedIn, email, Instagram)\n- Each takes 10-15 seconds sequentially = 40-55s total\n- Parallel execution with 4 teammates = 12-18s\n- **2x speedup** with identical results and same token cost\n\n### Platform-Specific Constraints\n\n| Platform | Char Limit | Hashtags | Format Notes |\n|----------|------------|----------|--------------|\n| Twitter | 280 per tweet | 2-3 per thread | Numbered thread (1/N format), hook in first tweet, CTA in last |\n| LinkedIn | 3000 max, 1300 target | 3-5 | Professional tone, hook before \"see more\" fold, line breaks |\n| Email | 500-800 words | N/A | Subject line (40-60 chars), scannable format, HTML + plain text |\n| Instagram | 2200 max | 10-15 | Casual tone, emojis (3-5), line breaks, hook before \"more\" button |\n\n### Tone Matching Pattern\n\n1. LLM analyzes source content with structured JSON schema\n2. Returns tone profile: {formality, technical_level, humor_level, primary_emotion, confidence}\n3. Platform generators receive tone profile in their prompts\n4. Each generator adjusts output style to match source tone (formal → formal, casual → casual)\n5. Result: All platforms sound like they were written by the same person\n\n### Failure Handling\n\n- **Scraping failure** -- Halt workflow immediately (no content = can't proceed). Log clear error about paywall/404/timeout.\n- **Tone analysis failure** -- Return default neutral profile, continue with warning. Quality slightly reduced but system remains functional.\n- **Single platform failure** -- Continue with other platforms. Include error structure for failed platform in output. Partial success is acceptable.\n- **Character limit exceeded** -- Tools automatically truncate with \"...\" and log warning. Better than crashing.\n\n### Performance & Cost\n\n**Execution time**:\n- Sequential: ~52-74 seconds\n- With Agent Teams: ~25-37 seconds (2x faster)\n\n**API cost per run** (Claude Sonnet 4):\n- Scraping: $0.01-0.02 (Firecrawl)\n- LLM calls: ~$0.09 (tone + 4 platforms)\n- **Total: ~$0.10-0.11 per blog post**\n\n### Success Criteria\n\n✅ Source content extracted (≥ 100 chars)\n✅ Tone analysis returns confidence ≥ 0.5\n✅ At least 2/4 platforms generate successfully\n✅ All character counts within platform limits\n✅ Output JSON file written to repo\n\n### Key Learnings\n\n- **Tone matching is powerful** -- AI can accurately replicate writing style when given structured tone analysis\n- **Parallel generation scales well** -- 4 independent LLM calls are perfect for Agent Teams\n- **Character validation is critical** -- Must validate before returning, not after\n- **Partial success is acceptable** -- 3/4 platforms working is better than all-or-nothing\n- **Firecrawl + fallback pattern works** -- Primary API with HTTP backup maximizes reliability\n- **Subagent specialization reduces complexity** -- Each subagent has clear, scoped responsibility\n\n### Related Patterns\n\nCombines elements of:\n- **Scrape > Process > Output** (content extraction)\n- **Fan-Out > Process > Merge** (parallel platform generation via Agent Teams)\n- **Generate > Review > Publish** (content creation with validation)\n\n## 12. Sequential State Management Pipeline\n\n**Summary:** A data transformation pipeline that maintains persistent state across runs using atomic file operations. Each execution reads current state, transforms data, generates output, updates state, and commits results -- all while preventing race conditions.\n\n### When to Use\n\n- You need sequential numbering or versioning that persists across workflow runs\n- Output depends on previous runs (counters, IDs, sequence numbers)\n- Concurrent execution must not create duplicates\n- State must survive container/server restarts\n- Git-native state storage is preferred over external databases\n\n### Steps\n\n1. **Read State** -- Load previous state from committed file\n2. **Lock State** -- Acquire file lock to prevent concurrent modifications\n3. **Transform Data** -- Process input using current state\n4. **Update State** -- Increment counter, add ID, update version\n5. **Generate Output** -- Produce artifacts using updated state\n6. **Commit State** -- Write state file and outputs to repo\n7. **Release Lock** -- Allow next execution to proceed\n\n### Key Tools / MCPs\n\n- **filelock** (Python) -- Atomic file locking with timeout\n- **Git** -- State file versioning and recovery\n- **filesystem MCP** -- Read/write state files\n- **JSON** (stdlib) -- State serialization\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  workflow_dispatch:\n    inputs:\n      data:\n        description: 'Input data to process'\n        required: true\n  push:\n    paths:\n      - 'input/**'\n```\n\n### Example Use Cases\n\n- Invoice generator with sequential invoice numbers (INV-1001, INV-1002...)\n- Document versioning system (v1.0, v1.1, v2.0...)\n- Sequential ID assignment for records\n- Incrementing batch job numbers\n- Auto-incrementing PR numbers in custom systems\n\n### State File Format\n\n```json\n{\n  \"last_value\": 1042,\n  \"prefix\": \"INV-\",\n  \"padding\": 4,\n  \"metadata\": {\n    \"last_updated\": \"2026-02-11T14:30:00Z\",\n    \"total_count\": 1042\n  }\n}\n```\n\n### Atomic Operations Pattern\n\n```python\nfrom filelock import FileLock, Timeout\n\nlock_path = f\"{state_file}.lock\"\ntry:\n    with FileLock(lock_path, timeout=5):\n        # Read current state\n        state = json.loads(Path(state_file).read_text())\n        \n        # Modify state\n        state[\"last_value\"] += 1\n        \n        # Write updated state\n        Path(state_file).write_text(json.dumps(state, indent=2))\n        \n        # Return new value\n        return state[\"last_value\"]\nexcept Timeout:\n    # Fallback: use timestamp or other unique value\n    return generate_fallback_value()\n```\n\n### Failure Handling\n\n**Lock timeout (5 seconds)**\n- **Cause**: Concurrent execution or filesystem issue\n- **Action**: Use fallback mechanism (timestamp, UUID, or queue for retry)\n- **Log**: Warning with details for investigation\n- **Continue**: Workflow proceeds with fallback value\n\n**State file missing**\n- **Cause**: First run or file deleted\n- **Action**: Initialize to default starting value\n- **Log**: Info message about initialization\n- **Continue**: Normal operation from initial state\n\n**State file corrupted**\n- **Cause**: Incomplete write, manual edit, merge conflict\n- **Action**: Restore from Git history or reinitialize\n- **Log**: Error with corrupted content\n- **Halt**: Require manual intervention (prevents data loss)\n\n**Concurrent modifications**\n- **Prevented by**: File locking ensures only one process modifies at a time\n- **If lock fails**: Fallback prevents workflow failure\n- **Trade-off**: Fallback may break sequential ordering but ensures system continues\n\n### Git-Native State Management\n\nState files are committed to the repository:\n\n**Benefits:**\n- Version history (full audit trail)\n- Rollback capability (`git checkout HEAD~5 state.json`)\n- Merge conflict detection\n- No external database needed\n- Free hosting on GitHub\n\n**Considerations:**\n- Not suitable for high-frequency updates (>100/min)\n- Commit overhead (~1-2 seconds per update)\n- Repo size impact (minimal for small JSON files)\n\n### Fallback Strategies\n\nWhen atomic state update fails, use one of these fallbacks:\n\n| Strategy | When to Use | Trade-off |\n|----------|-------------|-----------|\n| **Timestamp** | Unique value needed, order not critical | Breaks sequential numbering |\n| **UUID** | Unique value needed, no pattern required | No human-readable sequence |\n| **Queue for retry** | Sequential order is critical | Delays processing |\n| **Range allocation** | High concurrency expected | More complex state management |\n\n### Integration Points\n\n**GitHub Actions:**\n```yaml\nsteps:\n  - name: Get next value\n    id: state\n    run: |\n      python tools/manage_state.py state.json get_next > output.json\n      VALUE=$(jq -r '.value' output.json)\n      echo \"VALUE=$VALUE\" >> $GITHUB_OUTPUT\n  \n  - name: Use value\n    run: |\n      process_data.py --id ${{ steps.state.outputs.VALUE }}\n  \n  - name: Commit state\n    run: |\n      git add state.json output/*\n      git commit -m \"Process ID ${{ steps.state.outputs.VALUE }}\"\n      git push\n```\n\n### Success Criteria\n\n- State file exists and is valid JSON\n- Lock acquired within timeout (5 seconds)\n- State incremented correctly\n- Output files reference correct state value\n- State file committed (or queued for commit)\n- Audit trail complete (log entry written)\n\n### Performance Characteristics\n\n- **Lock acquisition**: < 100ms (no contention) to 5s (with contention)\n- **State read/write**: < 50ms (small JSON files)\n- **Git commit**: 1-2 seconds\n- **Total overhead**: 2-7 seconds per execution\n\n**Bottleneck:** Git commit (unavoidable for state persistence)\n\n### Key Learnings (from invoice-generator build)\n\n- **Lock timeout must have fallback** -- Never let locking failure halt the workflow\n- **Initialize on missing file** -- Don't fail on first run\n- **Timestamp fallback works** -- Breaks sequence but ensures uniqueness\n- **Audit log is separate** -- State update and audit logging are independent\n- **State-before-output** -- Update state BEFORE generating output (prevents duplicate detection)\n- **Decimal for currency** -- Use Decimal type when state affects financial calculations\n\n### Anti-Patterns to Avoid\n\n- **No locking** -- Race conditions create duplicates\n- **Hardcoded initial value** -- Make it configurable\n- **Fail without fallback** -- Lock timeout should not crash the system\n- **Update state after output** -- Output first, then state (wrong order leads to duplicates on failure)\n- **Manual state edits** -- Always use the tool (manual edits bypass validation)\n- **Ignore lock timeouts** -- Log and investigate when locks time out frequently\n\n### Related Patterns\n\nCombines elements of:\n- **Collect > Transform > Store** (base pipeline)\n- **Monitor > Detect > Alert** (state changes can trigger alerts)\n- **Generate > Review > Publish** (state affects generated content)\n\nOften used with:\n- **Intake > Enrich > Deliver** (state provides IDs or version numbers)\n- **Fan-Out > Process > Merge** (each parallel task gets unique ID from state)\n\n## 13. Multi-Source Weekly Monitor with Snapshot Comparison\n","truncated":true,"truncatedBy":"bytes","totalLines":1208,"totalBytes":59905,"outputLines":963,"outputBytes":51115,"lastLinePartial":false,"firstLineExceedsLimit":false,"maxLines":2000,"maxBytes":51200}},"isError":false,"timestamp":1771029073206}}
{"type":"message","id":"165b20b4","parentId":"c585d566","timestamp":"2026-02-14T00:31:13.209Z","message":{"role":"toolResult","toolCallId":"toolu_015TKX84qaUsgknBqmRVAeXa","toolName":"read","content":[{"type":"text","text":"# WAT Systems Factory -- Reusable Tool Pattern Catalog\n\n> **Version:** 0.1.0-seed\n> **Maintainer:** WAT Systems Factory / Agent HQ\n> **Purpose:** Canonical reference of composable tool patterns that factory-generated workflows can assemble into pipelines. Each pattern is self-contained, testable, and convertible to an MCP tool or n8n node.\n\n---\n\n## Table of Contents\n\n1. [Web Scraping & Search](#web-scraping--search)\n2. [File I/O](#file-io)\n3. [API Calls](#api-calls)\n4. [Data Processing](#data-processing)\n5. [AI Processing](#ai-processing)\n6. [Notifications](#notifications)\n7. [Git Operations](#git-operations)\n8. [GitHub API](#github-api)\n9. [Database Operations](#database-operations)\n10. [Media Processing](#media-processing)\n11. [n8n Conversion](#n8n-conversion)\n12. [Agent Teams Coordination](#agent-teams-coordination)\n13. [GitHub Agent HQ Interaction](#github-agent-hq-interaction)\n\n---\n\n## Web Scraping & Search\n\n### `firecrawl_scrape`\n\n**Description:** Scrapes a single URL or crawls an entire domain via the Firecrawl API, returning clean Markdown-formatted content suitable for downstream LLM consumption.\n\n| Field | Detail |\n|---|---|\n| **Input** | `url: str`, `mode: Literal[\"scrape\", \"crawl\"]`, `max_pages: int = 50`, `selectors: list[str] = []` |\n| **Output** | `dict` with keys `markdown: str`, `metadata: dict`, `links: list[str]` |\n| **Key Dependencies** | `firecrawl-py`, `httpx` |\n| **MCP Alternative** | `@anthropic/firecrawl-mcp` server -- exposes `firecrawl_scrape` and `firecrawl_crawl` tools directly |\n\n```python\nimport os\nfrom firecrawl import FirecrawlApp\n\ndef main(url: str, mode: str = \"scrape\", max_pages: int = 50,\n         selectors: list[str] | None = None) -> dict:\n    \"\"\"Scrape or crawl a URL via Firecrawl and return Markdown content.\"\"\"\n    app = FirecrawlApp(api_key=os.environ[\"FIRECRAWL_API_KEY\"])\n\n    if mode == \"crawl\":\n        result = app.crawl_url(url, params={\n            \"limit\": max_pages,\n            \"scrapeOptions\": {\"formats\": [\"markdown\"]},\n        })\n        pages = [{\"markdown\": p.get(\"markdown\", \"\"), \"url\": p.get(\"url\")}\n                 for p in result.get(\"data\", [])]\n        return {\"pages\": pages, \"count\": len(pages)}\n\n    # Default: single-page scrape\n    result = app.scrape_url(url, params={\"formats\": [\"markdown\"]})\n    return {\n        \"markdown\": result.get(\"markdown\", \"\"),\n        \"metadata\": result.get(\"metadata\", {}),\n        \"links\": result.get(\"links\", []),\n    }\n```\n\n---\n\n### `brave_search`\n\n**Description:** Runs a web search query through the Brave Search API and returns structured result objects with titles, URLs, and snippets.\n\n| Field | Detail |\n|---|---|\n| **Input** | `query: str`, `count: int = 10`, `freshness: str = \"\"` |\n| **Output** | `list[dict]` each with `title`, `url`, `description`, `age` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | `@anthropic/brave-search-mcp` server -- exposes `brave_web_search` and `brave_local_search` |\n\n```python\nimport os, httpx\n\nBRAVE_ENDPOINT = \"https://api.search.brave.com/res/v1/web/search\"\n\ndef main(query: str, count: int = 10, freshness: str = \"\") -> list[dict]:\n    \"\"\"Execute a Brave web search and return structured results.\"\"\"\n    headers = {\n        \"Accept\": \"application/json\",\n        \"X-Subscription-Token\": os.environ[\"BRAVE_API_KEY\"],\n    }\n    params = {\"q\": query, \"count\": min(count, 20)}\n    if freshness:\n        params[\"freshness\"] = freshness  # e.g. \"pd\" (past day), \"pw\", \"pm\"\n\n    resp = httpx.get(BRAVE_ENDPOINT, headers=headers, params=params, timeout=15)\n    resp.raise_for_status()\n\n    results = []\n    for item in resp.json().get(\"web\", {}).get(\"results\", []):\n        results.append({\n            \"title\": item.get(\"title\"),\n            \"url\": item.get(\"url\"),\n            \"description\": item.get(\"description\"),\n            \"age\": item.get(\"age\", \"\"),\n        })\n    return results\n```\n\n---\n\n### `puppeteer_screenshot`\n\n**Description:** Launches a headless browser via Puppeteer (through its MCP server or a local Node bridge) to capture a screenshot or extract JavaScript-rendered DOM content from a page.\n\n| Field | Detail |\n|---|---|\n| **Input** | `url: str`, `action: Literal[\"screenshot\", \"extract\"]`, `selector: str = \"body\"`, `viewport: dict = {\"width\": 1280, \"height\": 720}` |\n| **Output** | `dict` with `base64_image: str` (for screenshot) or `html: str` (for extract) |\n| **Key Dependencies** | `playwright` (Python) or Node `puppeteer` via subprocess |\n| **MCP Alternative** | `@anthropic/puppeteer-mcp` server -- exposes `puppeteer_navigate`, `puppeteer_screenshot`, `puppeteer_click`, `puppeteer_fill` |\n\n```python\nfrom playwright.async_api import async_playwright\nimport asyncio, base64\n\nasync def _run(url: str, action: str, selector: str, viewport: dict) -> dict:\n    async with async_playwright() as pw:\n        browser = await pw.chromium.launch(headless=True)\n        page = await browser.new_page(viewport_size=viewport)\n        await page.goto(url, wait_until=\"networkidle\")\n\n        if action == \"screenshot\":\n            buf = await page.screenshot(full_page=True)\n            await browser.close()\n            return {\"base64_image\": base64.b64encode(buf).decode()}\n\n        content = await page.inner_html(selector)\n        await browser.close()\n        return {\"html\": content}\n\ndef main(url: str, action: str = \"screenshot\", selector: str = \"body\",\n         viewport: dict | None = None) -> dict:\n    \"\"\"Capture a screenshot or extract rendered HTML from a URL.\"\"\"\n    vp = viewport or {\"width\": 1280, \"height\": 720}\n    return asyncio.run(_run(url, action, selector, vp))\n```\n\n---\n\n## File I/O\n\n### `csv_read_write`\n\n**Description:** Reads a CSV file into a list of dictionaries or writes a list of dictionaries out to CSV, handling encoding detection and BOM stripping automatically.\n\n| Field | Detail |\n|---|---|\n| **Input** | `path: str`, `mode: Literal[\"read\", \"write\"]`, `data: list[dict] = []`, `encoding: str = \"utf-8-sig\"` |\n| **Output** | `list[dict]` on read; `dict` with `rows_written: int, path: str` on write |\n| **Key Dependencies** | `csv` (stdlib), `chardet` (optional encoding detection) |\n| **MCP Alternative** | `@anthropic/filesystem-mcp` for raw read/write; CSV parsing done in-tool |\n\n```python\nimport csv, io, pathlib\n\ndef main(path: str, mode: str = \"read\", data: list[dict] | None = None,\n         encoding: str = \"utf-8-sig\") -> dict | list[dict]:\n    \"\"\"Read or write CSV files with automatic encoding handling.\"\"\"\n    p = pathlib.Path(path)\n\n    if mode == \"read\":\n        text = p.read_text(encoding=encoding)\n        reader = csv.DictReader(io.StringIO(text))\n        return [dict(row) for row in reader]\n\n    # Write mode\n    if not data:\n        raise ValueError(\"data is required for write mode\")\n    fieldnames = list(data[0].keys())\n    with p.open(\"w\", newline=\"\", encoding=encoding) as f:\n        writer = csv.DictWriter(f, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(data)\n    return {\"rows_written\": len(data), \"path\": str(p.resolve())}\n```\n\n---\n\n### `json_read_write`\n\n**Description:** Reads a JSON or JSONL file into Python objects or writes Python objects out to JSON/JSONL, with optional pretty-printing and schema validation.\n\n| Field | Detail |\n|---|---|\n| **Input** | `path: str`, `mode: Literal[\"read\", \"write\"]`, `data: Any = None`, `jsonl: bool = False`, `indent: int = 2` |\n| **Output** | Parsed object(s) on read; `dict` with `path: str, size_bytes: int` on write |\n| **Key Dependencies** | `json` (stdlib), `jsonschema` (optional validation) |\n| **MCP Alternative** | `@anthropic/filesystem-mcp` for raw I/O |\n\n```python\nimport json, pathlib\n\ndef main(path: str, mode: str = \"read\", data=None,\n         jsonl: bool = False, indent: int = 2):\n    \"\"\"Read or write JSON / JSONL files.\"\"\"\n    p = pathlib.Path(path)\n\n    if mode == \"read\":\n        text = p.read_text(encoding=\"utf-8\")\n        if jsonl:\n            return [json.loads(line) for line in text.splitlines() if line.strip()]\n        return json.loads(text)\n\n    # Write mode\n    if jsonl:\n        lines = [json.dumps(item, ensure_ascii=False) for item in data]\n        p.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n    else:\n        p.write_text(json.dumps(data, indent=indent, ensure_ascii=False),\n                      encoding=\"utf-8\")\n    return {\"path\": str(p.resolve()), \"size_bytes\": p.stat().st_size}\n```\n\n---\n\n### `apollo_enrich`\n\n**Description:** Enriches a company name/domain with company data and decision-maker contacts via the Apollo.io API. Returns structured company info, tech stack, and key contacts.\n\n| Field | Detail |\n|---|---|\n| **Input** | `company_name: str`, `domain: str = None` |\n| **Output** | `dict` with company data, tech stack, decision makers |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | None; direct HTTP API |\n\n---\n\n### `hunter_email_search`\n\n**Description:** Searches for email addresses at a given domain via the Hunter.io API. Returns verified contacts with confidence scores.\n\n| Field | Detail |\n|---|---|\n| **Input** | `domain: str`, `limit: int = 5` |\n| **Output** | `dict` with emails, patterns, organization info |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | None; direct HTTP API |\n\n---\n\n### `multi_dimension_scorer`\n\n**Description:** Scores a data record across N independent dimensions with configurable point values. Used for lead scoring, content quality assessment, or any multi-criteria evaluation.\n\n| Field | Detail |\n|---|---|\n| **Input** | `record: dict`, `dimensions: list[dict]` (each with name, max_points, scoring_function) |\n| **Output** | `dict` with dimension breakdown and total score |\n| **Key Dependencies** | None (stdlib only) |\n| **MCP Alternative** | None |\n\n---\n\n### `email_sequence_generator`\n\n**Description:** Generates a multi-email sequence (outreach or nurture) using Claude API. Takes company/audience context and a sequence template, returns structured JSON with subject lines, bodies, and send timing.\n\n| Field | Detail |\n|---|---|\n| **Input** | `context: dict`, `sequence_template: str`, `email_count: int` |\n| **Output** | `dict` with sequence of emails (subject, body, send_day) |\n| **Key Dependencies** | `anthropic` |\n| **MCP Alternative** | Anthropic MCP |\n\n---\n\n### `pdf_extract`\n\n**Description:** Extracts text content, metadata, and optionally tables from a PDF file, returning structured output ready for LLM ingestion or data-pipeline processing.\n\n| Field | Detail |\n|---|---|\n| **Input** | `path: str`, `pages: str = \"all\"`, `extract_tables: bool = False` |\n| **Output** | `dict` with `text: str`, `pages: list[dict]`, `metadata: dict`, `tables: list` |\n| **Key Dependencies** | `pymupdf` (fitz), `pdfplumber` (for tables) |\n| **MCP Alternative** | None standard; wrap as custom MCP tool |\n\n```python\nimport fitz  # pymupdf\n\ndef main(path: str, pages: str = \"all\", extract_tables: bool = False) -> dict:\n    \"\"\"Extract text and metadata from a PDF file.\"\"\"\n    doc = fitz.open(path)\n    metadata = dict(doc.metadata)\n\n    page_range = range(len(doc))\n    if pages != \"all\":\n        # Parse ranges like \"1-5\" or \"3,7,10-12\"\n        page_range = _parse_page_range(pages, len(doc))\n\n    extracted_pages = []\n    for i in page_range:\n        page = doc[i]\n        extracted_pages.append({\n            \"page_number\": i + 1,\n            \"text\": page.get_text(\"text\"),\n        })\n\n    result = {\n        \"text\": \"\\n\\n\".join(p[\"text\"] for p in extracted_pages),\n        \"pages\": extracted_pages,\n        \"metadata\": metadata,\n    }\n\n    if extract_tables:\n        import pdfplumber\n        with pdfplumber.open(path) as pdf:\n            result[\"tables\"] = []\n            for i in page_range:\n                for table in pdf.pages[i].extract_tables():\n                    result[\"tables\"].append({\"page\": i + 1, \"rows\": table})\n\n    return result\n\ndef _parse_page_range(spec: str, total: int) -> list[int]:\n    indices = []\n    for part in spec.split(\",\"):\n        if \"-\" in part:\n            start, end = part.split(\"-\", 1)\n            indices.extend(range(int(start) - 1, min(int(end), total)))\n        else:\n            indices.append(int(part) - 1)\n    return [i for i in indices if 0 <= i < total]\n```\n\n---\n\n## API Calls\n\n### `rest_client`\n\n**Description:** A generic REST API client with configurable retry logic, exponential backoff, header injection, and response normalization -- the universal building block for any HTTP-based integration.\n\n| Field | Detail |\n|---|---|\n| **Input** | `url: str`, `method: str = \"GET\"`, `headers: dict = {}`, `body: Any = None`, `retries: int = 3`, `backoff: float = 1.0`, `timeout: int = 30` |\n| **Output** | `dict` with `status: int`, `headers: dict`, `body: Any`, `elapsed_ms: float` |\n| **Key Dependencies** | `httpx`, `tenacity` |\n| **MCP Alternative** | `@anthropic/fetch-mcp` for simple GET/POST |\n\n```python\nimport httpx, time\nfrom tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception\n\ndef _is_retryable(exc):\n    if isinstance(exc, httpx.HTTPStatusError):\n        return exc.response.status_code in (429, 500, 502, 503, 504)\n    return isinstance(exc, (httpx.ConnectTimeout, httpx.ReadTimeout))\n\ndef main(url: str, method: str = \"GET\", headers: dict | None = None,\n         body=None, retries: int = 3, backoff: float = 1.0,\n         timeout: int = 30) -> dict:\n    \"\"\"Make an HTTP request with retry logic and exponential backoff.\"\"\"\n\n    @retry(\n        stop=stop_after_attempt(retries),\n        wait=wait_exponential(multiplier=backoff, min=1, max=30),\n        retry=retry_if_exception(_is_retryable),\n        reraise=True,\n    )\n    def _do_request():\n        t0 = time.monotonic()\n        resp = httpx.request(\n            method=method.upper(),\n            url=url,\n            headers=headers or {},\n            json=body if isinstance(body, (dict, list)) else None,\n            content=body if isinstance(body, (str, bytes)) else None,\n            timeout=timeout,\n        )\n        elapsed = (time.monotonic() - t0) * 1000\n        resp.raise_for_status()\n        try:\n            resp_body = resp.json()\n        except Exception:\n            resp_body = resp.text\n        return {\n            \"status\": resp.status_code,\n            \"headers\": dict(resp.headers),\n            \"body\": resp_body,\n            \"elapsed_ms\": round(elapsed, 2),\n        }\n\n    return _do_request()\n```\n\n---\n\n### `graphql_client`\n\n**Description:** Sends GraphQL queries and mutations to any endpoint, with variable substitution, error extraction, and the same retry semantics as `rest_client`.\n\n| Field | Detail |\n|---|---|\n| **Input** | `endpoint: str`, `query: str`, `variables: dict = {}`, `headers: dict = {}`, `retries: int = 3` |\n| **Output** | `dict` with `data: Any`, `errors: list | None`, `status: int` |\n| **Key Dependencies** | `httpx`, `tenacity` |\n| **MCP Alternative** | None standard; wrap as custom MCP tool |\n\n```python\nimport httpx\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\ndef main(endpoint: str, query: str, variables: dict | None = None,\n         headers: dict | None = None, retries: int = 3) -> dict:\n    \"\"\"Execute a GraphQL query or mutation with retry logic.\"\"\"\n\n    @retry(stop=stop_after_attempt(retries),\n           wait=wait_exponential(multiplier=1, min=1, max=15))\n    def _execute():\n        payload = {\"query\": query}\n        if variables:\n            payload[\"variables\"] = variables\n        resp = httpx.post(\n            endpoint,\n            json=payload,\n            headers={\"Content-Type\": \"application/json\", **(headers or {})},\n            timeout=30,\n        )\n        resp.raise_for_status()\n        result = resp.json()\n        return {\n            \"data\": result.get(\"data\"),\n            \"errors\": result.get(\"errors\"),\n            \"status\": resp.status_code,\n        }\n\n    return _execute()\n```\n\n---\n\n### `oauth_token_refresh`\n\n**Description:** Manages OAuth 2.0 token lifecycle -- obtains tokens via client_credentials or authorization_code grants, caches them, and transparently refreshes before expiry.\n\n| Field | Detail |\n|---|---|\n| **Input** | `token_url: str`, `client_id: str`, `client_secret: str`, `grant_type: str = \"client_credentials\"`, `refresh_token: str = \"\"`, `scopes: list[str] = []` |\n| **Output** | `dict` with `access_token: str`, `expires_in: int`, `token_type: str` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | None; typically embedded within other tools |\n\n```python\nimport httpx, time\n\n_token_cache: dict = {}\n\ndef main(token_url: str, client_id: str, client_secret: str,\n         grant_type: str = \"client_credentials\",\n         refresh_token: str = \"\", scopes: list[str] | None = None) -> dict:\n    \"\"\"Obtain or refresh an OAuth 2.0 access token.\"\"\"\n    cache_key = f\"{token_url}:{client_id}\"\n\n    # Return cached token if still valid\n    cached = _token_cache.get(cache_key)\n    if cached and cached[\"expires_at\"] > time.time() + 60:\n        return {\n            \"access_token\": cached[\"access_token\"],\n            \"expires_in\": int(cached[\"expires_at\"] - time.time()),\n            \"token_type\": cached[\"token_type\"],\n        }\n\n    data = {\"grant_type\": grant_type, \"client_id\": client_id,\n            \"client_secret\": client_secret}\n    if refresh_token:\n        data[\"refresh_token\"] = refresh_token\n    if scopes:\n        data[\"scope\"] = \" \".join(scopes)\n\n    resp = httpx.post(token_url, data=data, timeout=15)\n    resp.raise_for_status()\n    token_data = resp.json()\n\n    _token_cache[cache_key] = {\n        \"access_token\": token_data[\"access_token\"],\n        \"token_type\": token_data.get(\"token_type\", \"Bearer\"),\n        \"expires_at\": time.time() + token_data.get(\"expires_in\", 3600),\n    }\n    return {\n        \"access_token\": token_data[\"access_token\"],\n        \"expires_in\": token_data.get(\"expires_in\", 3600),\n        \"token_type\": token_data.get(\"token_type\", \"Bearer\"),\n    }\n```\n\n---\n\n## Data Processing\n\n### `filter_sort`\n\n**Description:** Applies filter predicates and multi-key sorting to a list of dictionaries, supporting comparison operators, regex matching, and null handling.\n\n| Field | Detail |\n|---|---|\n| **Input** | `data: list[dict]`, `filters: list[dict]` (each with `field`, `op`, `value`), `sort_by: list[dict]` (each with `field`, `order`) |\n| **Output** | `list[dict]` -- filtered and sorted records |\n| **Key Dependencies** | `re` (stdlib) |\n| **MCP Alternative** | None; pure logic tool |\n\n```python\nimport re, operator\n\nOPS = {\n    \"eq\": operator.eq, \"ne\": operator.ne,\n    \"gt\": operator.gt, \"ge\": operator.ge,\n    \"lt\": operator.lt, \"le\": operator.le,\n    \"contains\": lambda a, b: b in str(a),\n    \"regex\": lambda a, b: bool(re.search(b, str(a))),\n    \"in\": lambda a, b: a in b,\n}\n\ndef main(data: list[dict], filters: list[dict] | None = None,\n         sort_by: list[dict] | None = None) -> list[dict]:\n    \"\"\"Filter and sort a list of dictionaries.\"\"\"\n    result = data\n\n    # Apply filters\n    for f in (filters or []):\n        op_fn = OPS.get(f[\"op\"], operator.eq)\n        result = [row for row in result if op_fn(row.get(f[\"field\"]), f[\"value\"])]\n\n    # Apply sorting\n    if sort_by:\n        for s in reversed(sort_by):\n            reverse = s.get(\"order\", \"asc\") == \"desc\"\n            result.sort(key=lambda row: (row.get(s[\"field\"]) is None,\n                                          row.get(s[\"field\"], \"\")),\n                        reverse=reverse)\n\n    return result\n```\n\n---\n\n### `dedup`\n\n**Description:** Removes duplicate records from a dataset based on one or more key fields, with configurable strategies for which duplicate to keep (first, last, or merge).\n\n| Field | Detail |\n|---|---|\n| **Input** | `data: list[dict]`, `keys: list[str]`, `strategy: Literal[\"first\", \"last\", \"merge\"] = \"first\"` |\n| **Output** | `dict` with `unique: list[dict]`, `duplicates_removed: int` |\n| **Key Dependencies** | None (stdlib only) |\n| **MCP Alternative** | None; pure logic tool |\n\n```python\ndef main(data: list[dict], keys: list[str],\n         strategy: str = \"first\") -> dict:\n    \"\"\"Deduplicate records by key fields.\"\"\"\n    seen: dict[tuple, dict] = {}\n    order: list[tuple] = []\n\n    for row in data:\n        key = tuple(row.get(k) for k in keys)\n        if key not in seen:\n            seen[key] = row\n            order.append(key)\n        elif strategy == \"last\":\n            seen[key] = row\n        elif strategy == \"merge\":\n            # Merge: later values fill in blanks only\n            for k, v in row.items():\n                if seen[key].get(k) in (None, \"\"):\n                    seen[key][k] = v\n\n    unique = [seen[k] for k in order]\n    return {\n        \"unique\": unique,\n        \"duplicates_removed\": len(data) - len(unique),\n    }\n```\n\n---\n\n### `transform_map`\n\n**Description:** Applies field-level transformations to every record in a dataset -- renaming columns, computing derived fields, type-casting, and applying template expressions.\n\n| Field | Detail |\n|---|---|\n| **Input** | `data: list[dict]`, `mappings: list[dict]` (each with `source`, `target`, `transform`), `drop_unmapped: bool = False` |\n| **Output** | `list[dict]` -- transformed records |\n| **Key Dependencies** | None (stdlib only) |\n| **MCP Alternative** | None; pure logic tool |\n\n```python\nfrom datetime import datetime\n\nTRANSFORMS = {\n    \"upper\": lambda v: str(v).upper(),\n    \"lower\": lambda v: str(v).lower(),\n    \"strip\": lambda v: str(v).strip(),\n    \"int\": lambda v: int(v) if v not in (None, \"\") else None,\n    \"float\": lambda v: float(v) if v not in (None, \"\") else None,\n    \"isodate\": lambda v: datetime.fromisoformat(str(v)).isoformat(),\n    \"bool\": lambda v: str(v).lower() in (\"true\", \"1\", \"yes\"),\n    \"default\": lambda v: v,\n}\n\ndef main(data: list[dict], mappings: list[dict],\n         drop_unmapped: bool = False) -> list[dict]:\n    \"\"\"Apply field-level transformations to each record.\"\"\"\n    results = []\n    for row in data:\n        new_row = {} if drop_unmapped else dict(row)\n        for m in mappings:\n            source_val = row.get(m[\"source\"])\n            transform_fn = TRANSFORMS.get(m.get(\"transform\", \"default\"),\n                                           TRANSFORMS[\"default\"])\n            new_row[m[\"target\"]] = transform_fn(source_val)\n        results.append(new_row)\n    return results\n```\n\n---\n\n## AI Processing\n\n### `llm_prompt`\n\n**Description:** Sends a prompt to an LLM provider (OpenAI, Anthropic, or compatible endpoint) with system instructions, returning the raw text completion and token usage metadata.\n\n| Field | Detail |\n|---|---|\n| **Input** | `prompt: str`, `system: str = \"\"`, `provider: Literal[\"openai\", \"anthropic\"] = \"anthropic\"`, `model: str = \"\"`, `max_tokens: int = 4096`, `temperature: float = 0.3` |\n| **Output** | `dict` with `text: str`, `model: str`, `usage: dict` |\n| **Key Dependencies** | `anthropic`, `openai` |\n| **MCP Alternative** | None; this IS the core LLM call |\n\n```python\nimport os\n\ndef main(prompt: str, system: str = \"\", provider: str = \"anthropic\",\n         model: str = \"\", max_tokens: int = 4096,\n         temperature: float = 0.3) -> dict:\n    \"\"\"Send a prompt to an LLM and return the completion.\"\"\"\n\n    if provider == \"anthropic\":\n        import anthropic\n        client = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n        resolved_model = model or \"claude-sonnet-4-20250514\"\n        msg = client.messages.create(\n            model=resolved_model,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            system=system or \"You are a helpful assistant.\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n        )\n        return {\n            \"text\": msg.content[0].text,\n            \"model\": resolved_model,\n            \"usage\": {\"input_tokens\": msg.usage.input_tokens,\n                      \"output_tokens\": msg.usage.output_tokens},\n        }\n\n    # OpenAI / compatible\n    import openai\n    client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n    resolved_model = model or \"gpt-4o\"\n    messages = []\n    if system:\n        messages.append({\"role\": \"system\", \"content\": system})\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n    resp = client.chat.completions.create(\n        model=resolved_model, messages=messages,\n        max_tokens=max_tokens, temperature=temperature,\n    )\n    choice = resp.choices[0]\n    return {\n        \"text\": choice.message.content,\n        \"model\": resolved_model,\n        \"usage\": {\"input_tokens\": resp.usage.prompt_tokens,\n                  \"output_tokens\": resp.usage.completion_tokens},\n    }\n```\n\n---\n\n### `structured_extract`\n\n**Description:** Sends content to an LLM with a JSON schema instruction, parses the response into a validated Python object, and retries with corrective prompts if the response fails schema validation.\n\n| Field | Detail |\n|---|---|\n| **Input** | `content: str`, `schema: dict` (JSON Schema), `instructions: str = \"\"`, `retries: int = 2` |\n| **Output** | `dict` with `data: Any` (validated), `raw: str`, `attempts: int` |\n| **Key Dependencies** | `anthropic` or `openai`, `jsonschema` |\n| **MCP Alternative** | None; core LLM extraction pattern |\n\n```python\nimport json, os\nfrom jsonschema import validate, ValidationError\n\ndef main(content: str, schema: dict, instructions: str = \"\",\n         retries: int = 2) -> dict:\n    \"\"\"Extract structured data from content using an LLM with schema validation.\"\"\"\n    import anthropic\n    client = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n\n    system_prompt = (\n        \"Extract information from the provided content and return ONLY valid JSON \"\n        \"matching the schema below. No markdown fences, no commentary.\\n\\n\"\n        f\"Schema:\\n```json\\n{json.dumps(schema, indent=2)}\\n```\"\n    )\n    if instructions:\n        system_prompt += f\"\\n\\nAdditional instructions: {instructions}\"\n\n    user_content = content\n    for attempt in range(1, retries + 2):\n        msg = client.messages.create(\n            model=\"claude-sonnet-4-20250514\",\n            max_tokens=4096,\n            temperature=0.0,\n            system=system_prompt,\n            messages=[{\"role\": \"user\", \"content\": user_content}],\n        )\n        raw = msg.content[0].text.strip()\n\n        # Strip markdown fences if present\n        if raw.startswith(\"```\"):\n            raw = raw.split(\"\\n\", 1)[1].rsplit(\"```\", 1)[0].strip()\n\n        try:\n            parsed = json.loads(raw)\n            validate(instance=parsed, schema=schema)\n            return {\"data\": parsed, \"raw\": raw, \"attempts\": attempt}\n        except (json.JSONDecodeError, ValidationError) as exc:\n            if attempt > retries:\n                raise ValueError(f\"Failed after {attempt} attempts: {exc}\") from exc\n            user_content = (\n                f\"Your previous response was invalid: {exc}\\n\\n\"\n                f\"Original content:\\n{content}\\n\\n\"\n                \"Please try again, returning ONLY valid JSON.\"\n            )\n\n    raise RuntimeError(\"Unreachable\")\n```\n\n---\n\n### `embedding_generate`\n\n**Description:** Generates vector embeddings for one or more text inputs using OpenAI, Voyage, or a local model, returning arrays ready for storage in a vector database.\n\n| Field | Detail |\n|---|---|\n| **Input** | `texts: list[str]`, `provider: Literal[\"openai\", \"voyage\"] = \"openai\"`, `model: str = \"\"` |\n| **Output** | `dict` with `embeddings: list[list[float]]`, `model: str`, `dimensions: int` |\n| **Key Dependencies** | `openai` or `voyageai` |\n| **MCP Alternative** | None standard |\n\n```python\nimport os\n\ndef main(texts: list[str], provider: str = \"openai\",\n         model: str = \"\") -> dict:\n    \"\"\"Generate vector embeddings for text inputs.\"\"\"\n\n    if provider == \"openai\":\n        import openai\n        client = openai.OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n        resolved = model or \"text-embedding-3-small\"\n        resp = client.embeddings.create(input=texts, model=resolved)\n        vecs = [item.embedding for item in resp.data]\n        return {\n            \"embeddings\": vecs,\n            \"model\": resolved,\n            \"dimensions\": len(vecs[0]) if vecs else 0,\n        }\n\n    if provider == \"voyage\":\n        import voyageai\n        client = voyageai.Client(api_key=os.environ[\"VOYAGE_API_KEY\"])\n        resolved = model or \"voyage-2\"\n        resp = client.embed(texts, model=resolved)\n        return {\n            \"embeddings\": resp.embeddings,\n            \"model\": resolved,\n            \"dimensions\": len(resp.embeddings[0]) if resp.embeddings else 0,\n        }\n\n    raise ValueError(f\"Unsupported provider: {provider}\")\n```\n\n---\n\n## Notifications\n\n### `slack_notify`\n\n**Description:** Sends a message to a Slack channel or thread via the Slack Web API or an incoming webhook, supporting Block Kit for rich formatting.\n\n| Field | Detail |\n|---|---|\n| **Input** | `channel: str`, `text: str`, `blocks: list[dict] = []`, `thread_ts: str = \"\"`, `use_webhook: bool = False` |\n| **Output** | `dict` with `ok: bool`, `ts: str`, `channel: str` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | `@anthropic/slack-mcp` server -- exposes `slack_post_message`, `slack_list_channels`, etc. |\n\n```python\nimport os, httpx\n\ndef main(channel: str, text: str, blocks: list[dict] | None = None,\n         thread_ts: str = \"\", use_webhook: bool = False) -> dict:\n    \"\"\"Send a Slack message to a channel or thread.\"\"\"\n\n    if use_webhook:\n        webhook_url = os.environ[\"SLACK_WEBHOOK_URL\"]\n        payload = {\"text\": text}\n        if blocks:\n            payload[\"blocks\"] = blocks\n        resp = httpx.post(webhook_url, json=payload, timeout=10)\n        resp.raise_for_status()\n        return {\"ok\": True, \"ts\": \"\", \"channel\": channel}\n\n    token = os.environ[\"SLACK_BOT_TOKEN\"]\n    payload = {\"channel\": channel, \"text\": text}\n    if blocks:\n        payload[\"blocks\"] = blocks\n    if thread_ts:\n        payload[\"thread_ts\"] = thread_ts\n\n    resp = httpx.post(\n        \"https://slack.com/api/chat.postMessage\",\n        json=payload,\n        headers={\"Authorization\": f\"Bearer {token}\",\n                 \"Content-Type\": \"application/json\"},\n        timeout=10,\n    )\n    data = resp.json()\n    return {\"ok\": data.get(\"ok\", False), \"ts\": data.get(\"ts\", \"\"),\n            \"channel\": data.get(\"channel\", channel)}\n```\n\n---\n\n### `discord_webhook`\n\n**Description:** Posts a message or embed to a Discord channel via a webhook URL, with support for rich embeds, username overrides, and file attachments.\n\n| Field | Detail |\n|---|---|\n| **Input** | `webhook_url: str`, `content: str = \"\"`, `embeds: list[dict] = []`, `username: str = \"\"` |\n| **Output** | `dict` with `status: int`, `success: bool` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | None standard; wrap as custom MCP tool |\n\n```python\nimport httpx\n\ndef main(webhook_url: str, content: str = \"\", embeds: list[dict] | None = None,\n         username: str = \"\") -> dict:\n    \"\"\"Send a message or embed to Discord via webhook.\"\"\"\n    payload: dict = {}\n    if content:\n        payload[\"content\"] = content\n    if embeds:\n        payload[\"embeds\"] = embeds\n    if username:\n        payload[\"username\"] = username\n\n    if not payload:\n        raise ValueError(\"Must provide content or embeds\")\n\n    resp = httpx.post(webhook_url, json=payload, timeout=10)\n    return {\n        \"status\": resp.status_code,\n        \"success\": resp.status_code in (200, 204),\n    }\n```\n\n---\n\n### `email_send`\n\n**Description:** Sends an email via SMTP or a transactional email API (SendGrid, Resend), supporting HTML body, plain-text fallback, and file attachments.\n\n| Field | Detail |\n|---|---|\n| **Input** | `to: list[str]`, `subject: str`, `body_html: str = \"\"`, `body_text: str = \"\"`, `provider: Literal[\"smtp\", \"sendgrid\", \"resend\"] = \"resend\"`, `from_addr: str = \"\"` |\n| **Output** | `dict` with `success: bool`, `message_id: str` |\n| **Key Dependencies** | `httpx`, `smtplib` (stdlib) |\n| **MCP Alternative** | None standard |\n\n```python\nimport os, httpx\n\ndef main(to: list[str], subject: str, body_html: str = \"\",\n         body_text: str = \"\", provider: str = \"resend\",\n         from_addr: str = \"\") -> dict:\n    \"\"\"Send an email via a transactional provider or SMTP.\"\"\"\n    sender = from_addr or os.environ.get(\"EMAIL_FROM\", \"noreply@example.com\")\n\n    if provider == \"resend\":\n        resp = httpx.post(\n            \"https://api.resend.com/emails\",\n            headers={\"Authorization\": f\"Bearer {os.environ['RESEND_API_KEY']}\",\n                     \"Content-Type\": \"application/json\"},\n            json={\n                \"from\": sender, \"to\": to, \"subject\": subject,\n                \"html\": body_html or f\"<pre>{body_text}</pre>\",\n                \"text\": body_text or \"\",\n            },\n            timeout=15,\n        )\n        resp.raise_for_status()\n        data = resp.json()\n        return {\"success\": True, \"message_id\": data.get(\"id\", \"\")}\n\n    if provider == \"sendgrid\":\n        resp = httpx.post(\n            \"https://api.sendgrid.com/v3/mail/send\",\n            headers={\"Authorization\": f\"Bearer {os.environ['SENDGRID_API_KEY']}\",\n                     \"Content-Type\": \"application/json\"},\n            json={\n                \"personalizations\": [{\"to\": [{\"email\": e} for e in to]}],\n                \"from\": {\"email\": sender},\n                \"subject\": subject,\n                \"content\": [\n                    {\"type\": \"text/plain\", \"value\": body_text or \"\"},\n                    {\"type\": \"text/html\", \"value\": body_html or body_text},\n                ],\n            },\n            timeout=15,\n        )\n        resp.raise_for_status()\n        return {\"success\": True, \"message_id\": resp.headers.get(\"X-Message-Id\", \"\")}\n\n    # SMTP fallback\n    import smtplib\n    from email.mime.text import MIMEText\n    msg = MIMEText(body_html or body_text, \"html\" if body_html else \"plain\")\n    msg[\"Subject\"] = subject\n    msg[\"From\"] = sender\n    msg[\"To\"] = \", \".join(to)\n    with smtplib.SMTP(os.environ[\"SMTP_HOST\"], int(os.environ.get(\"SMTP_PORT\", 587))) as s:\n        s.starttls()\n        s.login(os.environ[\"SMTP_USER\"], os.environ[\"SMTP_PASS\"])\n        s.send_message(msg)\n    return {\"success\": True, \"message_id\": \"\"}\n```\n\n---\n\n## Git Operations\n\n### `git_commit_push`\n\n**Description:** Stages specified files (or all changes), creates a commit with a conventional-commit message, and optionally pushes to the remote -- the bread-and-butter of automated code updates.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo_path: str`, `message: str`, `files: list[str] = []`, `push: bool = True`, `branch: str = \"\"` |\n| **Output** | `dict` with `commit_sha: str`, `branch: str`, `pushed: bool` |\n| **Key Dependencies** | `subprocess` (stdlib), `git` CLI |\n| **MCP Alternative** | `@anthropic/git-mcp` server -- exposes `git_status`, `git_commit`, `git_push`, etc. |\n\n```python\nimport subprocess\n\ndef _git(repo: str, *args: str) -> str:\n    result = subprocess.run(\n        [\"git\", *args], cwd=repo,\n        capture_output=True, text=True, check=True,\n    )\n    return result.stdout.strip()\n\ndef main(repo_path: str, message: str, files: list[str] | None = None,\n         push: bool = True, branch: str = \"\") -> dict:\n    \"\"\"Stage, commit, and optionally push changes in a git repo.\"\"\"\n    if branch:\n        try:\n            _git(repo_path, \"checkout\", branch)\n        except subprocess.CalledProcessError:\n            _git(repo_path, \"checkout\", \"-b\", branch)\n\n    # Stage\n    if files:\n        for f in files:\n            _git(repo_path, \"add\", f)\n    else:\n        _git(repo_path, \"add\", \"-A\")\n\n    # Commit\n    _git(repo_path, \"commit\", \"-m\", message)\n    sha = _git(repo_path, \"rev-parse\", \"HEAD\")\n\n    current_branch = _git(repo_path, \"rev-parse\", \"--abbrev-ref\", \"HEAD\")\n\n    pushed = False\n    if push:\n        _git(repo_path, \"push\", \"-u\", \"origin\", current_branch)\n        pushed = True\n\n    return {\"commit_sha\": sha, \"branch\": current_branch, \"pushed\": pushed}\n```\n\n---\n\n### `git_branch_manage`\n\n**Description:** Creates, lists, switches, or deletes branches in a local repository, with optional remote tracking setup.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo_path: str`, `action: Literal[\"create\", \"switch\", \"delete\", \"list\"]`, `branch: str = \"\"`, `from_ref: str = \"main\"` |\n| **Output** | `dict` with `branch: str`, `action: str`, `branches: list[str]` (for list action) |\n| **Key Dependencies** | `subprocess` (stdlib), `git` CLI |\n| **MCP Alternative** | `@anthropic/git-mcp` |\n\n```python\nimport subprocess\n\ndef _git(repo: str, *args: str) -> str:\n    result = subprocess.run(\n        [\"git\", *args], cwd=repo,\n        capture_output=True, text=True, check=True,\n    )\n    return result.stdout.strip()\n\ndef main(repo_path: str, action: str = \"list\", branch: str = \"\",\n         from_ref: str = \"main\") -> dict:\n    \"\"\"Create, switch, delete, or list branches.\"\"\"\n\n    if action == \"list\":\n        raw = _git(repo_path, \"branch\", \"-a\")\n        branches = [b.strip().lstrip(\"* \") for b in raw.splitlines()]\n        return {\"action\": \"list\", \"branches\": branches, \"branch\": \"\"}\n\n    if not branch:\n        raise ValueError(\"branch name required for create/switch/delete\")\n\n    if action == \"create\":\n        _git(repo_path, \"checkout\", \"-b\", branch, from_ref)\n        return {\"action\": \"create\", \"branch\": branch, \"branches\": []}\n\n    if action == \"switch\":\n        _git(repo_path, \"checkout\", branch)\n        return {\"action\": \"switch\", \"branch\": branch, \"branches\": []}\n\n    if action == \"delete\":\n        _git(repo_path, \"branch\", \"-d\", branch)\n        return {\"action\": \"delete\", \"branch\": branch, \"branches\": []}\n\n    raise ValueError(f\"Unknown action: {action}\")\n```\n\n---\n\n### `git_read_repo`\n\n**Description:** Reads repository structure and file contents without requiring a local clone, using either a local path or the GitHub API for remote repos.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo: str` (local path or `owner/repo`), `path: str = \"\"`, `ref: str = \"main\"`, `depth: int = 2` |\n| **Output** | `dict` with `tree: list[dict]`, `file_content: str` (if path points to a file) |\n| **Key Dependencies** | `httpx`, `subprocess` (stdlib) |\n| **MCP Alternative** | `@anthropic/github-mcp` for remote repos |\n\n```python\nimport os, httpx, subprocess, pathlib\n\ndef main(repo: str, path: str = \"\", ref: str = \"main\",\n         depth: int = 2) -> dict:\n    \"\"\"Read repository tree or file contents.\"\"\"\n\n    # Local repo\n    if os.path.isdir(repo):\n        target = pathlib.Path(repo) / path\n        if target.is_file():\n            return {\"tree\": [], \"file_content\": target.read_text(encoding=\"utf-8\")}\n        tree = []\n        for item in sorted(target.rglob(\"*\")):\n            rel = item.relative_to(pathlib.Path(repo))\n            if len(rel.parts) <= depth and not any(p.startswith(\".\") for p in rel.parts):\n                tree.append({\n                    \"path\": str(rel), \"type\": \"dir\" if item.is_dir() else \"file\",\n                    \"size\": item.stat().st_size if item.is_file() else 0,\n                })\n        return {\"tree\": tree, \"file_content\": \"\"}\n\n    # Remote GitHub repo\n    headers = {\"Authorization\": f\"token {os.environ['GITHUB_TOKEN']}\",\n               \"Accept\": \"application/vnd.github.v3+json\"}\n    base = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n    resp = httpx.get(base, headers=headers, params={\"ref\": ref}, timeout=15)\n    resp.raise_for_status()\n    data = resp.json()\n\n    if isinstance(data, dict) and data.get(\"type\") == \"file\":\n        import base64\n        content = base64.b64decode(data[\"content\"]).decode(\"utf-8\")\n        return {\"tree\": [], \"file_content\": content}\n\n    tree = [{\"path\": item[\"path\"], \"type\": item[\"type\"], \"size\": item.get(\"size\", 0)}\n            for item in data]\n    return {\"tree\": tree, \"file_content\": \"\"}\n```\n\n---\n\n## GitHub API\n\n### `github_create_issue`\n\n**Description:** Creates a new issue on a GitHub repository with title, body, labels, assignees, and milestone, returning the full issue URL and number.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo: str` (`owner/repo`), `title: str`, `body: str = \"\"`, `labels: list[str] = []`, `assignees: list[str] = []`, `milestone: int = 0` |\n| **Output** | `dict` with `number: int`, `url: str`, `html_url: str` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | `@anthropic/github-mcp` -- exposes `create_issue` |\n\n```python\nimport os, httpx\n\ndef main(repo: str, title: str, body: str = \"\",\n         labels: list[str] | None = None,\n         assignees: list[str] | None = None,\n         milestone: int = 0) -> dict:\n    \"\"\"Create a GitHub issue.\"\"\"\n    headers = {\n        \"Authorization\": f\"token {os.environ['GITHUB_TOKEN']}\",\n        \"Accept\": \"application/vnd.github.v3+json\",\n    }\n    payload: dict = {\"title\": title, \"body\": body}\n    if labels:\n        payload[\"labels\"] = labels\n    if assignees:\n        payload[\"assignees\"] = assignees\n    if milestone:\n        payload[\"milestone\"] = milestone\n\n    resp = httpx.post(\n        f\"https://api.github.com/repos/{repo}/issues\",\n        headers=headers, json=payload, timeout=15,\n    )\n    resp.raise_for_status()\n    data = resp.json()\n    return {\n        \"number\": data[\"number\"],\n        \"url\": data[\"url\"],\n        \"html_url\": data[\"html_url\"],\n    }\n```\n\n---\n\n### `github_pull_request`\n\n**Description:** Creates or updates a pull request, with support for draft PRs, auto-merge labels, and reviewer assignment.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo: str`, `title: str`, `body: str`, `head: str`, `base: str = \"main\"`, `draft: bool = False`, `reviewers: list[str] = []` |\n| **Output** | `dict` with `number: int`, `html_url: str`, `state: str` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | `@anthropic/github-mcp` -- exposes `create_pull_request` |\n\n```python\nimport os, httpx\n\ndef main(repo: str, title: str, body: str, head: str,\n         base: str = \"main\", draft: bool = False,\n         reviewers: list[str] | None = None) -> dict:\n    \"\"\"Create a GitHub pull request.\"\"\"\n    headers = {\n        \"Authorization\": f\"token {os.environ['GITHUB_TOKEN']}\",\n        \"Accept\": \"application/vnd.github.v3+json\",\n    }\n    payload = {\n        \"title\": title, \"body\": body,\n        \"head\": head, \"base\": base, \"draft\": draft,\n    }\n\n    resp = httpx.post(\n        f\"https://api.github.com/repos/{repo}/pulls\",\n        headers=headers, json=payload, timeout=15,\n    )\n    resp.raise_for_status()\n    pr = resp.json()\n\n    # Request reviewers if specified\n    if reviewers:\n        httpx.post(\n            f\"https://api.github.com/repos/{repo}/pulls/{pr['number']}/requested_reviewers\",\n            headers=headers,\n            json={\"reviewers\": reviewers},\n            timeout=10,\n        )\n\n    return {\n        \"number\": pr[\"number\"],\n        \"html_url\": pr[\"html_url\"],\n        \"state\": pr[\"state\"],\n    }\n```\n\n---\n\n### `github_workflow_dispatch`\n\n**Description:** Triggers a GitHub Actions workflow via the repository dispatch or workflow_dispatch API, passing custom inputs and waiting optionally for completion status.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo: str`, `workflow_id: str` (filename or ID), `ref: str = \"main\"`, `inputs: dict = {}`, `wait: bool = False`, `timeout: int = 300` |\n| **Output** | `dict` with `triggered: bool`, `run_id: int`, `status: str`, `conclusion: str` |\n| **Key Dependencies** | `httpx`, `time` (stdlib) |\n| **MCP Alternative** | `@anthropic/github-mcp` |\n\n```python\nimport os, httpx, time\n\ndef main(repo: str, workflow_id: str, ref: str = \"main\",\n         inputs: dict | None = None, wait: bool = False,\n         timeout: int = 300) -> dict:\n    \"\"\"Trigger a GitHub Actions workflow and optionally wait for completion.\"\"\"\n    headers = {\n        \"Authorization\": f\"token {os.environ['GITHUB_TOKEN']}\",\n        \"Accept\": \"application/vnd.github.v3+json\",\n    }\n    # Trigger\n    resp = httpx.post(\n        f\"https://api.github.com/repos/{repo}/actions/workflows/{workflow_id}/dispatches\",\n        headers=headers,\n        json={\"ref\": ref, \"inputs\": inputs or {}},\n        timeout=15,\n    )\n    resp.raise_for_status()\n\n    if not wait:\n        return {\"triggered\": True, \"run_id\": 0, \"status\": \"queued\", \"conclusion\": \"\"}\n\n    # Poll for the new run\n    time.sleep(3)\n    deadline = time.time() + timeout\n    while time.time() < deadline:\n        runs_resp = httpx.get(\n            f\"https://api.github.com/repos/{repo}/actions/workflows/{workflow_id}/runs\",\n            headers=headers, params={\"branch\": ref, \"per_page\": 1},\n            timeout=15,\n        )\n        runs = runs_resp.json().get(\"workflow_runs\", [])\n        if runs:\n            run = runs[0]\n            if run[\"status\"] == \"completed\":\n                return {\n                    \"triggered\": True, \"run_id\": run[\"id\"],\n                    \"status\": run[\"status\"], \"conclusion\": run[\"conclusion\"],\n                }\n        time.sleep(10)\n\n    return {\"triggered\": True, \"run_id\": 0, \"status\": \"timeout\", \"conclusion\": \"\"}\n```\n\n---\n\n## Database Operations\n\n### `supabase_query`\n\n**Description:** Executes queries against a Supabase project using the PostgREST API -- supports select, insert, update, upsert, and delete with filter chaining.\n\n| Field | Detail |\n|---|---|\n| **Input** | `table: str`, `action: Literal[\"select\", \"insert\", \"update\", \"upsert\", \"delete\"]`, `data: dict | list[dict] = {}`, `filters: list[dict] = []`, `select_columns: str = \"*\"`, `limit: int = 100` |\n| **Output** | `dict` with `data: list[dict]`, `count: int`, `status: int` |\n| **Key Dependencies** | `supabase` (supabase-py) |\n| **MCP Alternative** | `@anthropic/supabase-mcp` server |\n\n```python\nimport os\nfrom supabase import create_client\n\ndef main(table: str, action: str = \"select\", data=None,\n         filters: list[dict] | None = None,\n         select_columns: str = \"*\", limit: int = 100) -> dict:\n    \"\"\"Execute a Supabase database operation.\"\"\"\n    client = create_client(\n        os.environ[\"SUPABASE_URL\"],\n        os.environ[\"SUPABASE_SERVICE_KEY\"],\n    )\n\n    query = client.table(table)\n\n    if action == \"select\":\n        query = query.select(select_columns)\n        for f in (filters or []):\n            query = getattr(query, f.get(\"op\", \"eq\"))(f[\"field\"], f[\"value\"])\n        result = query.limit(limit).execute()\n        return {\"data\": result.data, \"count\": len(result.data), \"status\": 200}\n\n    if action == \"insert\":\n        result = query.insert(data).execute()\n        return {\"data\": result.data, \"count\": len(result.data), \"status\": 201}\n\n    if action == \"upsert\":\n        result = query.upsert(data).execute()\n        return {\"data\": result.data, \"count\": len(result.data), \"status\": 200}\n\n    if action == \"update\":\n        q = query.update(data)\n        for f in (filters or []):\n            q = getattr(q, f.get(\"op\", \"eq\"))(f[\"field\"], f[\"value\"])\n        result = q.execute()\n        return {\"data\": result.data, \"count\": len(result.data), \"status\": 200}\n\n    if action == \"delete\":\n        q = query.delete()\n        for f in (filters or []):\n            q = getattr(q, f.get(\"op\", \"eq\"))(f[\"field\"], f[\"value\"])\n        result = q.execute()\n        return {\"data\": result.data, \"count\": len(result.data), \"status\": 200}\n\n    raise ValueError(f\"Unknown action: {action}\")\n```\n\n---\n\n### `sqlite_query`\n\n**Description:** Opens (or creates) a local SQLite database and executes arbitrary SQL, returning rows as dictionaries -- ideal for lightweight local state, caching, and intermediate pipeline storage.\n\n| Field | Detail |\n|---|---|\n| **Input** | `db_path: str`, `sql: str`, `params: list = []`, `mode: Literal[\"query\", \"execute\", \"script\"] = \"query\"` |\n| **Output** | `dict` with `rows: list[dict]`, `rowcount: int`, `lastrowid: int` |\n| **Key Dependencies** | `sqlite3` (stdlib) |\n| **MCP Alternative** | `@anthropic/sqlite-mcp` server |\n\n```python\nimport sqlite3\n\ndef main(db_path: str, sql: str, params: list | None = None,\n         mode: str = \"query\") -> dict:\n    \"\"\"Execute SQL against a SQLite database.\"\"\"\n    conn = sqlite3.connect(db_path)\n    conn.row_factory = sqlite3.Row\n    cur = conn.cursor()\n\n    try:\n        if mode == \"script\":\n            cur.executescript(sql)\n            conn.commit()\n            return {\"rows\": [], \"rowcount\": cur.rowcount, \"lastrowid\": cur.lastrowid}\n\n        if mode == \"execute\":\n            cur.execute(sql, params or [])\n            conn.commit()\n            return {\"rows\": [], \"rowcount\": cur.rowcount, \"lastrowid\": cur.lastrowid}\n\n        # query mode\n        cur.execute(sql, params or [])\n        rows = [dict(row) for row in cur.fetchall()]\n        return {\"rows\": rows, \"rowcount\": len(rows), \"lastrowid\": 0}\n\n    finally:\n        conn.close()\n```\n\n---\n\n### `airtable_ops`\n\n**Description:** Reads, creates, updates, or deletes records in an Airtable base, handling pagination and field-type coercion automatically.\n\n| Field | Detail |\n|---|---|\n| **Input** | `base_id: str`, `table_name: str`, `action: Literal[\"list\", \"create\", \"update\", \"delete\"]`, `records: list[dict] = []`, `filter_formula: str = \"\"`, `max_records: int = 100` |\n| **Output** | `dict` with `records: list[dict]`, `count: int` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | None standard; wrap as custom MCP tool |\n\n```python\nimport os, httpx\n\nBASE_URL = \"https://api.airtable.com/v0\"\n\ndef _headers():\n    return {\n        \"Authorization\": f\"Bearer {os.environ['AIRTABLE_API_KEY']}\",\n        \"Content-Type\": \"application/json\",\n    }\n\ndef main(base_id: str, table_name: str, action: str = \"list\",\n         records: list[dict] | None = None,\n         filter_formula: str = \"\", max_records: int = 100) -> dict:\n    \"\"\"Perform CRUD operations on Airtable records.\"\"\"\n    url = f\"{BASE_URL}/{base_id}/{table_name}\"\n\n    if action == \"list\":\n        params: dict = {\"maxRecords\": max_records}\n        if filter_formula:\n            params[\"filterByFormula\"] = filter_formula\n        all_records, offset = [], None\n        while True:\n            if offset:\n                params[\"offset\"] = offset\n            resp = httpx.get(url, headers=_headers(), params=params, timeout=15)\n            resp.raise_for_status()\n            data = resp.json()\n            all_records.extend(data.get(\"records\", []))\n            offset = data.get(\"offset\")\n            if not offset or len(all_records) >= max_records:\n                break\n        return {\"records\": all_records, \"count\": len(all_records)}\n\n    if action == \"create\":\n        payload = {\"records\": [{\"fields\": r} for r in (records or [])]}\n        resp = httpx.post(url, headers=_headers(), json=payload, timeout=15)\n        resp.raise_for_status()\n        created = resp.json().get(\"records\", [])\n        return {\"records\": created, \"count\": len(created)}\n\n    if action == \"update\":\n        payload = {\"records\": records or []}  # expects [{\"id\": ..., \"fields\": {...}}]\n        resp = httpx.patch(url, headers=_headers(), json=payload, timeout=15)\n        resp.raise_for_status()\n        updated = resp.json().get(\"records\", [])\n        return {\"records\": updated, \"count\": len(updated)}\n\n    if action == \"delete\":\n        ids = [r if isinstance(r, str) else r.get(\"id\") for r in (records or [])]\n        resp = httpx.delete(url, headers=_headers(),\n                            params={\"records[]\": ids}, timeout=15)\n        resp.raise_for_status()\n        return {\"records\": resp.json().get(\"records\", []),\n\n[Showing lines 1-1444 of 3559 (50.0KB limit). Use offset=1445 to continue.]"}],"details":{"truncation":{"content":"# WAT Systems Factory -- Reusable Tool Pattern Catalog\n\n> **Version:** 0.1.0-seed\n> **Maintainer:** WAT Systems Factory / Agent HQ\n> **Purpose:** Canonical reference of composable tool patterns that factory-generated workflows can assemble into pipelines. Each pattern is self-contained, testable, and convertible to an MCP tool or n8n node.\n\n---\n\n## Table of Contents\n\n1. [Web Scraping & Search](#web-scraping--search)\n2. [File I/O](#file-io)\n3. [API Calls](#api-calls)\n4. [Data Processing](#data-processing)\n5. [AI Processing](#ai-processing)\n6. [Notifications](#notifications)\n7. [Git Operations](#git-operations)\n8. [GitHub API](#github-api)\n9. [Database Operations](#database-operations)\n10. [Media Processing](#media-processing)\n11. [n8n Conversion](#n8n-conversion)\n12. [Agent Teams Coordination](#agent-teams-coordination)\n13. [GitHub Agent HQ Interaction](#github-agent-hq-interaction)\n\n---\n\n## Web Scraping & Search\n\n### `firecrawl_scrape`\n\n**Description:** Scrapes a single URL or crawls an entire domain via the Firecrawl API, returning clean Markdown-formatted content suitable for downstream LLM consumption.\n\n| Field | Detail |\n|---|---|\n| **Input** | `url: str`, `mode: Literal[\"scrape\", \"crawl\"]`, `max_pages: int = 50`, `selectors: list[str] = []` |\n| **Output** | `dict` with keys `markdown: str`, `metadata: dict`, `links: list[str]` |\n| **Key Dependencies** | `firecrawl-py`, `httpx` |\n| **MCP Alternative** | `@anthropic/firecrawl-mcp` server -- exposes `firecrawl_scrape` and `firecrawl_crawl` tools directly |\n\n```python\nimport os\nfrom firecrawl import FirecrawlApp\n\ndef main(url: str, mode: str = \"scrape\", max_pages: int = 50,\n         selectors: list[str] | None = None) -> dict:\n    \"\"\"Scrape or crawl a URL via Firecrawl and return Markdown content.\"\"\"\n    app = FirecrawlApp(api_key=os.environ[\"FIRECRAWL_API_KEY\"])\n\n    if mode == \"crawl\":\n        result = app.crawl_url(url, params={\n            \"limit\": max_pages,\n            \"scrapeOptions\": {\"formats\": [\"markdown\"]},\n        })\n        pages = [{\"markdown\": p.get(\"markdown\", \"\"), \"url\": p.get(\"url\")}\n                 for p in result.get(\"data\", [])]\n        return {\"pages\": pages, \"count\": len(pages)}\n\n    # Default: single-page scrape\n    result = app.scrape_url(url, params={\"formats\": [\"markdown\"]})\n    return {\n        \"markdown\": result.get(\"markdown\", \"\"),\n        \"metadata\": result.get(\"metadata\", {}),\n        \"links\": result.get(\"links\", []),\n    }\n```\n\n---\n\n### `brave_search`\n\n**Description:** Runs a web search query through the Brave Search API and returns structured result objects with titles, URLs, and snippets.\n\n| Field | Detail |\n|---|---|\n| **Input** | `query: str`, `count: int = 10`, `freshness: str = \"\"` |\n| **Output** | `list[dict]` each with `title`, `url`, `description`, `age` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | `@anthropic/brave-search-mcp` server -- exposes `brave_web_search` and `brave_local_search` |\n\n```python\nimport os, httpx\n\nBRAVE_ENDPOINT = \"https://api.search.brave.com/res/v1/web/search\"\n\ndef main(query: str, count: int = 10, freshness: str = \"\") -> list[dict]:\n    \"\"\"Execute a Brave web search and return structured results.\"\"\"\n    headers = {\n        \"Accept\": \"application/json\",\n        \"X-Subscription-Token\": os.environ[\"BRAVE_API_KEY\"],\n    }\n    params = {\"q\": query, \"count\": min(count, 20)}\n    if freshness:\n        params[\"freshness\"] = freshness  # e.g. \"pd\" (past day), \"pw\", \"pm\"\n\n    resp = httpx.get(BRAVE_ENDPOINT, headers=headers, params=params, timeout=15)\n    resp.raise_for_status()\n\n    results = []\n    for item in resp.json().get(\"web\", {}).get(\"results\", []):\n        results.append({\n            \"title\": item.get(\"title\"),\n            \"url\": item.get(\"url\"),\n            \"description\": item.get(\"description\"),\n            \"age\": item.get(\"age\", \"\"),\n        })\n    return results\n```\n\n---\n\n### `puppeteer_screenshot`\n\n**Description:** Launches a headless browser via Puppeteer (through its MCP server or a local Node bridge) to capture a screenshot or extract JavaScript-rendered DOM content from a page.\n\n| Field | Detail |\n|---|---|\n| **Input** | `url: str`, `action: Literal[\"screenshot\", \"extract\"]`, `selector: str = \"body\"`, `viewport: dict = {\"width\": 1280, \"height\": 720}` |\n| **Output** | `dict` with `base64_image: str` (for screenshot) or `html: str` (for extract) |\n| **Key Dependencies** | `playwright` (Python) or Node `puppeteer` via subprocess |\n| **MCP Alternative** | `@anthropic/puppeteer-mcp` server -- exposes `puppeteer_navigate`, `puppeteer_screenshot`, `puppeteer_click`, `puppeteer_fill` |\n\n```python\nfrom playwright.async_api import async_playwright\nimport asyncio, base64\n\nasync def _run(url: str, action: str, selector: str, viewport: dict) -> dict:\n    async with async_playwright() as pw:\n        browser = await pw.chromium.launch(headless=True)\n        page = await browser.new_page(viewport_size=viewport)\n        await page.goto(url, wait_until=\"networkidle\")\n\n        if action == \"screenshot\":\n            buf = await page.screenshot(full_page=True)\n            await browser.close()\n            return {\"base64_image\": base64.b64encode(buf).decode()}\n\n        content = await page.inner_html(selector)\n        await browser.close()\n        return {\"html\": content}\n\ndef main(url: str, action: str = \"screenshot\", selector: str = \"body\",\n         viewport: dict | None = None) -> dict:\n    \"\"\"Capture a screenshot or extract rendered HTML from a URL.\"\"\"\n    vp = viewport or {\"width\": 1280, \"height\": 720}\n    return asyncio.run(_run(url, action, selector, vp))\n```\n\n---\n\n## File I/O\n\n### `csv_read_write`\n\n**Description:** Reads a CSV file into a list of dictionaries or writes a list of dictionaries out to CSV, handling encoding detection and BOM stripping automatically.\n\n| Field | Detail |\n|---|---|\n| **Input** | `path: str`, `mode: Literal[\"read\", \"write\"]`, `data: list[dict] = []`, `encoding: str = \"utf-8-sig\"` |\n| **Output** | `list[dict]` on read; `dict` with `rows_written: int, path: str` on write |\n| **Key Dependencies** | `csv` (stdlib), `chardet` (optional encoding detection) |\n| **MCP Alternative** | `@anthropic/filesystem-mcp` for raw read/write; CSV parsing done in-tool |\n\n```python\nimport csv, io, pathlib\n\ndef main(path: str, mode: str = \"read\", data: list[dict] | None = None,\n         encoding: str = \"utf-8-sig\") -> dict | list[dict]:\n    \"\"\"Read or write CSV files with automatic encoding handling.\"\"\"\n    p = pathlib.Path(path)\n\n    if mode == \"read\":\n        text = p.read_text(encoding=encoding)\n        reader = csv.DictReader(io.StringIO(text))\n        return [dict(row) for row in reader]\n\n    # Write mode\n    if not data:\n        raise ValueError(\"data is required for write mode\")\n    fieldnames = list(data[0].keys())\n    with p.open(\"w\", newline=\"\", encoding=encoding) as f:\n        writer = csv.DictWriter(f, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(data)\n    return {\"rows_written\": len(data), \"path\": str(p.resolve())}\n```\n\n---\n\n### `json_read_write`\n\n**Description:** Reads a JSON or JSONL file into Python objects or writes Python objects out to JSON/JSONL, with optional pretty-printing and schema validation.\n\n| Field | Detail |\n|---|---|\n| **Input** | `path: str`, `mode: Literal[\"read\", \"write\"]`, `data: Any = None`, `jsonl: bool = False`, `indent: int = 2` |\n| **Output** | Parsed object(s) on read; `dict` with `path: str, size_bytes: int` on write |\n| **Key Dependencies** | `json` (stdlib), `jsonschema` (optional validation) |\n| **MCP Alternative** | `@anthropic/filesystem-mcp` for raw I/O |\n\n```python\nimport json, pathlib\n\ndef main(path: str, mode: str = \"read\", data=None,\n         jsonl: bool = False, indent: int = 2):\n    \"\"\"Read or write JSON / JSONL files.\"\"\"\n    p = pathlib.Path(path)\n\n    if mode == \"read\":\n        text = p.read_text(encoding=\"utf-8\")\n        if jsonl:\n            return [json.loads(line) for line in text.splitlines() if line.strip()]\n        return json.loads(text)\n\n    # Write mode\n    if jsonl:\n        lines = [json.dumps(item, ensure_ascii=False) for item in data]\n        p.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n    else:\n        p.write_text(json.dumps(data, indent=indent, ensure_ascii=False),\n                      encoding=\"utf-8\")\n    return {\"path\": str(p.resolve()), \"size_bytes\": p.stat().st_size}\n```\n\n---\n\n### `apollo_enrich`\n\n**Description:** Enriches a company name/domain with company data and decision-maker contacts via the Apollo.io API. Returns structured company info, tech stack, and key contacts.\n\n| Field | Detail |\n|---|---|\n| **Input** | `company_name: str`, `domain: str = None` |\n| **Output** | `dict` with company data, tech stack, decision makers |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | None; direct HTTP API |\n\n---\n\n### `hunter_email_search`\n\n**Description:** Searches for email addresses at a given domain via the Hunter.io API. Returns verified contacts with confidence scores.\n\n| Field | Detail |\n|---|---|\n| **Input** | `domain: str`, `limit: int = 5` |\n| **Output** | `dict` with emails, patterns, organization info |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | None; direct HTTP API |\n\n---\n\n### `multi_dimension_scorer`\n\n**Description:** Scores a data record across N independent dimensions with configurable point values. Used for lead scoring, content quality assessment, or any multi-criteria evaluation.\n\n| Field | Detail |\n|---|---|\n| **Input** | `record: dict`, `dimensions: list[dict]` (each with name, max_points, scoring_function) |\n| **Output** | `dict` with dimension breakdown and total score |\n| **Key Dependencies** | None (stdlib only) |\n| **MCP Alternative** | None |\n\n---\n\n### `email_sequence_generator`\n\n**Description:** Generates a multi-email sequence (outreach or nurture) using Claude API. Takes company/audience context and a sequence template, returns structured JSON with subject lines, bodies, and send timing.\n\n| Field | Detail |\n|---|---|\n| **Input** | `context: dict`, `sequence_template: str`, `email_count: int` |\n| **Output** | `dict` with sequence of emails (subject, body, send_day) |\n| **Key Dependencies** | `anthropic` |\n| **MCP Alternative** | Anthropic MCP |\n\n---\n\n### `pdf_extract`\n\n**Description:** Extracts text content, metadata, and optionally tables from a PDF file, returning structured output ready for LLM ingestion or data-pipeline processing.\n\n| Field | Detail |\n|---|---|\n| **Input** | `path: str`, `pages: str = \"all\"`, `extract_tables: bool = False` |\n| **Output** | `dict` with `text: str`, `pages: list[dict]`, `metadata: dict`, `tables: list` |\n| **Key Dependencies** | `pymupdf` (fitz), `pdfplumber` (for tables) |\n| **MCP Alternative** | None standard; wrap as custom MCP tool |\n\n```python\nimport fitz  # pymupdf\n\ndef main(path: str, pages: str = \"all\", extract_tables: bool = False) -> dict:\n    \"\"\"Extract text and metadata from a PDF file.\"\"\"\n    doc = fitz.open(path)\n    metadata = dict(doc.metadata)\n\n    page_range = range(len(doc))\n    if pages != \"all\":\n        # Parse ranges like \"1-5\" or \"3,7,10-12\"\n        page_range = _parse_page_range(pages, len(doc))\n\n    extracted_pages = []\n    for i in page_range:\n        page = doc[i]\n        extracted_pages.append({\n            \"page_number\": i + 1,\n            \"text\": page.get_text(\"text\"),\n        })\n\n    result = {\n        \"text\": \"\\n\\n\".join(p[\"text\"] for p in extracted_pages),\n        \"pages\": extracted_pages,\n        \"metadata\": metadata,\n    }\n\n    if extract_tables:\n        import pdfplumber\n        with pdfplumber.open(path) as pdf:\n            result[\"tables\"] = []\n            for i in page_range:\n                for table in pdf.pages[i].extract_tables():\n                    result[\"tables\"].append({\"page\": i + 1, \"rows\": table})\n\n    return result\n\ndef _parse_page_range(spec: str, total: int) -> list[int]:\n    indices = []\n    for part in spec.split(\",\"):\n        if \"-\" in part:\n            start, end = part.split(\"-\", 1)\n            indices.extend(range(int(start) - 1, min(int(end), total)))\n        else:\n            indices.append(int(part) - 1)\n    return [i for i in indices if 0 <= i < total]\n```\n\n---\n\n## API Calls\n\n### `rest_client`\n\n**Description:** A generic REST API client with configurable retry logic, exponential backoff, header injection, and response normalization -- the universal building block for any HTTP-based integration.\n\n| Field | Detail |\n|---|---|\n| **Input** | `url: str`, `method: str = \"GET\"`, `headers: dict = {}`, `body: Any = None`, `retries: int = 3`, `backoff: float = 1.0`, `timeout: int = 30` |\n| **Output** | `dict` with `status: int`, `headers: dict`, `body: Any`, `elapsed_ms: float` |\n| **Key Dependencies** | `httpx`, `tenacity` |\n| **MCP Alternative** | `@anthropic/fetch-mcp` for simple GET/POST |\n\n```python\nimport httpx, time\nfrom tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception\n\ndef _is_retryable(exc):\n    if isinstance(exc, httpx.HTTPStatusError):\n        return exc.response.status_code in (429, 500, 502, 503, 504)\n    return isinstance(exc, (httpx.ConnectTimeout, httpx.ReadTimeout))\n\ndef main(url: str, method: str = \"GET\", headers: dict | None = None,\n         body=None, retries: int = 3, backoff: float = 1.0,\n         timeout: int = 30) -> dict:\n    \"\"\"Make an HTTP request with retry logic and exponential backoff.\"\"\"\n\n    @retry(\n        stop=stop_after_attempt(retries),\n        wait=wait_exponential(multiplier=backoff, min=1, max=30),\n        retry=retry_if_exception(_is_retryable),\n        reraise=True,\n    )\n    def _do_request():\n        t0 = time.monotonic()\n        resp = httpx.request(\n            method=method.upper(),\n            url=url,\n            headers=headers or {},\n            json=body if isinstance(body, (dict, list)) else None,\n            content=body if isinstance(body, (str, bytes)) else None,\n            timeout=timeout,\n        )\n        elapsed = (time.monotonic() - t0) * 1000\n        resp.raise_for_status()\n        try:\n            resp_body = resp.json()\n        except Exception:\n            resp_body = resp.text\n        return {\n            \"status\": resp.status_code,\n            \"headers\": dict(resp.headers),\n            \"body\": resp_body,\n            \"elapsed_ms\": round(elapsed, 2),\n        }\n\n    return _do_request()\n```\n\n---\n\n### `graphql_client`\n\n**Description:** Sends GraphQL queries and mutations to any endpoint, with variable substitution, error extraction, and the same retry semantics as `rest_client`.\n\n| Field | Detail |\n|---|---|\n| **Input** | `endpoint: str`, `query: str`, `variables: dict = {}`, `headers: dict = {}`, `retries: int = 3` |\n| **Output** | `dict` with `data: Any`, `errors: list | None`, `status: int` |\n| **Key Dependencies** | `httpx`, `tenacity` |\n| **MCP Alternative** | None standard; wrap as custom MCP tool |\n\n```python\nimport httpx\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\ndef main(endpoint: str, query: str, variables: dict | None = None,\n         headers: dict | None = None, retries: int = 3) -> dict:\n    \"\"\"Execute a GraphQL query or mutation with retry logic.\"\"\"\n\n    @retry(stop=stop_after_attempt(retries),\n           wait=wait_exponential(multiplier=1, min=1, max=15))\n    def _execute():\n        payload = {\"query\": query}\n        if variables:\n            payload[\"variables\"] = variables\n        resp = httpx.post(\n            endpoint,\n            json=payload,\n            headers={\"Content-Type\": \"application/json\", **(headers or {})},\n            timeout=30,\n        )\n        resp.raise_for_status()\n        result = resp.json()\n        return {\n            \"data\": result.get(\"data\"),\n            \"errors\": result.get(\"errors\"),\n            \"status\": resp.status_code,\n        }\n\n    return _execute()\n```\n\n---\n\n### `oauth_token_refresh`\n\n**Description:** Manages OAuth 2.0 token lifecycle -- obtains tokens via client_credentials or authorization_code grants, caches them, and transparently refreshes before expiry.\n\n| Field | Detail |\n|---|---|\n| **Input** | `token_url: str`, `client_id: str`, `client_secret: str`, `grant_type: str = \"client_credentials\"`, `refresh_token: str = \"\"`, `scopes: list[str] = []` |\n| **Output** | `dict` with `access_token: str`, `expires_in: int`, `token_type: str` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | None; typically embedded within other tools |\n\n```python\nimport httpx, time\n\n_token_cache: dict = {}\n\ndef main(token_url: str, client_id: str, client_secret: str,\n         grant_type: str = \"client_credentials\",\n         refresh_token: str = \"\", scopes: list[str] | None = None) -> dict:\n    \"\"\"Obtain or refresh an OAuth 2.0 access token.\"\"\"\n    cache_key = f\"{token_url}:{client_id}\"\n\n    # Return cached token if still valid\n    cached = _token_cache.get(cache_key)\n    if cached and cached[\"expires_at\"] > time.time() + 60:\n        return {\n            \"access_token\": cached[\"access_token\"],\n            \"expires_in\": int(cached[\"expires_at\"] - time.time()),\n            \"token_type\": cached[\"token_type\"],\n        }\n\n    data = {\"grant_type\": grant_type, \"client_id\": client_id,\n            \"client_secret\": client_secret}\n    if refresh_token:\n        data[\"refresh_token\"] = refresh_token\n    if scopes:\n        data[\"scope\"] = \" \".join(scopes)\n\n    resp = httpx.post(token_url, data=data, timeout=15)\n    resp.raise_for_status()\n    token_data = resp.json()\n\n    _token_cache[cache_key] = {\n        \"access_token\": token_data[\"access_token\"],\n        \"token_type\": token_data.get(\"token_type\", \"Bearer\"),\n        \"expires_at\": time.time() + token_data.get(\"expires_in\", 3600),\n    }\n    return {\n        \"access_token\": token_data[\"access_token\"],\n        \"expires_in\": token_data.get(\"expires_in\", 3600),\n        \"token_type\": token_data.get(\"token_type\", \"Bearer\"),\n    }\n```\n\n---\n\n## Data Processing\n\n### `filter_sort`\n\n**Description:** Applies filter predicates and multi-key sorting to a list of dictionaries, supporting comparison operators, regex matching, and null handling.\n\n| Field | Detail |\n|---|---|\n| **Input** | `data: list[dict]`, `filters: list[dict]` (each with `field`, `op`, `value`), `sort_by: list[dict]` (each with `field`, `order`) |\n| **Output** | `list[dict]` -- filtered and sorted records |\n| **Key Dependencies** | `re` (stdlib) |\n| **MCP Alternative** | None; pure logic tool |\n\n```python\nimport re, operator\n\nOPS = {\n    \"eq\": operator.eq, \"ne\": operator.ne,\n    \"gt\": operator.gt, \"ge\": operator.ge,\n    \"lt\": operator.lt, \"le\": operator.le,\n    \"contains\": lambda a, b: b in str(a),\n    \"regex\": lambda a, b: bool(re.search(b, str(a))),\n    \"in\": lambda a, b: a in b,\n}\n\ndef main(data: list[dict], filters: list[dict] | None = None,\n         sort_by: list[dict] | None = None) -> list[dict]:\n    \"\"\"Filter and sort a list of dictionaries.\"\"\"\n    result = data\n\n    # Apply filters\n    for f in (filters or []):\n        op_fn = OPS.get(f[\"op\"], operator.eq)\n        result = [row for row in result if op_fn(row.get(f[\"field\"]), f[\"value\"])]\n\n    # Apply sorting\n    if sort_by:\n        for s in reversed(sort_by):\n            reverse = s.get(\"order\", \"asc\") == \"desc\"\n            result.sort(key=lambda row: (row.get(s[\"field\"]) is None,\n                                          row.get(s[\"field\"], \"\")),\n                        reverse=reverse)\n\n    return result\n```\n\n---\n\n### `dedup`\n\n**Description:** Removes duplicate records from a dataset based on one or more key fields, with configurable strategies for which duplicate to keep (first, last, or merge).\n\n| Field | Detail |\n|---|---|\n| **Input** | `data: list[dict]`, `keys: list[str]`, `strategy: Literal[\"first\", \"last\", \"merge\"] = \"first\"` |\n| **Output** | `dict` with `unique: list[dict]`, `duplicates_removed: int` |\n| **Key Dependencies** | None (stdlib only) |\n| **MCP Alternative** | None; pure logic tool |\n\n```python\ndef main(data: list[dict], keys: list[str],\n         strategy: str = \"first\") -> dict:\n    \"\"\"Deduplicate records by key fields.\"\"\"\n    seen: dict[tuple, dict] = {}\n    order: list[tuple] = []\n\n    for row in data:\n        key = tuple(row.get(k) for k in keys)\n        if key not in seen:\n            seen[key] = row\n            order.append(key)\n        elif strategy == \"last\":\n            seen[key] = row\n        elif strategy == \"merge\":\n            # Merge: later values fill in blanks only\n            for k, v in row.items():\n                if seen[key].get(k) in (None, \"\"):\n                    seen[key][k] = v\n\n    unique = [seen[k] for k in order]\n    return {\n        \"unique\": unique,\n        \"duplicates_removed\": len(data) - len(unique),\n    }\n```\n\n---\n\n### `transform_map`\n\n**Description:** Applies field-level transformations to every record in a dataset -- renaming columns, computing derived fields, type-casting, and applying template expressions.\n\n| Field | Detail |\n|---|---|\n| **Input** | `data: list[dict]`, `mappings: list[dict]` (each with `source`, `target`, `transform`), `drop_unmapped: bool = False` |\n| **Output** | `list[dict]` -- transformed records |\n| **Key Dependencies** | None (stdlib only) |\n| **MCP Alternative** | None; pure logic tool |\n\n```python\nfrom datetime import datetime\n\nTRANSFORMS = {\n    \"upper\": lambda v: str(v).upper(),\n    \"lower\": lambda v: str(v).lower(),\n    \"strip\": lambda v: str(v).strip(),\n    \"int\": lambda v: int(v) if v not in (None, \"\") else None,\n    \"float\": lambda v: float(v) if v not in (None, \"\") else None,\n    \"isodate\": lambda v: datetime.fromisoformat(str(v)).isoformat(),\n    \"bool\": lambda v: str(v).lower() in (\"true\", \"1\", \"yes\"),\n    \"default\": lambda v: v,\n}\n\ndef main(data: list[dict], mappings: list[dict],\n         drop_unmapped: bool = False) -> list[dict]:\n    \"\"\"Apply field-level transformations to each record.\"\"\"\n    results = []\n    for row in data:\n        new_row = {} if drop_unmapped else dict(row)\n        for m in mappings:\n            source_val = row.get(m[\"source\"])\n            transform_fn = TRANSFORMS.get(m.get(\"transform\", \"default\"),\n                                           TRANSFORMS[\"default\"])\n            new_row[m[\"target\"]] = transform_fn(source_val)\n        results.append(new_row)\n    return results\n```\n\n---\n\n## AI Processing\n\n### `llm_prompt`\n\n**Description:** Sends a prompt to an LLM provider (OpenAI, Anthropic, or compatible endpoint) with system instructions, returning the raw text completion and token usage metadata.\n\n| Field | Detail |\n|---|---|\n| **Input** | `prompt: str`, `system: str = \"\"`, `provider: Literal[\"openai\", \"anthropic\"] = \"anthropic\"`, `model: str = \"\"`, `max_tokens: int = 4096`, `temperature: float = 0.3` |\n| **Output** | `dict` with `text: str`, `model: str`, `usage: dict` |\n| **Key Dependencies** | `anthropic`, `openai` |\n| **MCP Alternative** | None; this IS the core LLM call |\n\n```python\nimport os\n\ndef main(prompt: str, system: str = \"\", provider: str = \"anthropic\",\n         model: str = \"\", max_tokens: int = 4096,\n         temperature: float = 0.3) -> dict:\n    \"\"\"Send a prompt to an LLM and return the completion.\"\"\"\n\n    if provider == \"anthropic\":\n        import anthropic\n        client = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n        resolved_model = model or \"claude-sonnet-4-20250514\"\n        msg = client.messages.create(\n            model=resolved_model,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            system=system or \"You are a helpful assistant.\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n        )\n        return {\n            \"text\": msg.content[0].text,\n            \"model\": resolved_model,\n            \"usage\": {\"input_tokens\": msg.usage.input_tokens,\n                      \"output_tokens\": msg.usage.output_tokens},\n        }\n\n    # OpenAI / compatible\n    import openai\n    client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n    resolved_model = model or \"gpt-4o\"\n    messages = []\n    if system:\n        messages.append({\"role\": \"system\", \"content\": system})\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n    resp = client.chat.completions.create(\n        model=resolved_model, messages=messages,\n        max_tokens=max_tokens, temperature=temperature,\n    )\n    choice = resp.choices[0]\n    return {\n        \"text\": choice.message.content,\n        \"model\": resolved_model,\n        \"usage\": {\"input_tokens\": resp.usage.prompt_tokens,\n                  \"output_tokens\": resp.usage.completion_tokens},\n    }\n```\n\n---\n\n### `structured_extract`\n\n**Description:** Sends content to an LLM with a JSON schema instruction, parses the response into a validated Python object, and retries with corrective prompts if the response fails schema validation.\n\n| Field | Detail |\n|---|---|\n| **Input** | `content: str`, `schema: dict` (JSON Schema), `instructions: str = \"\"`, `retries: int = 2` |\n| **Output** | `dict` with `data: Any` (validated), `raw: str`, `attempts: int` |\n| **Key Dependencies** | `anthropic` or `openai`, `jsonschema` |\n| **MCP Alternative** | None; core LLM extraction pattern |\n\n```python\nimport json, os\nfrom jsonschema import validate, ValidationError\n\ndef main(content: str, schema: dict, instructions: str = \"\",\n         retries: int = 2) -> dict:\n    \"\"\"Extract structured data from content using an LLM with schema validation.\"\"\"\n    import anthropic\n    client = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n\n    system_prompt = (\n        \"Extract information from the provided content and return ONLY valid JSON \"\n        \"matching the schema below. No markdown fences, no commentary.\\n\\n\"\n        f\"Schema:\\n```json\\n{json.dumps(schema, indent=2)}\\n```\"\n    )\n    if instructions:\n        system_prompt += f\"\\n\\nAdditional instructions: {instructions}\"\n\n    user_content = content\n    for attempt in range(1, retries + 2):\n        msg = client.messages.create(\n            model=\"claude-sonnet-4-20250514\",\n            max_tokens=4096,\n            temperature=0.0,\n            system=system_prompt,\n            messages=[{\"role\": \"user\", \"content\": user_content}],\n        )\n        raw = msg.content[0].text.strip()\n\n        # Strip markdown fences if present\n        if raw.startswith(\"```\"):\n            raw = raw.split(\"\\n\", 1)[1].rsplit(\"```\", 1)[0].strip()\n\n        try:\n            parsed = json.loads(raw)\n            validate(instance=parsed, schema=schema)\n            return {\"data\": parsed, \"raw\": raw, \"attempts\": attempt}\n        except (json.JSONDecodeError, ValidationError) as exc:\n            if attempt > retries:\n                raise ValueError(f\"Failed after {attempt} attempts: {exc}\") from exc\n            user_content = (\n                f\"Your previous response was invalid: {exc}\\n\\n\"\n                f\"Original content:\\n{content}\\n\\n\"\n                \"Please try again, returning ONLY valid JSON.\"\n            )\n\n    raise RuntimeError(\"Unreachable\")\n```\n\n---\n\n### `embedding_generate`\n\n**Description:** Generates vector embeddings for one or more text inputs using OpenAI, Voyage, or a local model, returning arrays ready for storage in a vector database.\n\n| Field | Detail |\n|---|---|\n| **Input** | `texts: list[str]`, `provider: Literal[\"openai\", \"voyage\"] = \"openai\"`, `model: str = \"\"` |\n| **Output** | `dict` with `embeddings: list[list[float]]`, `model: str`, `dimensions: int` |\n| **Key Dependencies** | `openai` or `voyageai` |\n| **MCP Alternative** | None standard |\n\n```python\nimport os\n\ndef main(texts: list[str], provider: str = \"openai\",\n         model: str = \"\") -> dict:\n    \"\"\"Generate vector embeddings for text inputs.\"\"\"\n\n    if provider == \"openai\":\n        import openai\n        client = openai.OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n        resolved = model or \"text-embedding-3-small\"\n        resp = client.embeddings.create(input=texts, model=resolved)\n        vecs = [item.embedding for item in resp.data]\n        return {\n            \"embeddings\": vecs,\n            \"model\": resolved,\n            \"dimensions\": len(vecs[0]) if vecs else 0,\n        }\n\n    if provider == \"voyage\":\n        import voyageai\n        client = voyageai.Client(api_key=os.environ[\"VOYAGE_API_KEY\"])\n        resolved = model or \"voyage-2\"\n        resp = client.embed(texts, model=resolved)\n        return {\n            \"embeddings\": resp.embeddings,\n            \"model\": resolved,\n            \"dimensions\": len(resp.embeddings[0]) if resp.embeddings else 0,\n        }\n\n    raise ValueError(f\"Unsupported provider: {provider}\")\n```\n\n---\n\n## Notifications\n\n### `slack_notify`\n\n**Description:** Sends a message to a Slack channel or thread via the Slack Web API or an incoming webhook, supporting Block Kit for rich formatting.\n\n| Field | Detail |\n|---|---|\n| **Input** | `channel: str`, `text: str`, `blocks: list[dict] = []`, `thread_ts: str = \"\"`, `use_webhook: bool = False` |\n| **Output** | `dict` with `ok: bool`, `ts: str`, `channel: str` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | `@anthropic/slack-mcp` server -- exposes `slack_post_message`, `slack_list_channels`, etc. |\n\n```python\nimport os, httpx\n\ndef main(channel: str, text: str, blocks: list[dict] | None = None,\n         thread_ts: str = \"\", use_webhook: bool = False) -> dict:\n    \"\"\"Send a Slack message to a channel or thread.\"\"\"\n\n    if use_webhook:\n        webhook_url = os.environ[\"SLACK_WEBHOOK_URL\"]\n        payload = {\"text\": text}\n        if blocks:\n            payload[\"blocks\"] = blocks\n        resp = httpx.post(webhook_url, json=payload, timeout=10)\n        resp.raise_for_status()\n        return {\"ok\": True, \"ts\": \"\", \"channel\": channel}\n\n    token = os.environ[\"SLACK_BOT_TOKEN\"]\n    payload = {\"channel\": channel, \"text\": text}\n    if blocks:\n        payload[\"blocks\"] = blocks\n    if thread_ts:\n        payload[\"thread_ts\"] = thread_ts\n\n    resp = httpx.post(\n        \"https://slack.com/api/chat.postMessage\",\n        json=payload,\n        headers={\"Authorization\": f\"Bearer {token}\",\n                 \"Content-Type\": \"application/json\"},\n        timeout=10,\n    )\n    data = resp.json()\n    return {\"ok\": data.get(\"ok\", False), \"ts\": data.get(\"ts\", \"\"),\n            \"channel\": data.get(\"channel\", channel)}\n```\n\n---\n\n### `discord_webhook`\n\n**Description:** Posts a message or embed to a Discord channel via a webhook URL, with support for rich embeds, username overrides, and file attachments.\n\n| Field | Detail |\n|---|---|\n| **Input** | `webhook_url: str`, `content: str = \"\"`, `embeds: list[dict] = []`, `username: str = \"\"` |\n| **Output** | `dict` with `status: int`, `success: bool` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | None standard; wrap as custom MCP tool |\n\n```python\nimport httpx\n\ndef main(webhook_url: str, content: str = \"\", embeds: list[dict] | None = None,\n         username: str = \"\") -> dict:\n    \"\"\"Send a message or embed to Discord via webhook.\"\"\"\n    payload: dict = {}\n    if content:\n        payload[\"content\"] = content\n    if embeds:\n        payload[\"embeds\"] = embeds\n    if username:\n        payload[\"username\"] = username\n\n    if not payload:\n        raise ValueError(\"Must provide content or embeds\")\n\n    resp = httpx.post(webhook_url, json=payload, timeout=10)\n    return {\n        \"status\": resp.status_code,\n        \"success\": resp.status_code in (200, 204),\n    }\n```\n\n---\n\n### `email_send`\n\n**Description:** Sends an email via SMTP or a transactional email API (SendGrid, Resend), supporting HTML body, plain-text fallback, and file attachments.\n\n| Field | Detail |\n|---|---|\n| **Input** | `to: list[str]`, `subject: str`, `body_html: str = \"\"`, `body_text: str = \"\"`, `provider: Literal[\"smtp\", \"sendgrid\", \"resend\"] = \"resend\"`, `from_addr: str = \"\"` |\n| **Output** | `dict` with `success: bool`, `message_id: str` |\n| **Key Dependencies** | `httpx`, `smtplib` (stdlib) |\n| **MCP Alternative** | None standard |\n\n```python\nimport os, httpx\n\ndef main(to: list[str], subject: str, body_html: str = \"\",\n         body_text: str = \"\", provider: str = \"resend\",\n         from_addr: str = \"\") -> dict:\n    \"\"\"Send an email via a transactional provider or SMTP.\"\"\"\n    sender = from_addr or os.environ.get(\"EMAIL_FROM\", \"noreply@example.com\")\n\n    if provider == \"resend\":\n        resp = httpx.post(\n            \"https://api.resend.com/emails\",\n            headers={\"Authorization\": f\"Bearer {os.environ['RESEND_API_KEY']}\",\n                     \"Content-Type\": \"application/json\"},\n            json={\n                \"from\": sender, \"to\": to, \"subject\": subject,\n                \"html\": body_html or f\"<pre>{body_text}</pre>\",\n                \"text\": body_text or \"\",\n            },\n            timeout=15,\n        )\n        resp.raise_for_status()\n        data = resp.json()\n        return {\"success\": True, \"message_id\": data.get(\"id\", \"\")}\n\n    if provider == \"sendgrid\":\n        resp = httpx.post(\n            \"https://api.sendgrid.com/v3/mail/send\",\n            headers={\"Authorization\": f\"Bearer {os.environ['SENDGRID_API_KEY']}\",\n                     \"Content-Type\": \"application/json\"},\n            json={\n                \"personalizations\": [{\"to\": [{\"email\": e} for e in to]}],\n                \"from\": {\"email\": sender},\n                \"subject\": subject,\n                \"content\": [\n                    {\"type\": \"text/plain\", \"value\": body_text or \"\"},\n                    {\"type\": \"text/html\", \"value\": body_html or body_text},\n                ],\n            },\n            timeout=15,\n        )\n        resp.raise_for_status()\n        return {\"success\": True, \"message_id\": resp.headers.get(\"X-Message-Id\", \"\")}\n\n    # SMTP fallback\n    import smtplib\n    from email.mime.text import MIMEText\n    msg = MIMEText(body_html or body_text, \"html\" if body_html else \"plain\")\n    msg[\"Subject\"] = subject\n    msg[\"From\"] = sender\n    msg[\"To\"] = \", \".join(to)\n    with smtplib.SMTP(os.environ[\"SMTP_HOST\"], int(os.environ.get(\"SMTP_PORT\", 587))) as s:\n        s.starttls()\n        s.login(os.environ[\"SMTP_USER\"], os.environ[\"SMTP_PASS\"])\n        s.send_message(msg)\n    return {\"success\": True, \"message_id\": \"\"}\n```\n\n---\n\n## Git Operations\n\n### `git_commit_push`\n\n**Description:** Stages specified files (or all changes), creates a commit with a conventional-commit message, and optionally pushes to the remote -- the bread-and-butter of automated code updates.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo_path: str`, `message: str`, `files: list[str] = []`, `push: bool = True`, `branch: str = \"\"` |\n| **Output** | `dict` with `commit_sha: str`, `branch: str`, `pushed: bool` |\n| **Key Dependencies** | `subprocess` (stdlib), `git` CLI |\n| **MCP Alternative** | `@anthropic/git-mcp` server -- exposes `git_status`, `git_commit`, `git_push`, etc. |\n\n```python\nimport subprocess\n\ndef _git(repo: str, *args: str) -> str:\n    result = subprocess.run(\n        [\"git\", *args], cwd=repo,\n        capture_output=True, text=True, check=True,\n    )\n    return result.stdout.strip()\n\ndef main(repo_path: str, message: str, files: list[str] | None = None,\n         push: bool = True, branch: str = \"\") -> dict:\n    \"\"\"Stage, commit, and optionally push changes in a git repo.\"\"\"\n    if branch:\n        try:\n            _git(repo_path, \"checkout\", branch)\n        except subprocess.CalledProcessError:\n            _git(repo_path, \"checkout\", \"-b\", branch)\n\n    # Stage\n    if files:\n        for f in files:\n            _git(repo_path, \"add\", f)\n    else:\n        _git(repo_path, \"add\", \"-A\")\n\n    # Commit\n    _git(repo_path, \"commit\", \"-m\", message)\n    sha = _git(repo_path, \"rev-parse\", \"HEAD\")\n\n    current_branch = _git(repo_path, \"rev-parse\", \"--abbrev-ref\", \"HEAD\")\n\n    pushed = False\n    if push:\n        _git(repo_path, \"push\", \"-u\", \"origin\", current_branch)\n        pushed = True\n\n    return {\"commit_sha\": sha, \"branch\": current_branch, \"pushed\": pushed}\n```\n\n---\n\n### `git_branch_manage`\n\n**Description:** Creates, lists, switches, or deletes branches in a local repository, with optional remote tracking setup.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo_path: str`, `action: Literal[\"create\", \"switch\", \"delete\", \"list\"]`, `branch: str = \"\"`, `from_ref: str = \"main\"` |\n| **Output** | `dict` with `branch: str`, `action: str`, `branches: list[str]` (for list action) |\n| **Key Dependencies** | `subprocess` (stdlib), `git` CLI |\n| **MCP Alternative** | `@anthropic/git-mcp` |\n\n```python\nimport subprocess\n\ndef _git(repo: str, *args: str) -> str:\n    result = subprocess.run(\n        [\"git\", *args], cwd=repo,\n        capture_output=True, text=True, check=True,\n    )\n    return result.stdout.strip()\n\ndef main(repo_path: str, action: str = \"list\", branch: str = \"\",\n         from_ref: str = \"main\") -> dict:\n    \"\"\"Create, switch, delete, or list branches.\"\"\"\n\n    if action == \"list\":\n        raw = _git(repo_path, \"branch\", \"-a\")\n        branches = [b.strip().lstrip(\"* \") for b in raw.splitlines()]\n        return {\"action\": \"list\", \"branches\": branches, \"branch\": \"\"}\n\n    if not branch:\n        raise ValueError(\"branch name required for create/switch/delete\")\n\n    if action == \"create\":\n        _git(repo_path, \"checkout\", \"-b\", branch, from_ref)\n        return {\"action\": \"create\", \"branch\": branch, \"branches\": []}\n\n    if action == \"switch\":\n        _git(repo_path, \"checkout\", branch)\n        return {\"action\": \"switch\", \"branch\": branch, \"branches\": []}\n\n    if action == \"delete\":\n        _git(repo_path, \"branch\", \"-d\", branch)\n        return {\"action\": \"delete\", \"branch\": branch, \"branches\": []}\n\n    raise ValueError(f\"Unknown action: {action}\")\n```\n\n---\n\n### `git_read_repo`\n\n**Description:** Reads repository structure and file contents without requiring a local clone, using either a local path or the GitHub API for remote repos.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo: str` (local path or `owner/repo`), `path: str = \"\"`, `ref: str = \"main\"`, `depth: int = 2` |\n| **Output** | `dict` with `tree: list[dict]`, `file_content: str` (if path points to a file) |\n| **Key Dependencies** | `httpx`, `subprocess` (stdlib) |\n| **MCP Alternative** | `@anthropic/github-mcp` for remote repos |\n\n```python\nimport os, httpx, subprocess, pathlib\n\ndef main(repo: str, path: str = \"\", ref: str = \"main\",\n         depth: int = 2) -> dict:\n    \"\"\"Read repository tree or file contents.\"\"\"\n\n    # Local repo\n    if os.path.isdir(repo):\n        target = pathlib.Path(repo) / path\n        if target.is_file():\n            return {\"tree\": [], \"file_content\": target.read_text(encoding=\"utf-8\")}\n        tree = []\n        for item in sorted(target.rglob(\"*\")):\n            rel = item.relative_to(pathlib.Path(repo))\n            if len(rel.parts) <= depth and not any(p.startswith(\".\") for p in rel.parts):\n                tree.append({\n                    \"path\": str(rel), \"type\": \"dir\" if item.is_dir() else \"file\",\n                    \"size\": item.stat().st_size if item.is_file() else 0,\n                })\n        return {\"tree\": tree, \"file_content\": \"\"}\n\n    # Remote GitHub repo\n    headers = {\"Authorization\": f\"token {os.environ['GITHUB_TOKEN']}\",\n               \"Accept\": \"application/vnd.github.v3+json\"}\n    base = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n    resp = httpx.get(base, headers=headers, params={\"ref\": ref}, timeout=15)\n    resp.raise_for_status()\n    data = resp.json()\n\n    if isinstance(data, dict) and data.get(\"type\") == \"file\":\n        import base64\n        content = base64.b64decode(data[\"content\"]).decode(\"utf-8\")\n        return {\"tree\": [], \"file_content\": content}\n\n    tree = [{\"path\": item[\"path\"], \"type\": item[\"type\"], \"size\": item.get(\"size\", 0)}\n            for item in data]\n    return {\"tree\": tree, \"file_content\": \"\"}\n```\n\n---\n\n## GitHub API\n\n### `github_create_issue`\n\n**Description:** Creates a new issue on a GitHub repository with title, body, labels, assignees, and milestone, returning the full issue URL and number.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo: str` (`owner/repo`), `title: str`, `body: str = \"\"`, `labels: list[str] = []`, `assignees: list[str] = []`, `milestone: int = 0` |\n| **Output** | `dict` with `number: int`, `url: str`, `html_url: str` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | `@anthropic/github-mcp` -- exposes `create_issue` |\n\n```python\nimport os, httpx\n\ndef main(repo: str, title: str, body: str = \"\",\n         labels: list[str] | None = None,\n         assignees: list[str] | None = None,\n         milestone: int = 0) -> dict:\n    \"\"\"Create a GitHub issue.\"\"\"\n    headers = {\n        \"Authorization\": f\"token {os.environ['GITHUB_TOKEN']}\",\n        \"Accept\": \"application/vnd.github.v3+json\",\n    }\n    payload: dict = {\"title\": title, \"body\": body}\n    if labels:\n        payload[\"labels\"] = labels\n    if assignees:\n        payload[\"assignees\"] = assignees\n    if milestone:\n        payload[\"milestone\"] = milestone\n\n    resp = httpx.post(\n        f\"https://api.github.com/repos/{repo}/issues\",\n        headers=headers, json=payload, timeout=15,\n    )\n    resp.raise_for_status()\n    data = resp.json()\n    return {\n        \"number\": data[\"number\"],\n        \"url\": data[\"url\"],\n        \"html_url\": data[\"html_url\"],\n    }\n```\n\n---\n\n### `github_pull_request`\n\n**Description:** Creates or updates a pull request, with support for draft PRs, auto-merge labels, and reviewer assignment.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo: str`, `title: str`, `body: str`, `head: str`, `base: str = \"main\"`, `draft: bool = False`, `reviewers: list[str] = []` |\n| **Output** | `dict` with `number: int`, `html_url: str`, `state: str` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | `@anthropic/github-mcp` -- exposes `create_pull_request` |\n\n```python\nimport os, httpx\n\ndef main(repo: str, title: str, body: str, head: str,\n         base: str = \"main\", draft: bool = False,\n         reviewers: list[str] | None = None) -> dict:\n    \"\"\"Create a GitHub pull request.\"\"\"\n    headers = {\n        \"Authorization\": f\"token {os.environ['GITHUB_TOKEN']}\",\n        \"Accept\": \"application/vnd.github.v3+json\",\n    }\n    payload = {\n        \"title\": title, \"body\": body,\n        \"head\": head, \"base\": base, \"draft\": draft,\n    }\n\n    resp = httpx.post(\n        f\"https://api.github.com/repos/{repo}/pulls\",\n        headers=headers, json=payload, timeout=15,\n    )\n    resp.raise_for_status()\n    pr = resp.json()\n\n    # Request reviewers if specified\n    if reviewers:\n        httpx.post(\n            f\"https://api.github.com/repos/{repo}/pulls/{pr['number']}/requested_reviewers\",\n            headers=headers,\n            json={\"reviewers\": reviewers},\n            timeout=10,\n        )\n\n    return {\n        \"number\": pr[\"number\"],\n        \"html_url\": pr[\"html_url\"],\n        \"state\": pr[\"state\"],\n    }\n```\n\n---\n\n### `github_workflow_dispatch`\n\n**Description:** Triggers a GitHub Actions workflow via the repository dispatch or workflow_dispatch API, passing custom inputs and waiting optionally for completion status.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo: str`, `workflow_id: str` (filename or ID), `ref: str = \"main\"`, `inputs: dict = {}`, `wait: bool = False`, `timeout: int = 300` |\n| **Output** | `dict` with `triggered: bool`, `run_id: int`, `status: str`, `conclusion: str` |\n| **Key Dependencies** | `httpx`, `time` (stdlib) |\n| **MCP Alternative** | `@anthropic/github-mcp` |\n\n```python\nimport os, httpx, time\n\ndef main(repo: str, workflow_id: str, ref: str = \"main\",\n         inputs: dict | None = None, wait: bool = False,\n         timeout: int = 300) -> dict:\n    \"\"\"Trigger a GitHub Actions workflow and optionally wait for completion.\"\"\"\n    headers = {\n        \"Authorization\": f\"token {os.environ['GITHUB_TOKEN']}\",\n        \"Accept\": \"application/vnd.github.v3+json\",\n    }\n    # Trigger\n    resp = httpx.post(\n        f\"https://api.github.com/repos/{repo}/actions/workflows/{workflow_id}/dispatches\",\n        headers=headers,\n        json={\"ref\": ref, \"inputs\": inputs or {}},\n        timeout=15,\n    )\n    resp.raise_for_status()\n\n    if not wait:\n        return {\"triggered\": True, \"run_id\": 0, \"status\": \"queued\", \"conclusion\": \"\"}\n\n    # Poll for the new run\n    time.sleep(3)\n    deadline = time.time() + timeout\n    while time.time() < deadline:\n        runs_resp = httpx.get(\n            f\"https://api.github.com/repos/{repo}/actions/workflows/{workflow_id}/runs\",\n            headers=headers, params={\"branch\": ref, \"per_page\": 1},\n            timeout=15,\n        )\n        runs = runs_resp.json().get(\"workflow_runs\", [])\n        if runs:\n            run = runs[0]\n            if run[\"status\"] == \"completed\":\n                return {\n                    \"triggered\": True, \"run_id\": run[\"id\"],\n                    \"status\": run[\"status\"], \"conclusion\": run[\"conclusion\"],\n                }\n        time.sleep(10)\n\n    return {\"triggered\": True, \"run_id\": 0, \"status\": \"timeout\", \"conclusion\": \"\"}\n```\n\n---\n\n## Database Operations\n\n### `supabase_query`\n\n**Description:** Executes queries against a Supabase project using the PostgREST API -- supports select, insert, update, upsert, and delete with filter chaining.\n\n| Field | Detail |\n|---|---|\n| **Input** | `table: str`, `action: Literal[\"select\", \"insert\", \"update\", \"upsert\", \"delete\"]`, `data: dict | list[dict] = {}`, `filters: list[dict] = []`, `select_columns: str = \"*\"`, `limit: int = 100` |\n| **Output** | `dict` with `data: list[dict]`, `count: int`, `status: int` |\n| **Key Dependencies** | `supabase` (supabase-py) |\n| **MCP Alternative** | `@anthropic/supabase-mcp` server |\n\n```python\nimport os\nfrom supabase import create_client\n\ndef main(table: str, action: str = \"select\", data=None,\n         filters: list[dict] | None = None,\n         select_columns: str = \"*\", limit: int = 100) -> dict:\n    \"\"\"Execute a Supabase database operation.\"\"\"\n    client = create_client(\n        os.environ[\"SUPABASE_URL\"],\n        os.environ[\"SUPABASE_SERVICE_KEY\"],\n    )\n\n    query = client.table(table)\n\n    if action == \"select\":\n        query = query.select(select_columns)\n        for f in (filters or []):\n            query = getattr(query, f.get(\"op\", \"eq\"))(f[\"field\"], f[\"value\"])\n        result = query.limit(limit).execute()\n        return {\"data\": result.data, \"count\": len(result.data), \"status\": 200}\n\n    if action == \"insert\":\n        result = query.insert(data).execute()\n        return {\"data\": result.data, \"count\": len(result.data), \"status\": 201}\n\n    if action == \"upsert\":\n        result = query.upsert(data).execute()\n        return {\"data\": result.data, \"count\": len(result.data), \"status\": 200}\n\n    if action == \"update\":\n        q = query.update(data)\n        for f in (filters or []):\n            q = getattr(q, f.get(\"op\", \"eq\"))(f[\"field\"], f[\"value\"])\n        result = q.execute()\n        return {\"data\": result.data, \"count\": len(result.data), \"status\": 200}\n\n    if action == \"delete\":\n        q = query.delete()\n        for f in (filters or []):\n            q = getattr(q, f.get(\"op\", \"eq\"))(f[\"field\"], f[\"value\"])\n        result = q.execute()\n        return {\"data\": result.data, \"count\": len(result.data), \"status\": 200}\n\n    raise ValueError(f\"Unknown action: {action}\")\n```\n\n---\n\n### `sqlite_query`\n\n**Description:** Opens (or creates) a local SQLite database and executes arbitrary SQL, returning rows as dictionaries -- ideal for lightweight local state, caching, and intermediate pipeline storage.\n\n| Field | Detail |\n|---|---|\n| **Input** | `db_path: str`, `sql: str`, `params: list = []`, `mode: Literal[\"query\", \"execute\", \"script\"] = \"query\"` |\n| **Output** | `dict` with `rows: list[dict]`, `rowcount: int`, `lastrowid: int` |\n| **Key Dependencies** | `sqlite3` (stdlib) |\n| **MCP Alternative** | `@anthropic/sqlite-mcp` server |\n\n```python\nimport sqlite3\n\ndef main(db_path: str, sql: str, params: list | None = None,\n         mode: str = \"query\") -> dict:\n    \"\"\"Execute SQL against a SQLite database.\"\"\"\n    conn = sqlite3.connect(db_path)\n    conn.row_factory = sqlite3.Row\n    cur = conn.cursor()\n\n    try:\n        if mode == \"script\":\n            cur.executescript(sql)\n            conn.commit()\n            return {\"rows\": [], \"rowcount\": cur.rowcount, \"lastrowid\": cur.lastrowid}\n\n        if mode == \"execute\":\n            cur.execute(sql, params or [])\n            conn.commit()\n            return {\"rows\": [], \"rowcount\": cur.rowcount, \"lastrowid\": cur.lastrowid}\n\n        # query mode\n        cur.execute(sql, params or [])\n        rows = [dict(row) for row in cur.fetchall()]\n        return {\"rows\": rows, \"rowcount\": len(rows), \"lastrowid\": 0}\n\n    finally:\n        conn.close()\n```\n\n---\n\n### `airtable_ops`\n\n**Description:** Reads, creates, updates, or deletes records in an Airtable base, handling pagination and field-type coercion automatically.\n\n| Field | Detail |\n|---|---|\n| **Input** | `base_id: str`, `table_name: str`, `action: Literal[\"list\", \"create\", \"update\", \"delete\"]`, `records: list[dict] = []`, `filter_formula: str = \"\"`, `max_records: int = 100` |\n| **Output** | `dict` with `records: list[dict]`, `count: int` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | None standard; wrap as custom MCP tool |\n\n```python\nimport os, httpx\n\nBASE_URL = \"https://api.airtable.com/v0\"\n\ndef _headers():\n    return {\n        \"Authorization\": f\"Bearer {os.environ['AIRTABLE_API_KEY']}\",\n        \"Content-Type\": \"application/json\",\n    }\n\ndef main(base_id: str, table_name: str, action: str = \"list\",\n         records: list[dict] | None = None,\n         filter_formula: str = \"\", max_records: int = 100) -> dict:\n    \"\"\"Perform CRUD operations on Airtable records.\"\"\"\n    url = f\"{BASE_URL}/{base_id}/{table_name}\"\n\n    if action == \"list\":\n        params: dict = {\"maxRecords\": max_records}\n        if filter_formula:\n            params[\"filterByFormula\"] = filter_formula\n        all_records, offset = [], None\n        while True:\n            if offset:\n                params[\"offset\"] = offset\n            resp = httpx.get(url, headers=_headers(), params=params, timeout=15)\n            resp.raise_for_status()\n            data = resp.json()\n            all_records.extend(data.get(\"records\", []))\n            offset = data.get(\"offset\")\n            if not offset or len(all_records) >= max_records:\n                break\n        return {\"records\": all_records, \"count\": len(all_records)}\n\n    if action == \"create\":\n        payload = {\"records\": [{\"fields\": r} for r in (records or [])]}\n        resp = httpx.post(url, headers=_headers(), json=payload, timeout=15)\n        resp.raise_for_status()\n        created = resp.json().get(\"records\", [])\n        return {\"records\": created, \"count\": len(created)}\n\n    if action == \"update\":\n        payload = {\"records\": records or []}  # expects [{\"id\": ..., \"fields\": {...}}]\n        resp = httpx.patch(url, headers=_headers(), json=payload, timeout=15)\n        resp.raise_for_status()\n        updated = resp.json().get(\"records\", [])\n        return {\"records\": updated, \"count\": len(updated)}\n\n    if action == \"delete\":\n        ids = [r if isinstance(r, str) else r.get(\"id\") for r in (records or [])]\n        resp = httpx.delete(url, headers=_headers(),\n                            params={\"records[]\": ids}, timeout=15)\n        resp.raise_for_status()\n        return {\"records\": resp.json().get(\"records\", []),","truncated":true,"truncatedBy":"bytes","totalLines":3559,"totalBytes":128386,"outputLines":1444,"outputBytes":51200,"lastLinePartial":false,"firstLineExceedsLimit":false,"maxLines":2000,"maxBytes":51200}},"isError":false,"timestamp":1771029073209}}
{"type":"message","id":"4df6a2a7","parentId":"165b20b4","timestamp":"2026-02-14T00:31:13.210Z","message":{"role":"toolResult","toolCallId":"toolu_01VYGpzD1oWuytyeHpm27Fqg","toolName":"read","content":[{"type":"text","text":"# MCP Registry\n\nThe factory's awareness of all available MCPs and integrations. This is the single source of truth for what tools are available when building WAT systems.\n\n**Usage**: Before generating tools, always check this registry. Prefer MCPs over raw API calls when available — less code to maintain, fewer bugs. But always provide a fallback.\n\n**Adding new MCPs**: Add a new entry following the format below. The factory will automatically discover it during the Research step.\n\n---\n\n## Web & Data Access\n\n### Firecrawl\n- **Description**: Web scraping and crawling service. Extracts clean content from websites, handles JavaScript rendering, and returns structured data.\n- **Key Capabilities**: Scrape single pages, crawl entire sites, extract structured data, handle JS-heavy sites, return markdown or JSON\n- **Common Use Cases**: Company research, content extraction, competitive analysis, price monitoring\n- **Tool Pattern**: `firecrawl_scrape(url) → markdown/JSON content`\n- **Secret**: `FIRECRAWL_API_KEY`\n- **Fallback**: Direct HTTP with `requests` + `beautifulsoup4` (won't handle JS rendering)\n\n### Brave Search\n- **Description**: Web search API. Returns search results with snippets and metadata.\n- **Key Capabilities**: Web search, news search, image search, summarized results\n- **Common Use Cases**: Research gathering, fact-checking, finding URLs, news monitoring\n- **Tool Pattern**: `brave_search(query) → list of search results with URLs and snippets`\n- **Secret**: `BRAVE_SEARCH_API_KEY`\n- **Fallback**: Google Custom Search API or SerpAPI\n\n### Puppeteer\n- **Description**: Browser automation MCP. Controls a headless Chrome instance for web interaction.\n- **Key Capabilities**: Navigate pages, fill forms, click buttons, take screenshots, extract DOM content\n- **Common Use Cases**: Scraping dynamic sites, form submission, visual testing, PDF generation\n- **Tool Pattern**: `puppeteer_navigate(url) → page content; puppeteer_screenshot() → image`\n- **Secret**: None (local browser)\n- **Fallback**: Playwright via Python `playwright` package\n\n### Fetch\n- **Description**: Simple HTTP fetch MCP. Makes HTTP requests and returns responses.\n- **Key Capabilities**: GET, POST, PUT, DELETE requests with headers and body\n- **Common Use Cases**: API calls, webhooks, simple page fetches\n- **Tool Pattern**: `fetch(url, method, headers, body) → response`\n- **Secret**: None\n- **Fallback**: Python `requests` library\n\n---\n\n## Code & DevOps\n\n### GitHub\n- **Description**: GitHub API integration. Full access to repos, issues, PRs, Actions, and more.\n- **Key Capabilities**: Create/read/update repos, issues, PRs, manage Actions, read file contents, manage releases\n- **Common Use Cases**: Automated PR creation, issue management, repo setup, CI/CD integration\n- **Tool Pattern**: `github_create_issue(repo, title, body); github_create_pr(repo, ...)`\n- **Secret**: `GITHUB_TOKEN` or `GITHUB_PAT`\n- **Fallback**: `gh` CLI or direct GitHub REST API via `requests`\n\n### Docker\n- **Description**: Docker container management MCP.\n- **Key Capabilities**: Build, run, stop, and manage containers\n- **Common Use Cases**: Isolated tool execution, testing environments\n- **Tool Pattern**: `docker_run(image, command) → output`\n- **Secret**: None (local Docker daemon)\n- **Fallback**: `subprocess.run([\"docker\", ...])`\n\n---\n\n## Communication\n\n### Slack\n- **Description**: Slack messaging MCP. Send messages, read channels, manage threads.\n- **Key Capabilities**: Send messages, post to channels, read channel history, manage threads, upload files\n- **Common Use Cases**: Notifications, alerts, status updates, team communication\n- **Tool Pattern**: `slack_send(channel, message); slack_read(channel) → messages`\n- **Secret**: `SLACK_BOT_TOKEN` or `SLACK_WEBHOOK_URL`\n- **Fallback**: Slack Incoming Webhook via HTTP POST\n\n### Telegram\n- **Description**: Telegram Bot API MCP. Send and receive messages via Telegram bots.\n- **Key Capabilities**: Send messages, photos, documents; read updates; inline keyboards\n- **Common Use Cases**: Alerts, notifications, command-driven bots, status reports\n- **Tool Pattern**: `telegram_send(chat_id, message); telegram_get_updates() → messages`\n- **Secret**: `TELEGRAM_BOT_TOKEN`\n- **Fallback**: Telegram Bot API via HTTP\n\n### Gmail\n- **Description**: Gmail API MCP. Send and read emails.\n- **Key Capabilities**: Send emails, read inbox, search emails, manage labels\n- **Common Use Cases**: Email notifications, inbox monitoring, automated responses\n- **Tool Pattern**: `gmail_send(to, subject, body); gmail_search(query) → emails`\n- **Secret**: `GMAIL_CREDENTIALS`\n- **Fallback**: SMTP via `smtplib`\n\n### Discord\n- **Description**: Discord messaging via webhooks or bot.\n- **Key Capabilities**: Send messages, embeds, files to channels\n- **Common Use Cases**: Team notifications, alerts, community updates\n- **Tool Pattern**: `discord_send(webhook_url, message)`\n- **Secret**: `DISCORD_WEBHOOK_URL`\n- **Fallback**: Discord webhook via HTTP POST\n\n---\n\n## Storage & Databases\n\n### Supabase\n- **Description**: Supabase (Postgres + Auth + Storage) MCP. Full database and storage access.\n- **Key Capabilities**: CRUD operations, real-time subscriptions, file storage, auth management\n- **Common Use Cases**: Persistent data storage, user management, file hosting\n- **Tool Pattern**: `supabase_insert(table, data); supabase_query(table, filters) → rows`\n- **Secret**: `SUPABASE_URL`, `SUPABASE_KEY`\n- **Fallback**: Direct PostgreSQL via `psycopg2` or Supabase REST API\n\n### Google Drive\n- **Description**: Google Drive file management MCP.\n- **Key Capabilities**: Upload, download, list, search, share files and folders\n- **Common Use Cases**: File storage, document sharing, backup\n- **Tool Pattern**: `gdrive_upload(file, folder_id); gdrive_list(folder_id) → files`\n- **Secret**: `GOOGLE_CREDENTIALS`\n- **Fallback**: Google Drive API via `google-api-python-client`\n\n### Airtable\n- **Description**: Airtable database MCP. CRUD operations on Airtable bases and tables.\n- **Key Capabilities**: Create, read, update, delete records; list tables; manage views\n- **Common Use Cases**: Lightweight database, CRM data, project tracking, content calendars\n- **Tool Pattern**: `airtable_create(base, table, record); airtable_list(base, table) → records`\n- **Secret**: `AIRTABLE_API_KEY`\n- **Fallback**: Airtable REST API via `requests`\n\n### Notion\n- **Description**: Notion API MCP. Manage pages, databases, and blocks.\n- **Key Capabilities**: Create/update pages, query databases, manage blocks\n- **Common Use Cases**: Knowledge base, documentation, project management, content publishing\n- **Tool Pattern**: `notion_create_page(parent, title, content); notion_query_db(db_id, filter) → pages`\n- **Secret**: `NOTION_API_KEY`\n- **Fallback**: Notion API via `requests`\n\n### SQLite\n- **Description**: Local SQLite database MCP. Embedded SQL database.\n- **Key Capabilities**: Create tables, CRUD operations, run SQL queries\n- **Common Use Cases**: Local data caching, lightweight storage, offline-first data\n- **Tool Pattern**: `sqlite_query(db_path, sql) → rows`\n- **Secret**: None (local file)\n- **Fallback**: Python `sqlite3` standard library module\n\n---\n\n## AI & Processing\n\n### OpenAI\n- **Description**: OpenAI API MCP. Access GPT models, DALL-E, Whisper, and embeddings.\n- **Key Capabilities**: Chat completions, image generation, speech-to-text, embeddings, structured output\n- **Common Use Cases**: Text generation, summarization, classification, image creation, transcription\n- **Tool Pattern**: `openai_chat(messages) → response; openai_embed(text) → vector`\n- **Secret**: `OPENAI_API_KEY`\n- **Fallback**: OpenAI Python SDK directly\n\n### Anthropic\n- **Description**: Anthropic API for Claude models.\n- **Key Capabilities**: Chat completions, tool use, structured output, long context\n- **Common Use Cases**: Complex reasoning, analysis, code generation, document processing\n- **Tool Pattern**: `anthropic_message(messages) → response`\n- **Secret**: `ANTHROPIC_API_KEY`\n- **Fallback**: Anthropic Python SDK directly\n\n### ElevenLabs\n- **Description**: AI voice synthesis and cloning.\n- **Key Capabilities**: Text-to-speech, voice cloning, audio generation\n- **Common Use Cases**: Audio content creation, podcasts, voice notifications\n- **Tool Pattern**: `elevenlabs_tts(text, voice_id) → audio_file`\n- **Secret**: `ELEVENLABS_API_KEY`\n- **Fallback**: ElevenLabs REST API\n\n### Replicate\n- **Description**: Run ML models via API. Access thousands of open-source models.\n- **Key Capabilities**: Image generation, video generation, audio processing, custom models\n- **Common Use Cases**: Image manipulation, model inference, content generation\n- **Tool Pattern**: `replicate_run(model, input) → output`\n- **Secret**: `REPLICATE_API_TOKEN`\n- **Fallback**: Replicate Python SDK or REST API\n\n---\n\n## Business & Productivity\n\n### Stripe\n- **Description**: Stripe payment processing API.\n- **Key Capabilities**: Create charges, manage subscriptions, list transactions, handle webhooks\n- **Common Use Cases**: Payment tracking, subscription management, revenue reporting\n- **Tool Pattern**: `stripe_list_charges(filters) → charges`\n- **Secret**: `STRIPE_API_KEY`\n- **Fallback**: Stripe Python SDK\n\n### Google Sheets\n- **Description**: Google Sheets API MCP.\n- **Key Capabilities**: Read/write cells, create sheets, format, formulas\n- **Common Use Cases**: Data input/output, reporting, dashboards, data collection\n- **Tool Pattern**: `sheets_read(spreadsheet_id, range) → data; sheets_write(spreadsheet_id, range, data)`\n- **Secret**: `GOOGLE_CREDENTIALS`\n- **Fallback**: `gspread` Python library\n\n### Google Calendar\n- **Description**: Google Calendar API MCP.\n- **Key Capabilities**: Create/read/update events, list calendars, manage attendees\n- **Common Use Cases**: Scheduling, event tracking, availability checks\n- **Tool Pattern**: `calendar_create_event(calendar_id, event); calendar_list(calendar_id) → events`\n- **Secret**: `GOOGLE_CREDENTIALS`\n- **Fallback**: Google Calendar API via `google-api-python-client`\n\n### Linear\n- **Description**: Linear project management API.\n- **Key Capabilities**: Create/update issues, manage projects, track cycles\n- **Common Use Cases**: Task creation, sprint management, automated issue tracking\n- **Tool Pattern**: `linear_create_issue(team, title, description)`\n- **Secret**: `LINEAR_API_KEY`\n- **Fallback**: Linear GraphQL API via `requests`\n\n---\n\n## Local & System\n\n### Filesystem\n- **Description**: Local filesystem access MCP.\n- **Key Capabilities**: Read, write, list, delete files and directories\n- **Common Use Cases**: File processing, data storage, configuration management\n- **Tool Pattern**: `fs_read(path) → content; fs_write(path, content)`\n- **Secret**: None\n- **Fallback**: Python `pathlib` / built-in `open()`\n\n### Shell / Desktop Commander\n- **Description**: Execute shell commands on the local system.\n- **Key Capabilities**: Run commands, manage processes, system information\n- **Common Use Cases**: Tool execution, system automation, build processes\n- **Tool Pattern**: `shell_exec(command) → output`\n- **Secret**: None\n- **Fallback**: Python `subprocess`\n\n---\n\n## Social & Content\n\n### Twitter/X\n- **Description**: Twitter/X API integration.\n- **Key Capabilities**: Post tweets, read timelines, search tweets, manage lists\n- **Common Use Cases**: Social media posting, monitoring mentions, content distribution\n- **Tool Pattern**: `twitter_post(text); twitter_search(query) → tweets`\n- **Secret**: `TWITTER_BEARER_TOKEN`\n- **Fallback**: Twitter API v2 via `requests`\n\n### LinkedIn\n- **Description**: LinkedIn API integration.\n- **Key Capabilities**: Post updates, read profile, company pages\n- **Common Use Cases**: Professional content posting, company updates\n- **Tool Pattern**: `linkedin_post(text, image_url)`\n- **Secret**: `LINKEDIN_ACCESS_TOKEN`\n- **Fallback**: LinkedIn API via `requests`\n\n### YouTube\n- **Description**: YouTube Data API.\n- **Key Capabilities**: Search videos, get video details, manage playlists, read comments\n- **Common Use Cases**: Video research, content analysis, playlist management\n- **Tool Pattern**: `youtube_search(query) → videos; youtube_video_details(video_id) → metadata`\n- **Secret**: `YOUTUBE_API_KEY`\n- **Fallback**: YouTube Data API via `google-api-python-client`\n\n---\n\n## Custom / Any API\n\nAny REST API can be integrated into a WAT system without an MCP:\n\n1. Write a Python tool using `requests` with proper auth headers\n2. Store API credentials in GitHub Secrets\n3. Reference via `os.environ[\"SECRET_NAME\"]` in the tool\n4. Add retry logic with `tenacity` or manual retry loops\n5. Document the API in the system's CLAUDE.md\n\nThe factory is never limited to the MCPs listed here. This registry helps with tool selection, but any API accessible via HTTP can be wrapped as a Python tool.\n\n---\n\n## Media Processing\n\n### NCA Toolkit (Self-Hosted)\n- **Description**: Self-hosted media processing API running at `https://media.wat-factory.cloud`. Replaces services like CloudConvert, Placid, and PDF.co. Handles audio, video, image conversion, transcription, captioning, and more.\n- **Base URL**: `https://media.wat-factory.cloud`\n- **Auth**: `x-api-key` header with `NCA_API_KEY` environment variable\n- **Key Capabilities**:\n  - Transcribe audio/video to text (Whisper) — `POST /v1/media/transcribe`\n  - Convert media between formats — `POST /v1/media/convert`\n  - Convert to MP3 — `POST /v1/media/convert/mp3`\n  - Add captions to video — `POST /v1/video/caption`\n  - Concatenate videos — `POST /v1/video/concatenate`\n  - Concatenate audio — `POST /v1/audio/concatenate`\n  - Trim video — `POST /v1/video/trim`\n  - Cut segments from video — `POST /v1/video/cut`\n  - Split video — `POST /v1/video/split`\n  - Extract thumbnail — `POST /v1/video/thumbnail`\n  - Screenshot webpage — `POST /v1/image/screenshot/webpage`\n  - Convert image to video — `POST /v1/image/convert/video`\n  - Download media from URL (yt-dlp) — `POST /v1/BETA/media/download`\n  - Detect silence — `POST /v1/media/silence`\n  - Extract metadata — `POST /v1/media/metadata`\n  - Generate ASS subtitles — `POST /v1/media/generate/ass`\n  - Upload to S3/R2 — `POST /v1/s3/upload`\n  - Run FFmpeg commands — `POST /v1/ffmpeg/compose`\n  - Execute Python code — `POST /v1/code/execute/python`\n  - Test connectivity — `GET /v1/toolkit/test`\n  - Check job status — `GET /v1/toolkit/job/status?job_id=<id>`\n- **Common Use Cases**: Video transcription, audio/video format conversion, auto-captioning videos for social media, webpage screenshots, media downloading, thumbnail extraction\n- **Tool Pattern**:\n  ```python\n  import os, requests\n  from urllib.parse import quote\n\n  NCA_URL = \"https://media.wat-factory.cloud\"\n  R2_PUBLIC = \"https://pub-b692c36a8374480593a007a38bc7116c.r2.dev\"\n  headers = {\"x-api-key\": os.environ[\"NCA_API_KEY\"], \"Content-Type\": \"application/json\"}\n\n  # === YouTube Transcription (two-step process) ===\n  # Step 1: Download the video (uses yt-dlp, stores on R2)\n  dl = requests.post(f\"{NCA_URL}/v1/BETA/media/download\", headers=headers, json={\n      \"media_url\": \"https://youtube.com/watch?v=VIDEO_ID\"\n  }).json()\n\n  # Step 2: Get the filename from the response and build the PUBLIC R2 URL\n  # IMPORTANT: The storage URL returned uses the private R2 endpoint which is not accessible.\n  # You MUST use the public R2 dev URL instead.\n  # IMPORTANT: Filenames often contain spaces, parentheses, and unicode characters.\n  # You MUST URL-encode the filename.\n  filename = dl[\"response\"][\"media\"][\"media_url\"].split(\"/nca-media/\")[-1]\n  public_url = f\"{R2_PUBLIC}/{filename}\"  # filename is already URL-encoded from the response\n\n  # Step 3: Transcribe using the public URL\n  transcript = requests.post(f\"{NCA_URL}/v1/media/transcribe\", headers=headers, json={\n      \"media_url\": public_url\n  }).json()\n  text = transcript[\"response\"][\"text\"]\n\n  # === Direct media transcription (non-YouTube, publicly accessible URL) ===\n  resp = requests.post(f\"{NCA_URL}/v1/media/transcribe\", headers=headers, json={\n      \"media_url\": \"https://example.com/audio.mp3\"\n  })\n\n  # === Screenshot a webpage ===\n  resp = requests.post(f\"{NCA_URL}/v1/image/screenshot/webpage\", headers=headers, json={\n      \"url\": \"https://example.com\",\n      \"width\": 1920,\n      \"height\": 1080\n  })\n  ```\n- **Secret**: `NCA_API_KEY`\n- **Fallback**: Direct FFmpeg via subprocess (limited — no transcription, no cloud upload)\n- **CRITICAL NOTES**:\n  - The `/v1/media/transcribe` endpoint does NOT support YouTube URLs directly. You MUST first download via `/v1/BETA/media/download`, then transcribe the downloaded file.\n  - The `media_url` returned by the download endpoint uses the private R2 storage URL (`fa8408d96e71300f9e63b83ad7177384.r2.cloudflarestorage.com`). This URL is NOT publicly accessible. Replace the domain with the public R2 URL: `pub-b692c36a8374480593a007a38bc7116c.r2.dev`\n  - All field names use `media_url` (not `url`), except for screenshot which uses `url`.\n  - Transcription runs Whisper on CPU. A 20-minute video takes ~3-4 minutes. Set request timeouts accordingly (at least 600 seconds).\n  - Supports async processing with `webhook_url` parameter for long-running jobs.\n"}],"isError":false,"timestamp":1771029073210}}
{"type":"message","id":"468ef8e8","parentId":"4df6a2a7","timestamp":"2026-02-14T00:31:13.211Z","message":{"role":"toolResult","toolCallId":"toolu_01UVV25vgWmnPjK6E1B8vzaa","toolName":"read","content":[{"type":"text","text":"name: \"{System Name}\"\ndescription: |\n\n## Purpose\nWAT System PRP (Product Requirements Prompt) — a structured blueprint that gives the factory enough context to build a complete, working system in one pass.\n\n## Core Principles\n1. **Context is King**: Include ALL necessary documentation, references, and caveats\n2. **Validation Loops**: Provide executable checks the factory can run at each gate\n3. **Information Dense**: Use keywords and patterns from the WAT framework\n4. **Progressive Success**: Start simple, validate, then enhance\n5. **Global rules**: Follow all rules in CLAUDE.md\n\n---\n\n## Goal\n[What system needs to be built. Be specific about the end state: what it does, who uses it, and what success looks like when deployed.]\n\n## Why\n- [Business value and user impact]\n- [What manual process this automates or what gap it fills]\n- [Who benefits and how often they benefit]\n\n## What\n[User-visible behavior and technical requirements. Describe what the system does from the perspective of someone triggering it.]\n\n### Success Criteria\n- [ ] [Specific measurable outcome 1]\n- [ ] [Specific measurable outcome 2]\n- [ ] [Specific measurable outcome 3]\n- [ ] System runs autonomously via GitHub Actions on schedule\n- [ ] Results are committed back to repo or delivered via webhook/notification\n- [ ] All three execution paths work: CLI, GitHub Actions, Agent HQ\n\n---\n\n## Inputs\n[What goes into the system. Be specific about format, source, and any validation requirements.]\n\n```yaml\n- name: \"{input_name}\"\n  type: \"{string | JSON | file | URL | list}\"\n  source: \"{manual input | API | file path | webhook payload}\"\n  required: true\n  description: \"{What this input is and what format it should be in}\"\n  example: \"{A concrete example value}\"\n```\n\n## Outputs\n[What comes out of the system. Where do results go?]\n\n```yaml\n- name: \"{output_name}\"\n  type: \"{JSON | CSV | Markdown | file | webhook}\"\n  destination: \"{repo commit | email | Slack | webhook URL | file path}\"\n  description: \"{What this output contains}\"\n  example: \"{A concrete example or link to sample}\"\n```\n\n---\n\n## All Needed Context\n\n### Documentation & References\n```yaml\n# MUST READ — Include these in context when building\n- url: \"{Official API docs URL}\"\n  why: \"{Specific sections/methods the tools will need}\"\n\n- url: \"{Library documentation URL}\"\n  why: \"{Specific patterns or gotchas to follow}\"\n\n- file: \"{path/to/existing/pattern.py}\"\n  why: \"{Pattern to follow, conventions to match}\"\n\n- doc: \"config/mcp_registry.md\"\n  why: \"Check which MCPs provide the capabilities this system needs\"\n\n- doc: \"library/patterns.md\"\n  why: \"Select the best workflow pattern for this system\"\n\n- doc: \"library/tool_catalog.md\"\n  why: \"Identify reusable tool patterns to adapt\"\n```\n\n### Workflow Pattern Selection\n```yaml\n# Reference library/patterns.md — select the best-fit pattern\npattern: \"{Pattern name from library/patterns.md, e.g., Scrape > Process > Output}\"\nrationale: \"{Why this pattern fits the problem}\"\nmodifications: \"{Any adjustments needed to the standard pattern}\"\n```\n\n### MCP & Tool Requirements\n```yaml\n# Reference config/mcp_registry.md — list capabilities needed\ncapabilities:\n  - name: \"{capability, e.g., web scraping}\"\n    primary_mcp: \"{MCP name from registry, e.g., firecrawl}\"\n    alternative_mcp: \"{Alternative MCP}\"\n    fallback: \"{Direct API/HTTP approach if no MCP available}\"\n    secret_name: \"{UPPERCASE_SECRET_NAME}\"\n\n  - name: \"{capability 2}\"\n    primary_mcp: \"{MCP name}\"\n    alternative_mcp: \"{Alternative}\"\n    fallback: \"{Fallback approach}\"\n    secret_name: \"{SECRET_NAME}\"\n```\n\n### Known Gotchas & Constraints\n```\n# CRITICAL: {API/service name} has rate limit of {N} requests per {period}\n# CRITICAL: {Library} requires {specific setup or version}\n# CRITICAL: {Data source} returns {unexpected format} — must handle {edge case}\n# CRITICAL: Secrets are NEVER hardcoded — always use GitHub Secrets or .env\n# CRITICAL: Every tool must have try/except, logging, type hints, and a main() function\n```\n\n---\n\n## System Design\n\n### Subagent Architecture\n[Define the specialist subagents this system needs. One subagent per major capability or workflow phase.]\n\n```yaml\nsubagents:\n  - name: \"{function}-specialist\"\n    description: \"When Claude should delegate to this subagent — be specific\"\n    tools: \"Read, Bash, Grep\"\n    model: \"sonnet\"\n    permissionMode: \"default\"\n    responsibilities:\n      - \"{What this subagent does — step 1}\"\n      - \"{What this subagent does — step 2}\"\n    inputs: \"{What data it receives}\"\n    outputs: \"{What data it produces}\"\n\n  - name: \"{function}-specialist\"\n    description: \"When Claude should delegate to this subagent\"\n    tools: \"Read, Write, Bash\"\n    model: \"sonnet\"\n    permissionMode: \"default\"\n    responsibilities:\n      - \"{Responsibility 1}\"\n      - \"{Responsibility 2}\"\n    inputs: \"{Input data}\"\n    outputs: \"{Output data}\"\n```\n\n### Agent Teams Analysis\n```yaml\n# Apply the 3+ Independent Tasks Rule\nindependent_tasks:\n  - \"{Task A — no dependency on other tasks}\"\n  - \"{Task B — no dependency on other tasks}\"\n  - \"{Task C — no dependency on other tasks}\"\n\nindependent_task_count: \"{N}\"\nrecommendation: \"{Use Agent Teams | Sequential execution}\"\nrationale: \"{Why — e.g., '3 independent API calls each taking 10s = 30s sequential vs 10s parallel'}\"\n\n# If Agent Teams recommended:\nteam_lead_responsibilities:\n  - \"Create shared task list\"\n  - \"Spawn teammates for independent tasks\"\n  - \"Merge results and run quality control\"\n\nteammates:\n  - name: \"{Teammate 1}\"\n    task: \"{Scoped task description}\"\n    inputs: \"{What it receives}\"\n    outputs: \"{What it produces}\"\n\n# If NOT recommended:\nsequential_rationale: \"{Why sequential is sufficient — e.g., 'all tasks depend on previous output'}\"\n```\n\n### GitHub Actions Triggers\n```yaml\ntriggers:\n  - type: \"{workflow_dispatch | schedule | repository_dispatch}\"\n    config: \"{cron expression, input parameters, or event type}\"\n    description: \"{When and why this trigger fires}\"\n```\n\n---\n\n## Implementation Blueprint\n\n### Workflow Steps\n[Ordered list of workflow phases. Each step becomes a section in workflow.md.]\n\n```yaml\nsteps:\n  - name: \"{Step 1 name}\"\n    description: \"{What happens in this step}\"\n    subagent: \"{Which subagent handles this, if any}\"\n    tools: [\"{tool_1.py}\", \"{tool_2.py}\"]\n    inputs: \"{What this step receives}\"\n    outputs: \"{What this step produces}\"\n    failure_mode: \"{What can go wrong}\"\n    fallback: \"{What to do if it fails}\"\n\n  - name: \"{Step 2 name}\"\n    description: \"{Description}\"\n    subagent: \"{subagent name}\"\n    tools: [\"{tool_name.py}\"]\n    inputs: \"{From previous step}\"\n    outputs: \"{For next step}\"\n    failure_mode: \"{Failure scenario}\"\n    fallback: \"{Recovery action}\"\n```\n\n### Tool Specifications\n[For each tool the system needs. Reference library/tool_catalog.md for reusable patterns.]\n\n```yaml\ntools:\n  - name: \"{tool_name}.py\"\n    purpose: \"{One-line description}\"\n    catalog_pattern: \"{Pattern name from tool_catalog.md, or 'new' if custom}\"\n    inputs:\n      - \"{arg1}: {type} — {description}\"\n      - \"{arg2}: {type} — {description}\"\n    outputs: \"{JSON structure or file format}\"\n    dependencies: [\"{pip package 1}\", \"{pip package 2}\"]\n    mcp_used: \"{MCP name, or 'none'}\"\n    error_handling: \"{How errors are handled}\"\n\n  - name: \"{tool_name_2}.py\"\n    purpose: \"{Description}\"\n    catalog_pattern: \"{Pattern or 'new'}\"\n    inputs:\n      - \"{arg}: {type} — {description}\"\n    outputs: \"{Output format}\"\n    dependencies: [\"{package}\"]\n    mcp_used: \"{MCP or 'none'}\"\n    error_handling: \"{Error approach}\"\n```\n\n### Per-Tool Pseudocode\n```python\n# {tool_name}.py\ndef main():\n    # PATTERN: {Which catalog pattern this follows}\n    # Step 1: Parse inputs\n    args = parse_args()  # argparse or env vars\n\n    # Step 2: {Core logic description}\n    # GOTCHA: {Important caveat about this API/library}\n    result = do_work(args.input)\n\n    # Step 3: Output results\n    # PATTERN: Structured JSON to stdout\n    print(json.dumps(result))\n\n# {tool_name_2}.py\ndef main():\n    # PATTERN: {Catalog pattern}\n    # CRITICAL: {Key constraint — rate limit, auth, format}\n    pass\n```\n\n### Integration Points\n```yaml\nSECRETS:\n  - name: \"{SECRET_NAME}\"\n    purpose: \"{What API/service this authenticates}\"\n    required: true\n\nENVIRONMENT:\n  - file: \".env.example\"\n    vars:\n      - \"{VAR_NAME}={placeholder_value}  # {description}\"\n\nDEPENDENCIES:\n  - file: \"requirements.txt\"\n    packages:\n      - \"{package}=={version}  # {why}\"\n\nGITHUB_ACTIONS:\n  - trigger: \"{workflow_dispatch | schedule | repository_dispatch}\"\n    config: \"{Details}\"\n```\n\n---\n\n## Validation Loop\n\n### Level 1: Syntax & Structure\n```bash\n# Run FIRST — every tool must pass before proceeding to Level 2\n# AST parse — verify valid Python syntax\npython -c \"import ast; ast.parse(open('tools/{tool_name}.py').read())\"\n\n# Import check — verify no missing dependencies\npython -c \"import importlib; importlib.import_module('tools.{tool_name}')\"\n\n# Structure check — verify main() exists\npython -c \"from tools.{tool_name} import main; assert callable(main)\"\n\n# Repeat for every tool file\n# Expected: All pass with no errors. If any fail, fix before proceeding.\n```\n\n### Level 2: Unit Tests\n```bash\n# Run SECOND — each tool must produce correct output for sample inputs\n# Test each tool independently with mock/sample data\n\npython tools/{tool_name}.py --input \"{sample_input}\"\n# Expected output: {expected JSON or result}\n\npython tools/{tool_name_2}.py --input \"{sample_input}\"\n# Expected output: {expected result}\n\n# If any tool fails: Read the error, fix the root cause, re-run.\n# NEVER mock to make tests pass — fix the actual code.\n```\n\n### Level 3: Integration Tests\n```bash\n# Run THIRD — verify tools work together as a pipeline\n# Simulate the full workflow with sample data\n\n# Step 1: Run first tool\npython tools/{tool_1}.py --input \"{sample}\" > /tmp/step1_output.json\n\n# Step 2: Feed output to next tool\npython tools/{tool_2}.py --input /tmp/step1_output.json > /tmp/step2_output.json\n\n# Step 3: Verify final output\npython -c \"\nimport json\nresult = json.load(open('/tmp/step2_output.json'))\nassert '{expected_key}' in result, 'Missing expected key in output'\nprint('Integration test passed')\n\"\n\n# Verify workflow.md references match actual tool files\n# Verify CLAUDE.md documents all tools and subagents\n# Verify .github/workflows/ YAML is valid\n```\n\n---\n\n## Final Validation Checklist\n- [ ] All tools pass Level 1 (syntax, imports, structure)\n- [ ] All tools pass Level 2 (unit tests with sample data)\n- [ ] Pipeline passes Level 3 (integration test end-to-end)\n- [ ] workflow.md has failure modes and fallbacks for every step\n- [ ] CLAUDE.md documents all tools, subagents, MCPs, and secrets\n- [ ] .github/workflows/ has timeout-minutes and failure notifications\n- [ ] .env.example lists all required environment variables\n- [ ] .gitignore excludes .env, __pycache__/, credentials\n- [ ] README.md covers all three execution paths (CLI, Actions, Agent HQ)\n- [ ] No hardcoded secrets anywhere in the codebase\n- [ ] Subagent files have valid YAML frontmatter and specific system prompts\n- [ ] requirements.txt lists all Python dependencies\n\n---\n\n## Anti-Patterns to Avoid\n- Do not hardcode API keys, tokens, or credentials — use GitHub Secrets or .env\n- Do not use `git add -A` or `git add .` — stage only specific files\n- Do not skip validation because \"it should work\" — run all three levels\n- Do not catch bare `except:` — always catch specific exception types\n- Do not build tools that require interactive input — all tools must run unattended\n- Do not create MCP-dependent tools without HTTP/API fallbacks\n- Do not design subagents that call other subagents — only the main agent delegates\n- Do not use Agent Teams when fewer than 3 independent tasks exist — the overhead is not justified\n- Do not commit .env files, credentials, or API keys to the repository\n- Do not ignore failing tests — fix the root cause, never mock to pass\n- Do not generate workflow steps without failure modes and fallback actions\n- Do not write tools without try/except, logging, type hints, and a main() function\n\n---\n\n## Confidence Score: {N}/10\n\n**Score rationale:**\n- [{Area}]: {Why confident or uncertain} — Confidence: {high | medium | low}\n- [{Area}]: {Rationale} — Confidence: {high | medium | low}\n- [{Area}]: {Rationale} — Confidence: {high | medium | low}\n\n**Ambiguity flags** (areas requiring clarification before building):\n- [ ] {Ambiguous requirement 1 — what needs clarification and why}\n- [ ] {Ambiguous requirement 2 — what needs clarification and why}\n\n**If any ambiguity flag is checked, DO NOT proceed to build. Ask the user to clarify first.**\n\n---\n\n## Factory Build Prompt\n\nWhen this PRP is ready, execute the build with:\n\n```\n/execute-prp PRPs/{system-name}.md\n```\n\nThis will trigger `factory/workflow.md` with the above requirements as structured input.\n"}],"isError":false,"timestamp":1771029073211}}
{"type":"message","id":"d88f09de","parentId":"468ef8e8","timestamp":"2026-02-14T00:36:44.162Z","message":{"role":"assistant","content":[{"type":"thinking","thinking":"Excellent! Now I have all the reference materials I need to generate a comprehensive PRP for the weekly Instagram content publisher system. Let me analyze what I've gathered:\n\nFrom the patterns:\n- This is primarily a **Generate > Review > Publish** pattern (two-pass generation with quality gates)\n- Could also incorporate **Monitor > Collect > Transform > Deliver** for content calendar management\n- May benefit from **Sequential State Management** for tracking published posts\n- Content generation could use **Fan-Out > Process > Merge** (Agent Teams) if generating 3+ posts in parallel\n\nFrom the tool catalog:\n- `llm_prompt` for content generation\n- `structured_extract` for extracting structured data from generation\n- Instagram Graph API integration (not in MCP registry, will need direct HTTP)\n- `firecrawl_scrape` for reference content from URLs\n- `rest_client` for Instagram API calls\n- `json_read_write` for content pack storage\n- `email_send` for notifications (optional)\n- `multi_dimension_scorer` could be adapted for brand voice scoring\n\nFrom MCP registry:\n- Firecrawl for reference content extraction\n- Anthropic for content generation and review\n- No native Instagram MCP (will need direct API integration)\n- Could use Slack/Discord/email for notifications\n\nKey considerations for this system:\n1. **Two-pass process**: Generate > Review > Publish is critical\n2. **Quality gates**: Brand voice, compliance, hashtag hygiene, IG format validation\n3. **Auto-publish gating**: Must be able to fall back to content packs\n4. **Subagent architecture**: content-strategist, copywriter, hashtag-specialist, reviewer-specialist\n5. **Agent Teams**: If generating 3+ posts (like \"3 reels + 2 carousels + 1 single\"), that's 6 independent generation tasks → perfect for Agent Teams\n6. **Instagram Graph API**: Need to research API requirements, rate limits, authentication flow\n\n\n7. **State persistence**: Must maintain a rolling archive of published content for deduplication and scheduling\n8. **API constraints**: Instagram enforces strict rate limiting that will require throttling between requests\n\nLet me structure this PRP carefully following the template.","thinkingSignature":"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"},{"type":"text","text":"Now I'll generate a comprehensive PRP for the weekly Instagram content publisher system."},{"type":"toolCall","id":"toolu_01Tsay1HofnGKsotjjpLFw62","name":"write","arguments":{"path":"PRPs/weekly-instagram-content-publisher.md","content":"name: \"weekly-instagram-content-publisher\"\ndescription: |\n  Weekly Instagram Content Publisher — An AI-powered social media content generation system that creates Instagram-ready content packs (captions, hashtags, creative briefs, image prompts) based on brand voice and weekly themes, runs multi-dimensional quality review checks, and either auto-publishes via Instagram Graph API or prepares manual upload packages.\n\n## Purpose\nWAT System PRP (Product Requirements Prompt) — a structured blueprint that gives the factory enough context to build a complete, working system in one pass.\n\n## Core Principles\n1. **Context is King**: Include ALL necessary documentation, references, and caveats\n2. **Validation Loops**: Provide executable checks the factory can run at each gate\n3. **Information Dense**: Use keywords and patterns from the WAT framework\n4. **Progressive Success**: Start simple, validate, then enhance\n5. **Global rules**: Follow all rules in CLAUDE.md\n\n---\n\n## Goal\nBuild a weekly automated Instagram content generation system that produces on-brand, compliance-checked content packs with options for automatic publishing or manual review. The system takes brand voice parameters, weekly themes, and post plans as input, generates full content packs with captions/hashtags/creative briefs, runs quality gates (brand voice alignment, compliance, format validation), and either publishes to Instagram via Graph API or outputs ready-to-post packages for manual upload.\n\nSuccess looks like: Marketing teams open an issue or trigger a workflow with a weekly theme, and 5-10 minutes later receive a complete content pack that's either auto-published or sitting in a ready-to-upload folder with all assets prepared.\n\n## Why\n- **Saves time**: Manual Instagram content creation takes 2-4 hours/week per channel. This reduces it to 10 minutes of setup + automated execution.\n- **Ensures brand consistency**: Automated brand voice checks prevent off-brand content from being published.\n- **Reduces compliance risk**: Automated scanning for prohibited claims, banned topics, and regulatory violations before publishing.\n- **Scales content production**: One person can manage 5+ Instagram accounts with weekly content schedules.\n- **Who benefits**: Marketing teams, social media managers, content creators, agencies managing multiple client accounts.\n- **Frequency**: Weekly execution (Monday 9 AM UTC), with on-demand manual triggers for campaigns.\n\n## What\nFrom a user's perspective:\n1. User opens a GitHub Issue or triggers a workflow with weekly theme + brand profile + post plan\n2. System fetches reference content from provided URLs (blog posts, product pages, announcements)\n3. System generates content for each post type (captions, hooks, CTAs, hashtags, alt text, creative brief)\n4. System runs automated quality review (brand voice match, compliance checks, format validation)\n5. If all quality gates pass AND auto_publish is enabled: Posts are published to Instagram with scheduling\n6. If quality gates fail OR content_pack_only mode: A complete markdown + JSON content pack is committed to the repo with manual upload instructions\n7. User receives notification (GitHub comment or Slack) with summary and links to outputs\n\n### Success Criteria\n- [ ] Generates complete content packs with 3-10 posts (captions, hashtags, creative briefs, alt text, posting times)\n- [ ] Runs 5-dimensional quality review (brand voice, compliance, hashtag hygiene, format validation, claims verification)\n- [ ] Auto-publishes to Instagram Graph API when quality gates pass and mode is \"auto_publish\"\n- [ ] Falls back to manual content packs when quality gates fail or mode is \"content_pack_only\"\n- [ ] Outputs structured JSON + human-readable Markdown for each content pack\n- [ ] Maintains rolling archive of content packs with latest.md index\n- [ ] System runs autonomously via GitHub Actions on schedule\n- [ ] Results are committed back to repo with clear naming (content_pack_2026-02-14.md)\n- [ ] All three execution paths work: CLI, GitHub Actions, Agent HQ\n\n---\n\n## Inputs\n[What goes into the system. Be specific about format, source, and any validation requirements.]\n\n```yaml\n- name: \"brand_profile\"\n  type: \"JSON\"\n  source: \"workflow input or config file (config/brand_profile.json)\"\n  required: true\n  description: \"Brand voice + guardrails defining tone, do/don't lists, target audience, products/services, banned topics, preferred CTAs, emoji style\"\n  example: |\n    {\n      \"brand_name\": \"EcoFlow Energy\",\n      \"tone\": \"friendly, informative, eco-conscious\",\n      \"target_audience\": \"environmentally conscious consumers, ages 25-45\",\n      \"products\": [\"solar panels\", \"home batteries\", \"EV chargers\"],\n      \"banned_topics\": [\"politics\", \"religion\", \"controversial social issues\"],\n      \"prohibited_claims\": [\"guaranteed savings\", \"100% renewable\", \"cure\", \"proven\"],\n      \"preferred_cta\": [\"Learn more\", \"Shop now\", \"Get started\", \"Join the movement\"],\n      \"emoji_style\": \"minimal (1-2 per post)\",\n      \"hashtag_preferences\": {\n        \"count\": \"8-12 per post\",\n        \"avoid\": [\"generic\", \"spammy\", \"banned\"]\n      }\n    }\n\n- name: \"weekly_theme\"\n  type: \"string\"\n  source: \"workflow input\"\n  required: true\n  description: \"The focus/topic for this week's posts (e.g., 'spring launch', 'behind the scenes', 'customer stories', 'product tips')\"\n  example: \"Spring 2026 Product Launch - New Solar Roof Tiles\"\n\n- name: \"post_plan\"\n  type: \"JSON\"\n  source: \"workflow input\"\n  required: true\n  description: \"Number and types of posts to create (reels, carousels, single images), with any required series/templates\"\n  example: |\n    {\n      \"reels\": 3,\n      \"carousels\": 2,\n      \"single_images\": 2,\n      \"stories\": 0,\n      \"series\": [\n        {\"name\": \"Tip Tuesday\", \"count\": 1, \"format\": \"single_image\"},\n        {\"name\": \"Behind the Scenes\", \"count\": 1, \"format\": \"reel\"}\n      ],\n      \"posting_schedule\": \"spread evenly across 7 days, starting Monday 10 AM local time\"\n    }\n\n- name: \"reference_links\"\n  type: \"JSON array\"\n  source: \"workflow input (optional)\"\n  required: false\n  description: \"URLs to recent blog posts, announcements, or product pages to pull accurate details from\"\n  example: |\n    [\n      {\"url\": \"https://ecoflow.com/blog/solar-roof-launch\", \"purpose\": \"product details\"},\n      {\"url\": \"https://ecoflow.com/products/solar-roof-tiles\", \"purpose\": \"specs and pricing\"}\n    ]\n\n- name: \"publishing_mode\"\n  type: \"choice: auto_publish | content_pack_only\"\n  source: \"workflow input\"\n  required: true\n  description: \"Whether to auto-publish to Instagram (if quality gates pass) or always output content packs for manual upload\"\n  example: \"content_pack_only\"\n```\n\n## Outputs\n[What comes out of the system. Where do results go?]\n\n```yaml\n- name: \"content_pack_markdown\"\n  type: \"Markdown\"\n  destination: \"output/instagram/content_pack_{YYYY-MM-DD}.md\"\n  description: \"Human-readable content pack with post-by-post details (hook, caption, CTA, hashtags, alt text, posting time, creative brief)\"\n  example: \"output/instagram/content_pack_2026-02-14.md\"\n\n- name: \"content_pack_json\"\n  type: \"JSON\"\n  destination: \"output/instagram/content_pack_{YYYY-MM-DD}.json\"\n  description: \"Structured data for each post (machine-readable, suitable for automation)\"\n  example: |\n    {\n      \"metadata\": {\n        \"generated_at\": \"2026-02-14T09:15:00Z\",\n        \"theme\": \"Spring 2026 Product Launch\",\n        \"brand\": \"EcoFlow Energy\",\n        \"total_posts\": 7\n      },\n      \"posts\": [\n        {\n          \"post_id\": 1,\n          \"type\": \"reel\",\n          \"posting_time\": \"2026-02-17T10:00:00-08:00\",\n          \"hook\": \"☀️ Introducing the future of home energy\",\n          \"caption\": \"Our new Solar Roof Tiles seamlessly integrate...\",\n          \"cta\": \"Learn more (link in bio)\",\n          \"hashtags\": [\"#SolarRoof\", \"#CleanEnergy\", \"#EcoFlowEnergy\"],\n          \"alt_text\": \"Animated reel showing solar roof tiles installation\",\n          \"creative_brief\": \"15-30s reel: time-lapse of installation, cut to house powering at night\",\n          \"image_prompt\": \"realistic photo: modern house with sleek black solar roof tiles, golden hour lighting\"\n        }\n      ]\n    }\n\n- name: \"review_report\"\n  type: \"Markdown\"\n  destination: \"output/instagram/review_report_{YYYY-MM-DD}.md\"\n  description: \"Quality review results (brand voice match score, compliance checks, hashtag hygiene, format validation)\"\n  example: |\n    # Quality Review Report - 2026-02-14\n    \n    ## Overall Score: 87/100 ✅ PASS\n    \n    ### Brand Voice Alignment: 90/100\n    - Tone match: Excellent (friendly, informative)\n    - Audience fit: Strong (eco-conscious messaging)\n    - Minor issue: Post 3 uses slightly formal language\n    \n    ### Compliance Checks: 100/100\n    - No banned topics detected\n    - No prohibited claims found\n    - All CTAs approved\n    \n    ### Hashtag Hygiene: 85/100\n    - Count: 8-12 per post ✓\n    - No banned hashtags ✓\n    - 2 generic hashtags flagged (#love, #instagood) - consider alternatives\n    \n    ### Format Validation: 80/100\n    - Caption lengths OK (all under 2200 chars)\n    - Alt text present for all posts ✓\n    - 1 creative brief lacks technical detail\n    \n    ### Claims Verification: 100/100\n    - All factual claims sourced from reference links ✓\n    - No unsupported statistics\n\n- name: \"publish_log\"\n  type: \"JSON\"\n  destination: \"output/instagram/publish_log_{YYYY-MM-DD}.json (only if auto_publish mode)\"\n  description: \"Instagram Graph API response for each published post (media IDs, URLs, or failure reasons)\"\n  example: |\n    {\n      \"published_at\": \"2026-02-14T09:20:00Z\",\n      \"mode\": \"auto_publish\",\n      \"results\": [\n        {\n          \"post_id\": 1,\n          \"status\": \"success\",\n          \"ig_media_id\": \"17895695668004550\",\n          \"ig_permalink\": \"https://instagram.com/p/ABC123/\",\n          \"scheduled_for\": \"2026-02-17T10:00:00-08:00\"\n        },\n        {\n          \"post_id\": 2,\n          \"status\": \"failed\",\n          \"error\": \"Rate limit exceeded - retry in 3600s\",\n          \"fallback\": \"content included in manual upload pack\"\n        }\n      ]\n    }\n\n- name: \"upload_checklist\"\n  type: \"Markdown\"\n  destination: \"output/instagram/upload_checklist_{YYYY-MM-DD}.md (only if content_pack_only mode)\"\n  description: \"Manual upload instructions with copy-paste ready captions, hashtag lists, and posting times\"\n  example: \"output/instagram/upload_checklist_2026-02-14.md\"\n\n- name: \"latest_index\"\n  type: \"Markdown\"\n  destination: \"output/instagram/latest.md (updated each run)\"\n  description: \"Rolling index pointing to the most recent content pack with quick links\"\n  example: |\n    # Latest Instagram Content Pack\n    \n    **Generated:** 2026-02-14 09:15 UTC\n    **Theme:** Spring 2026 Product Launch\n    \n    - [Content Pack (Markdown)](./content_pack_2026-02-14.md)\n    - [Content Pack (JSON)](./content_pack_2026-02-14.json)\n    - [Review Report](./review_report_2026-02-14.md)\n    - [Upload Checklist](./upload_checklist_2026-02-14.md)\n```\n\n---\n\n## All Needed Context\n\n### Documentation & References\n```yaml\n# MUST READ — Include these in context when building\n- url: \"https://developers.facebook.com/docs/instagram-api/overview\"\n  why: \"Instagram Graph API overview - authentication, endpoints, rate limits\"\n\n- url: \"https://developers.facebook.com/docs/instagram-api/reference/ig-user/media\"\n  why: \"Creating Instagram posts via API - required fields, media types, error codes\"\n\n- url: \"https://developers.facebook.com/docs/instagram-api/reference/ig-user/media_publish\"\n  why: \"Publishing flow - container creation, media publish, scheduling\"\n\n- url: \"https://developers.facebook.com/docs/instagram-api/guides/content-publishing\"\n  why: \"Content publishing guide - image requirements, video specs, carousel format\"\n\n- url: \"https://developers.facebook.com/docs/graph-api/overview/rate-limiting\"\n  why: \"Rate limits - 200 API calls per hour per user, batch request limits\"\n\n- doc: \"config/mcp_registry.md\"\n  why: \"Check which MCPs provide Firecrawl (reference content), Anthropic (generation), HTTP (Instagram API)\"\n\n- doc: \"library/patterns.md\"\n  why: \"Primary pattern: Generate > Review > Publish. Secondary: Fan-Out > Process > Merge for parallel post generation\"\n\n- doc: \"library/tool_catalog.md\"\n  why: \"Reusable patterns: llm_prompt, structured_extract, multi_dimension_scorer, rest_client, json_read_write\"\n\n- url: \"https://help.instagram.com/478745558852297\"\n  why: \"Instagram Community Guidelines - what content is prohibited\"\n\n- url: \"https://business.instagram.com/blog/instagram-algorithm-ranking-system\"\n  why: \"Best practices for captions, hashtags, engagement optimization\"\n```\n\n### Workflow Pattern Selection\n```yaml\n# Reference library/patterns.md — select the best-fit pattern\npattern: \"Generate > Review > Publish\"\nrationale: |\n  This system follows the classic two-pass content creation flow:\n  1. Generate: Claude produces draft content for all posts\n  2. Review: Automated quality gates check brand voice, compliance, format\n  3. Publish: Either auto-publish to IG API or output manual content packs\n  \n  The review step is critical - it prevents off-brand or non-compliant content from reaching production.\n\nmodifications: |\n  - Add Fan-Out > Process > Merge (Agent Teams) for parallel post generation (3+ posts = independent tasks)\n  - Add Sequential State Management for tracking published posts and preventing duplicates\n  - Gate decision: Quality review score determines auto-publish vs manual fallback\n\nsecondary_pattern: \"Fan-Out > Process > Merge (Agent Teams)\"\nrationale: |\n  If post_plan specifies 3+ posts (e.g., \"3 reels + 2 carousels + 2 singles\" = 7 independent tasks),\n  use Agent Teams to generate all posts in parallel. Each teammate generates one post type,\n  team lead merges results and coordinates review.\n  \n  Sequential generation of 7 posts = ~70-90 seconds\n  Parallel generation with Agent Teams = ~12-18 seconds (5x faster, same token cost)\n```\n\n### MCP & Tool Requirements\n```yaml\n# Reference config/mcp_registry.md — list capabilities needed\ncapabilities:\n  - name: \"Reference content extraction\"\n    primary_mcp: \"firecrawl\"\n    alternative_mcp: \"fetch\"\n    fallback: \"Direct HTTP with requests + BeautifulSoup\"\n    secret_name: \"FIRECRAWL_API_KEY\"\n\n  - name: \"AI content generation\"\n    primary_mcp: \"anthropic\"\n    alternative_mcp: \"openai\"\n    fallback: \"N/A - generation requires LLM\"\n    secret_name: \"ANTHROPIC_API_KEY\"\n\n  - name: \"Instagram Graph API\"\n    primary_mcp: \"none\"\n    alternative_mcp: \"none\"\n    fallback: \"Direct HTTP API via rest_client tool pattern\"\n    secret_name: \"INSTAGRAM_ACCESS_TOKEN\"\n\n  - name: \"Structured data extraction\"\n    primary_mcp: \"anthropic\"\n    alternative_mcp: \"openai\"\n    fallback: \"Regex + manual parsing\"\n    secret_name: \"ANTHROPIC_API_KEY\"\n\n  - name: \"Notifications\"\n    primary_mcp: \"slack (optional)\"\n    alternative_mcp: \"email (smtp)\"\n    fallback: \"GitHub Issue comment\"\n    secret_name: \"SLACK_WEBHOOK_URL or SMTP_PASSWORD\"\n```\n\n### Known Gotchas & Constraints\n```\n# CRITICAL: Instagram Graph API rate limit is 200 calls/hour per user\n# CRITICAL: Media must be publicly accessible HTTPS URLs for Graph API upload\n# CRITICAL: Carousel posts require 2-10 media items, all must be same type (all images or all videos)\n# CRITICAL: Reels must be MP4, H.264 codec, 1080x1920 (9:16), max 90 seconds, max 100MB\n# CRITICAL: Captions max 2200 characters, but optimal is 125-150 for full visibility before \"more\" button\n# CRITICAL: Hashtags count toward caption character limit, optimal count is 8-12 (not 30)\n# CRITICAL: Alt text (accessibility caption) is separate field, max 100 characters\n# CRITICAL: Scheduled posts require Business or Creator account, max 75 scheduled posts at once\n# CRITICAL: Access tokens expire - use long-lived tokens (60 days), implement refresh flow\n# CRITICAL: Cannot publish to other users' accounts - must authenticate as account owner\n# CRITICAL: Brand voice scoring requires examples of on-brand vs off-brand content in the prompt\n# CRITICAL: Compliance checks must include FTC guidelines for sponsored/affiliate content if applicable\n# CRITICAL: Secrets are NEVER hardcoded — always use GitHub Secrets or .env\n# CRITICAL: Every tool must have try/except, logging, type hints, and a main() function\n# CRITICAL: Image prompts for generative AI are guidance only - this system does NOT generate images\n# CRITICAL: Creative briefs are instructions for manual content creation or briefing designers\n```\n\n---\n\n## System Design\n\n### Subagent Architecture\n[Define the specialist subagents this system needs. One subagent per major capability or workflow phase.]\n\n```yaml\nsubagents:\n  - name: \"content-strategist\"\n    description: |\n      Delegate when you need to plan the content strategy for the week:\n      - Analyze brand profile and weekly theme\n      - Determine post mix and sequencing\n      - Create content pillars and messaging framework\n      - Suggest optimal posting times\n      - Coordinate with reference content extraction\n    tools: \"Read, Bash\"\n    model: \"sonnet\"\n    permissionMode: \"default\"\n    responsibilities:\n      - \"Parse brand_profile.json and weekly_theme input\"\n      - \"Fetch and extract content from reference_links URLs (via Firecrawl)\"\n      - \"Synthesize brand voice, theme, and reference material into content strategy\"\n      - \"Generate posting schedule with optimal times\"\n      - \"Create content brief for each post type (reels, carousels, singles)\"\n    inputs: \"brand_profile (JSON), weekly_theme (string), post_plan (JSON), reference_links (array)\"\n    outputs: \"content_strategy.json with post briefs, themes, posting schedule\"\n\n  - name: \"copywriter-specialist\"\n    description: |\n      Delegate when you need to generate Instagram copy (captions, hooks, CTAs):\n      - Write engaging hooks (first 125 chars)\n      - Craft full captions matching brand voice\n      - Generate CTAs aligned with brand preferences\n      - Write alt text for accessibility\n      - Ensure tone, style, and emoji usage match brand guidelines\n    tools: \"Read, Write\"\n    model: \"sonnet\"\n    permissionMode: \"default\"\n    responsibilities:\n      - \"Read content brief from strategist\"\n      - \"Generate hook (attention-grabbing first line)\"\n      - \"Write full caption (125-300 words, brand voice matched)\"\n      - \"Select appropriate CTA from brand preferences\"\n      - \"Write descriptive alt text (max 100 chars)\"\n      - \"Apply emoji style per brand guidelines\"\n    inputs: \"content brief (JSON), brand_profile (JSON)\"\n    outputs: \"post_copy.json with hook, caption, cta, alt_text for each post\"\n\n  - name: \"hashtag-specialist\"\n    description: |\n      Delegate when you need to generate hashtag sets:\n      - Research trending hashtags relevant to theme\n      - Generate mix of broad/niche hashtags (8-12 per post)\n      - Avoid banned, generic, or spammy hashtags\n      - Ensure hashtags match brand preferences\n      - Validate hashtag hygiene\n    tools: \"Read, Bash\"\n    model: \"sonnet\"\n    permissionMode: \"default\"\n    responsibilities:\n      - \"Research hashtags relevant to weekly theme and brand\"\n      - \"Generate 8-12 hashtags per post (mix of sizes)\"\n      - \"Filter out banned, generic, spammy hashtags\"\n      - \"Check against brand hashtag preferences\"\n      - \"Validate total character count (hashtags count toward 2200 limit)\"\n    inputs: \"weekly_theme (string), brand_profile (JSON), post_type (string)\"\n    outputs: \"hashtags.json with 8-12 hashtags per post, size breakdown\"\n\n  - name: \"creative-director\"\n    description: |\n      Delegate when you need to create creative briefs and image prompts:\n      - Write creative briefs for video/image production\n      - Generate AI image prompts (for guidance, not generation)\n      - Specify technical requirements (aspect ratio, format, duration)\n      - Suggest visual themes and styling\n      - Align creative direction with brand guidelines\n    tools: \"Read, Write\"\n    model: \"sonnet\"\n    permissionMode: \"default\"\n    responsibilities:\n      - \"Read content brief and copy\"\n      - \"Write creative brief for each post (what to show, how to shoot/design)\"\n      - \"Generate image prompt for AI guidance (style, composition, mood)\"\n      - \"Specify technical requirements (1080x1080 for feed, 1080x1920 for reels, etc.)\"\n      - \"Ensure visual direction matches brand aesthetic\"\n    inputs: \"content brief (JSON), post_copy (JSON), brand_profile (JSON)\"\n    outputs: \"creative_briefs.json with brief, image_prompt, technical_specs per post\"\n\n  - name: \"reviewer-specialist\"\n    description: |\n      Delegate when you need to run quality review and compliance checks:\n      - Score brand voice alignment (0-100)\n      - Check for banned topics and prohibited claims\n      - Validate hashtag hygiene\n      - Verify Instagram format compliance\n      - Cross-check claims against reference content\n      - Generate review report with pass/fail decision\n    tools: \"Read, Write\"\n    model: \"sonnet\"\n    permissionMode: \"default\"\n    responsibilities:\n      - \"Load all generated content (copy, hashtags, creative briefs)\"\n      - \"Score brand voice alignment across 5 dimensions\"\n      - \"Scan for banned topics from brand_profile\"\n      - \"Check for prohibited claims (guarantees, medical claims, etc.)\"\n      - \"Validate hashtag count, banned hashtags, generic hashtags\"\n      - \"Verify caption length, alt text presence, technical specs\"\n      - \"Cross-reference factual claims with reference content\"\n      - \"Calculate overall quality score\"\n      - \"Generate detailed review report\"\n      - \"Return pass/fail decision for auto-publish gate\"\n    inputs: \"generated_content (JSON), brand_profile (JSON), reference_content (text)\"\n    outputs: \"review_report.json with scores, issues, pass/fail decision\"\n\n  - name: \"instagram-publisher\"\n    description: |\n      Delegate when you need to publish to Instagram Graph API or prepare manual upload:\n      - Format content for Instagram Graph API\n      - Handle media container creation\n      - Execute publish API calls with retry logic\n      - Handle rate limiting (200 calls/hour)\n      - Fall back to manual content pack on API failure\n      - Generate publish log or upload checklist\n    tools: \"Read, Write, Bash\"\n    model: \"sonnet\"\n    permissionMode: \"default\"\n    responsibilities:\n      - \"Check publishing_mode (auto_publish vs content_pack_only)\"\n      - \"If auto_publish: Format posts for Instagram Graph API\"\n      - \"Create media containers for each post (images/videos must be public URLs)\"\n      - \"Execute publish API calls with exponential backoff retry\"\n      - \"Handle rate limits (pause and resume)\"\n      - \"Log publish results (media IDs, permalinks, errors)\"\n      - \"If content_pack_only OR publish fails: Generate manual upload checklist\"\n      - \"Format upload checklist with copy-paste ready captions, hashtags, times\"\n    inputs: \"generated_content (JSON), review_report (JSON), publishing_mode (string)\"\n    outputs: \"publish_log.json OR upload_checklist.md\"\n```\n\n### Agent Teams Analysis\n```yaml\n# Apply the 3+ Independent Tasks Rule\nindependent_tasks:\n  - \"Generate reel post 1 (hook, caption, CTA, hashtags, creative brief, image prompt)\"\n  - \"Generate reel post 2 (independent of post 1)\"\n  - \"Generate reel post 3 (independent of posts 1-2)\"\n  - \"Generate carousel post 1 (independent of reels)\"\n  - \"Generate carousel post 2 (independent of other posts)\"\n  - \"Generate single image post 1 (independent of other posts)\"\n  - \"Generate single image post 2 (independent of other posts)\"\n\nindependent_task_count: \"7 (for a typical post_plan with 3 reels + 2 carousels + 2 singles)\"\nrecommendation: \"Use Agent Teams for post generation phase\"\nrationale: |\n  Each post is an independent unit of work - it doesn't depend on other posts being generated first.\n  Sequential generation: ~10-15 seconds per post × 7 posts = 70-105 seconds\n  Parallel with Agent Teams: ~12-18 seconds (all 7 posts generated simultaneously)\n  \n  Result: 5x wall-time speedup, same token cost, identical quality.\n  \n  Exception: If post_plan specifies fewer than 3 posts, use sequential (overhead not justified).\n\n# Agent Teams structure for post generation\nteam_lead_responsibilities:\n  - \"Read content_strategy.json from content-strategist\"\n  - \"Create post generation task list (one per post in post_plan)\"\n  - \"Spawn 1 teammate per post (or batch into groups of 3-4 if >10 posts)\"\n  - \"Each teammate generates: hook, caption, CTA, hashtags, alt_text, creative_brief, image_prompt\"\n  - \"Merge all teammate results into unified generated_content.json\"\n  - \"Run consistency check (ensure no duplicate hooks/captions across posts)\"\n  - \"Pass to reviewer-specialist for quality gates\"\n\nteammates:\n  - name: \"post-generator-reel-1\"\n    task: \"Generate complete content for Reel Post 1 per content brief\"\n    inputs: \"content_strategy.json (brief for reel 1), brand_profile.json\"\n    outputs: \"reel_1_content.json {hook, caption, cta, hashtags, alt_text, creative_brief, image_prompt}\"\n\n  - name: \"post-generator-carousel-1\"\n    task: \"Generate complete content for Carousel Post 1 per content brief\"\n    inputs: \"content_strategy.json (brief for carousel 1), brand_profile.json\"\n    outputs: \"carousel_1_content.json {hook, caption, cta, hashtags, alt_text, creative_brief, image_prompt}\"\n\n  # ... one teammate per post in post_plan\n\nsequential_fallback: |\n  If Agent Teams is not available OR post count < 3:\n  - Main agent generates posts sequentially\n  - Delegates to copywriter-specialist, hashtag-specialist, creative-director in sequence\n  - Produces identical output structure\n  - Takes longer wall time but requires no parallelization infrastructure\n```\n\n### GitHub Actions Triggers\n```yaml\ntriggers:\n  - type: \"schedule\"\n    config: \"cron: '0 9 * * 1'  # Every Monday at 09:00 UTC\"\n    description: \"Weekly automated content generation for standard posting schedule\"\n\n  - type: \"workflow_dispatch\"\n    config: |\n      inputs:\n        brand_profile_path:\n          description: 'Path to brand profile JSON file'\n          required: true\n          default: 'config/brand_profile.json'\n        weekly_theme:\n          description: 'Weekly theme or campaign focus'\n          required: true\n        post_plan:\n          description: 'JSON post plan (reels, carousels, singles counts)'\n          required: true\n        reference_links:\n          description: 'JSON array of reference URLs (optional)'\n          required: false\n          default: '[]'\n        publishing_mode:\n          description: 'Publishing mode'\n          required: true\n          type: choice\n          options:\n            - content_pack_only\n            - auto_publish\n          default: 'content_pack_only'\n    description: \"Manual trigger for ad-hoc campaigns or testing\"\n\n  - type: \"issues\"\n    config: |\n      types: [opened, labeled]\n      # Trigger when issue is labeled 'instagram-content-request'\n    description: \"Agent HQ pattern - open issue with content request, system processes and comments with results\"\n```\n\n---\n\n## Implementation Blueprint\n\n### Workflow Steps\n[Ordered list of workflow phases. Each step becomes a section in workflow.md.]\n\n```yaml\nsteps:\n  - name: \"1. Input Validation & Setup\"\n    description: \"Parse inputs, validate required fields, load brand profile, initialize output directory structure\"\n    subagent: \"none (main agent)\"\n    tools: [\"validate_inputs.py\", \"setup_output.py\"]\n    inputs: \"brand_profile_path, weekly_theme, post_plan, reference_links, publishing_mode (all from workflow inputs)\"\n    outputs: \"validated_inputs.json, output directory initialized (output/instagram/YYYY-MM-DD/)\"\n    failure_mode: \"Missing required fields, invalid JSON, brand_profile.json not found\"\n    fallback: \"Halt workflow with clear error message listing missing/invalid inputs. Do not proceed.\"\n\n  - name: \"2. Reference Content Extraction\"\n    description: \"Fetch and extract clean content from reference URLs using Firecrawl, fall back to HTTP+BeautifulSoup if Firecrawl unavailable\"\n    subagent: \"content-strategist\"\n    tools: [\"fetch_reference_content.py\"]\n    inputs: \"reference_links (array of URLs), validated_inputs.json\"\n    outputs: \"reference_content.json with extracted text, metadata, links from each URL\"\n    failure_mode: \"URL unreachable, paywall, timeout, invalid response\"\n    fallback: \"Log failed URL, continue with available references. If ALL references fail, proceed without reference content (flag in strategy as 'no reference material').\"\n\n  - name: \"3. Content Strategy Generation\"\n    description: \"Analyze brand voice, weekly theme, post plan, and reference content. Generate content strategy with per-post briefs, themes, posting schedule.\"\n    subagent: \"content-strategist\"\n    tools: [\"generate_content_strategy.py\"]\n    inputs: \"validated_inputs.json, reference_content.json, brand_profile.json\"\n    outputs: \"content_strategy.json with post briefs (one per post in post_plan), posting schedule, content themes\"\n    failure_mode: \"LLM API failure, insufficient context, unclear theme\"\n    fallback: \"Retry once with simplified prompt. If fails again, halt and open GitHub Issue requesting human clarification of theme.\"\n\n  - name: \"4. Post Generation (Parallel with Agent Teams)\"\n    description: \"Generate all post content in parallel (if 3+ posts) using Agent Teams. Each teammate generates one post: hook, caption, CTA, hashtags, alt text, creative brief, image prompt.\"\n    subagent: \"Team lead (coordinates) + teammates (one per post)\"\n    tools: [\"generate_post_content.py (parallelized via Agent Teams)\"]\n    inputs: \"content_strategy.json, brand_profile.json\"\n    outputs: \"generated_content.json with complete content for all posts\"\n    failure_mode: \"LLM API failure, teammate timeout, incomplete generation\"\n    fallback: \"If Agent Teams fails, fall back to sequential generation (main agent + copywriter/hashtag/creative subagents). Slower but produces identical output.\"\n\n  - name: \"5. Quality Review & Compliance Checks\"\n    description: \"Run multi-dimensional quality review: brand voice alignment, compliance (banned topics, prohibited claims), hashtag hygiene, format validation, claims verification.\"\n    subagent: \"reviewer-specialist\"\n    tools: [\"review_content.py\"]\n    inputs: \"generated_content.json, brand_profile.json, reference_content.json\"\n    outputs: \"review_report.json with scores (brand_voice, compliance, hashtags, format, claims), overall score, pass/fail decision, detailed issues list\"\n    failure_mode: \"LLM API failure during review\"\n    fallback: \"Retry review once. If fails, default to FAIL (conservative - do not auto-publish without review). Output content pack for manual review.\"\n\n  - name: \"6. Gate Decision: Auto-Publish vs Manual Content Pack\"\n    description: \"Check review_report.json overall score and publishing_mode. If score >= 80/100 AND mode = auto_publish: proceed to publish. Otherwise: generate manual content pack.\"\n    subagent: \"none (main agent)\"\n    tools: [\"gate_decision.py\"]\n    inputs: \"review_report.json, publishing_mode (from inputs)\"\n    outputs: \"publish_decision.json with action (publish | manual_pack), rationale\"\n    failure_mode: \"N/A (deterministic logic)\"\n    fallback: \"N/A\"\n\n  - name: \"7a. Auto-Publish to Instagram (if gate passes)\"\n    description: \"Format posts for Instagram Graph API, create media containers, execute publish API calls with retry logic, handle rate limiting, log results.\"\n    subagent: \"instagram-publisher\"\n    tools: [\"publish_to_instagram.py\"]\n    inputs: \"generated_content.json, publish_decision.json, INSTAGRAM_ACCESS_TOKEN\"\n    outputs: \"publish_log.json with media IDs, permalinks, scheduled times, or error details\"\n    failure_mode: \"Rate limit (200/hour), auth failure, media URL not accessible, API error\"\n    fallback: \"On rate limit: pause, log next retry time. On auth/media failure: halt publish, fall back to step 7b (manual content pack), log specific error.\"\n\n  - name: \"7b. Generate Manual Content Pack (if gate fails OR auto-publish fails)\"\n    description: \"Generate human-readable Markdown and JSON content packs, plus upload checklist with copy-paste ready captions, hashtags, posting times.\"\n    subagent: \"instagram-publisher\"\n    tools: [\"generate_content_pack.py\", \"generate_upload_checklist.py\"]\n    inputs: \"generated_content.json, review_report.json\"\n    outputs: \"content_pack_YYYY-MM-DD.md, content_pack_YYYY-MM-DD.json, upload_checklist_YYYY-MM-DD.md\"\n    failure_mode: \"File write failure\"\n    fallback: \"Retry write once. If fails, log to stdout so GitHub Actions captures it in logs.\"\n\n  - name: \"8. Update Latest Index & Archive\"\n    description: \"Update latest.md with links to current content pack. Commit all outputs to repo. Maintain rolling archive (keep last 12 weeks, delete older).\"\n    subagent: \"none (main agent)\"\n    tools: [\"update_latest_index.py\", \"archive_cleanup.py\"]\n    inputs: \"All output files from step 7a/7b\"\n    outputs: \"latest.md updated, old archives pruned (12-week retention)\"\n    failure_mode: \"Git commit failure\"\n    fallback: \"Retry commit once. If fails, outputs still exist locally (GitHub Actions artifacts can preserve them).\"\n\n  - name: \"9. Notification & Summary\"\n    description: \"Post summary to GitHub Issue (Agent HQ) or send Slack notification. Include links to outputs, review score, publish status.\"\n    subagent: \"none (main agent)\"\n    tools: [\"send_notification.py\"]\n    inputs: \"review_report.json, publish_log.json OR upload_checklist path, publish_decision.json\"\n    outputs: \"GitHub comment OR Slack message with summary + links\"\n    failure_mode: \"Notification delivery failure (Slack API, GitHub API)\"\n    fallback: \"Log notification failure. Do not halt workflow - outputs are still committed to repo.\"\n```\n\n### Tool Specifications\n[For each tool the system needs. Reference library/tool_catalog.md for reusable patterns.]\n\n```yaml\ntools:\n  - name: \"validate_inputs.py\"\n    purpose: \"Parse and validate all workflow inputs (brand_profile, weekly_theme, post_plan, reference_links, publishing_mode)\"\n    catalog_pattern: \"new\"\n    inputs:\n      - \"brand_profile_path: str — Path to brand profile JSON file\"\n      - \"weekly_theme: str — Weekly content theme\"\n      - \"post_plan: str — JSON string with post counts and types\"\n      - \"reference_links: str — JSON array of URLs (optional)\"\n      - \"publishing_mode: str — auto_publish or content_pack_only\"\n    outputs: \"validated_inputs.json with parsed and validated data\"\n    dependencies: [\"json\", \"pathlib\", \"jsonschema\"]\n    mcp_used: \"none\"\n    error_handling: \"Raises ValueError with specific field if validation fails. Main workflow catches and halts.\"\n\n  - name: \"setup_output.py\"\n    purpose: \"Initialize output directory structure for this run (output/instagram/YYYY-MM-DD/)\"\n    catalog_pattern: \"new\"\n    inputs:\n      - \"base_path: str — Base output path (default: output/instagram)\"\n    outputs: \"Created directories, returns output_dir path\"\n    dependencies: [\"pathlib\"]\n    mcp_used: \"none\"\n    error_handling: \"Creates directories if missing. Logs warning if directory exists (overwrite scenario).\"\n\n  - name: \"fetch_reference_content.py\"\n    purpose: \"Fetch and extract clean content from reference URLs using Firecrawl, fall back to HTTP+BeautifulSoup\"\n    catalog_pattern: \"firecrawl_scrape + rest_client fallback\"\n    inputs:\n      - \"reference_links: list[dict] — Array of {url, purpose} objects\"\n    outputs: \"reference_content.json with {url, content, metadata, success} per link\"\n    dependencies: [\"firecrawl-py\", \"httpx\", \"beautifulsoup4\"]\n    mcp_used: \"firecrawl (primary), none (fallback)\"\n    error_handling: \"Try Firecrawl first. On failure, try HTTP GET + BeautifulSoup. Log failure and continue if both fail.\"\n\n  - name: \"generate_content_strategy.py\"\n    purpose: \"Generate content strategy: post briefs, themes, posting schedule based on brand voice and weekly theme\"\n    catalog_pattern: \"structured_extract\"\n    inputs:\n      - \"brand_profile: dict — Brand voice and guardrails\"\n      - \"weekly_theme: str — Content theme\"\n      - \"post_plan: dict — Post counts and types\"\n      - \"reference_content: dict — Extracted reference material\"\n    outputs: \"content_strategy.json with per-post briefs, themes, schedule\"\n    dependencies: [\"anthropic\"]\n    mcp_used: \"anthropic\"\n    error_handling: \"Retry once with simplified prompt on LLM failure. Raise exception if both attempts fail.\"\n\n  - name: \"generate_post_content.py\"\n    purpose: \"Generate complete post content (hook, caption, CTA, hashtags, alt text, creative brief, image prompt) for all posts. Supports both parallel (Agent Teams) and sequential execution.\"\n    catalog_pattern: \"llm_prompt + structured_extract (parallelized)\"\n    inputs:\n      - \"content_strategy: dict — Post briefs from strategist\"\n      - \"brand_profile: dict — Brand voice guidelines\"\n      - \"use_agent_teams: bool — True if 3+ posts, False otherwise\"\n    outputs: \"generated_content.json with complete content for all posts\"\n    dependencies: [\"anthropic\", \"json\"]\n    mcp_used: \"anthropic\"\n    error_handling: \"If Agent Teams fails, fall back to sequential generation. Log mode used (parallel vs sequential).\"\n\n  - name: \"review_content.py\"\n    purpose: \"Run multi-dimensional quality review: brand voice (0-100), compliance checks, hashtag hygiene, format validation, claims verification. Output pass/fail decision.\"\n    catalog_pattern: \"multi_dimension_scorer + structured_extract\"\n    inputs:\n      - \"generated_content: dict — All post content\"\n      - \"brand_profile: dict — Brand voice and guardrails\"\n      - \"reference_content: dict — For claims verification\"\n    outputs: \"review_report.json with dimension scores, overall score, pass/fail, detailed issues\"\n    dependencies: [\"anthropic\", \"json\"]\n    mcp_used: \"anthropic\"\n    error_handling: \"Retry once on LLM failure. If fails, return FAIL decision (conservative default).\"\n\n  - name: \"gate_decision.py\"\n    purpose: \"Evaluate review score and publishing_mode. Decide: auto-publish OR manual content pack.\"\n    catalog_pattern: \"new\"\n    inputs:\n      - \"review_report: dict — Review scores and pass/fail\"\n      - \"publishing_mode: str — auto_publish or content_pack_only\"\n    outputs: \"publish_decision.json with action (publish | manual_pack) and rationale\"\n    dependencies: [\"json\"]\n    mcp_used: \"none\"\n    error_handling: \"Deterministic logic - no failure modes.\"\n\n  - name: \"publish_to_instagram.py\"\n    purpose: \"Publish posts to Instagram via Graph API. Create media containers, execute publish calls, handle rate limiting, log results.\"\n    catalog_pattern: \"rest_client + oauth_token_refresh\"\n    inputs:\n      - \"generated_content: dict — Posts to publish\"\n      - \"access_token: str — Instagram Graph API token (from env)\"\n      - \"ig_user_id: str — Instagram Business Account ID (from env)\"\n    outputs: \"publish_log.json with media IDs, permalinks, errors\"\n    dependencies: [\"httpx\", \"tenacity\"]\n    mcp_used: \"none (direct HTTP)\"\n    error_handling: \"Retry with exponential backoff on transient errors. On rate limit (429), log and halt. On auth failure, halt and log error.\"\n\n  - name: \"generate_content_pack.py\"\n    purpose: \"Generate human-readable Markdown content pack from generated_content.json\"\n    catalog_pattern: \"json_read_write\"\n    inputs:\n      - \"generated_content: dict — All post content\"\n      - \"review_report: dict — Review scores\"\n      - \"output_dir: str — Where to write file\"\n    outputs: \"content_pack_YYYY-MM-DD.md and content_pack_YYYY-MM-DD.json\"\n    dependencies: [\"json\", \"pathlib\"]\n    mcp_used: \"none\"\n    error_handling: \"Retry write once on IOError. Log error if fails.\"\n\n  - name: \"generate_upload_checklist.py\"\n    purpose: \"Generate manual upload checklist with copy-paste ready captions, hashtags, posting times\"\n    catalog_pattern: \"new\"\n    inputs:\n      - \"generated_content: dict — All post content\"\n      - \"output_dir: str — Where to write file\"\n    outputs: \"upload_checklist_YYYY-MM-DD.md\"\n    dependencies: [\"pathlib\"]\n    mcp_used: \"none\"\n    error_handling: \"Retry write once on IOError.\"\n\n  - name: \"update_latest_index.py\"\n    purpose: \"Update latest.md with links to current content pack\"\n    catalog_pattern: \"new\"\n    inputs:\n      - \"content_pack_path: str — Path to current content pack\"\n      - \"review_report_path: str — Path to review report\"\n      - \"latest_index_path: str — Path to latest.md (default: output/instagram/latest.md)\"\n    outputs: \"Updated latest.md\"\n    dependencies: [\"pathlib\"]\n    mcp_used: \"none\"\n    error_handling: \"Create latest.md if missing. Overwrite existing.\"\n\n  - name: \"archive_cleanup.py\"\n    purpose: \"Prune old content packs (keep last 12 weeks, delete older)\"\n    catalog_pattern: \"new\"\n    inputs:\n      - \"archive_dir: str — Path to archive directory (output/instagram/)\"\n      - \"retention_weeks: int — How many weeks to keep (default: 12)\"\n    outputs: \"List of deleted files\"\n    dependencies: [\"pathlib\", \"datetime\"]\n    mcp_used: \"none\"\n    error_handling: \"Log deletion errors but continue (old archives are not critical).\"\n\n  - name: \"send_notification.py\"\n    purpose: \"Send summary notification to GitHub Issue comment or Slack\"\n    catalog_pattern: \"slack_notify + github_create_issue\"\n    inputs:\n      - \"summary: dict — Workflow summary data\"\n      - \"notification_target: str — github or slack\"\n      - \"target_id: str — Issue number or Slack channel\"\n    outputs: \"Notification sent confirmation\"\n    dependencies: [\"httpx\"]\n    mcp_used: \"slack (optional)\"\n    error_handling: \"Log failure but do not halt workflow. Outputs are in repo regardless of notification success.\"\n```\n\n### Per-Tool Pseudocode\n```python\n# validate_inputs.py\ndef main():\n    # PATTERN: Input validation with JSON schema\n    # Step 1: Parse inputs from args or env\n    args = parse_args()  # brand_profile_path, weekly_theme, post_plan, reference_links, publishing_mode\n\n    # Step 2: Load brand_profile.json\n    # GOTCHA: Path might be relative or absolute\n    brand_profile = json.loads(Path(args.brand_profile_path).read_text())\n\n    # Step 3: Validate required fields in brand_profile\n    required = [\"brand_name\", \"tone\", \"target_audience\", \"products\", \"banned_topics\", \"preferred_cta\"]\n    for field in required:\n        if field not in brand_profile:\n            raise ValueError(f\"Missing required field in brand_profile: {field}\")\n\n    # Step 4: Parse and validate post_plan JSON\n    post_plan = json.loads(args.post_plan)\n    if not any(k in post_plan for k in [\"reels\", \"carousels\", \"single_images\"]):\n        raise ValueError(\"post_plan must specify at least one post type\")\n\n    # Step 5: Parse reference_links (optional)\n    reference_links = json.loads(args.reference_links) if args.reference_links else []\n\n    # Step 6: Validate publishing_mode\n    if args.publishing_mode not in [\"auto_publish\", \"content_pack_only\"]:\n        raise ValueError(f\"Invalid publishing_mode: {args.publishing_mode}\")\n\n    # Step 7: Output validated inputs\n    output = {\n        \"brand_profile\": brand_profile,\n        \"weekly_theme\": args.weekly_theme,\n        \"post_plan\": post_plan,\n        \"reference_links\": reference_links,\n        \"publishing_mode\": args.publishing_mode\n    }\n    print(json.dumps(output, indent=2))\n\n# fetch_reference_content.py\ndef main():\n    # PATTERN: firecrawl_scrape with HTTP fallback\n    # CRITICAL: Firecrawl requires API key, HTTP fallback does not\n    args = parse_args()  # reference_links (JSON array)\n    results = []\n\n    for link in json.loads(args.reference_links):\n        url = link[\"url\"]\n        try:\n            # Try Firecrawl first\n            from firecrawl import FirecrawlApp\n            app = FirecrawlApp(api_key=os.environ[\"FIRECRAWL_API_KEY\"])\n            scraped = app.scrape_url(url, params={\"formats\": [\"markdown\"]})\n            results.append({\n                \"url\": url,\n                \"content\": scraped.get(\"markdown\", \"\"),\n                \"metadata\": scraped.get(\"metadata\", {}),\n                \"success\": True,\n                \"method\": \"firecrawl\"\n            })\n        except Exception as e:\n            logging.warning(f\"Firecrawl failed for {url}: {e}, trying HTTP fallback\")\n            try:\n                # HTTP + BeautifulSoup fallback\n                import httpx\n                from bs4 import BeautifulSoup\n                resp = httpx.get(url, timeout=15, follow_redirects=True)\n                resp.raise_for_status()\n                soup = BeautifulSoup(resp.text, \"html.parser\")\n                # Extract main content (heuristic: get all <p> tags)\n                content = \"\\n\\n\".join(p.get_text() for p in soup.find_all(\"p\"))\n                results.append({\n                    \"url\": url,\n                    \"content\": content,\n                    \"metadata\": {\"title\": soup.title.string if soup.title else \"\"},\n                    \"success\": True,\n                    \"method\": \"http_fallback\"\n                })\n            except Exception as e2:\n                logging.error(f\"Both Firecrawl and HTTP failed for {url}: {e2}\")\n                results.append({\n                    \"url\": url,\n                    \"content\": \"\",\n                    \"metadata\": {},\n                    \"success\": False,\n                    \"error\": str(e2)\n                })\n\n    print(json.dumps({\"reference_content\": results}, indent=2))\n\n# generate_content_strategy.py\ndef main():\n    # PATTERN: structured_extract\n    # CRITICAL: Prompt must include brand voice examples, weekly theme context, reference material\n    args = parse_args()\n    brand_profile = json.loads(args.brand_profile)\n    weekly_theme = args.weekly_theme\n    post_plan = json.loads(args.post_plan)\n    reference_content = json.loads(args.reference_content)\n\n    # Build context for LLM\n    context = f\"\"\"\n    Brand: {brand_profile['brand_name']}\n    Tone: {brand_profile['tone']}\n    Target Audience: {brand_profile['target_audience']}\n    Weekly Theme: {weekly_theme}\n\n    Post Plan:\n    - Reels: {post_plan.get('reels', 0)}\n    - Carousels: {post_plan.get('carousels', 0)}\n    - Single Images: {post_plan.get('single_images', 0)}\n\n    Reference Material:\n    {reference_content}\n\n    Generate a content strategy with per-post briefs.\n    \"\"\"\n\n    schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"post_briefs\": {\"type\": \"array\", \"items\": {\"type\": \"object\"}},\n            \"posting_schedule\": {\"type\": \"array\"},\n            \"content_themes\": {\"type\": \"array\"}\n        },\n        \"required\": [\"post_briefs\", \"posting_schedule\"]\n    }\n\n    # Call structured_extract (uses Anthropic with JSON schema validation)\n    result = structured_extract(context, schema, retries=2)\n    print(json.dumps(result[\"data\"], indent=2))\n\n# generate_post_content.py\ndef main():\n    # PATTERN: llm_prompt + structured_extract (parallelized with Agent Teams if 3+ posts)\n    # CRITICAL: Check post count to decide parallel vs sequential\n    args = parse_args()\n    content_strategy = json.loads(args.content_strategy)\n    brand_profile = json.loads(args.brand_profile)\n    post_briefs = content_strategy[\"post_briefs\"]\n\n    if len(post_briefs) >= 3 and args.use_agent_teams:\n        # Parallel generation with Agent Teams\n        logging.info(\"Using Agent Teams for parallel generation\")\n        # Team lead creates task list, spawns teammates\n        # Each teammate generates one post\n        # Merge results\n        generated_posts = agent_teams_generate(post_briefs, brand_profile)\n    else:\n        # Sequential generation\n        logging.info(\"Using sequential generation\")\n        generated_posts = []\n        for brief in post_briefs:\n            post = generate_single_post(brief, brand_profile)\n            generated_posts.append(post)\n\n    print(json.dumps({\"posts\": generated_posts}, indent=2))\n\n# review_content.py\ndef main():\n    # PATTERN: multi_dimension_scorer + structured_extract\n    # CRITICAL: 5 dimensions - brand_voice, compliance, hashtags, format, claims\n    args = parse_args()\n    generated_content = json.loads(args.generated_content)\n    brand_profile = json.loads(args.brand_profile)\n    reference_content = json.loads(args.reference_content)\n\n    dimensions = [\n        {\"name\": \"brand_voice\", \"max_points\": 100, \"scorer\": score_brand_voice},\n        {\"name\": \"compliance\", \"max_points\": 100, \"scorer\": score_compliance},\n        {\"name\": \"hashtags\", \"max_points\": 100, \"scorer\": score_hashtags},\n        {\"name\": \"format\", \"max_points\": 100, \"scorer\": score_format},\n        {\"name\": \"claims\", \"max_points\": 100, \"scorer\": score_claims}\n    ]\n\n    scores = {}\n    for dim in dimensions:\n        scores[dim[\"name\"]] = dim[\"scorer\"](generated_content, brand_profile, reference_content)\n\n    overall = sum(scores.values()) / len(scores)\n    pass_fail = \"PASS\" if overall >= 80 else \"FAIL\"\n\n    report = {\n        \"scores\": scores,\n        \"overall_score\": overall,\n        \"pass_fail\": pass_fail,\n        \"issues\": []  # Detailed issues list from scorers\n    }\n\n    print(json.dumps(report, indent=2))\n\n# publish_to_instagram.py\ndef main():\n    # PATTERN: rest_client + exponential backoff retry\n    # CRITICAL: Instagram Graph API requires public HTTPS URLs for media\n    # CRITICAL: Rate limit is 200 calls/hour\n    args = parse_args()\n    generated_content = json.loads(args.generated_content)\n    access_token = os.environ[\"INSTAGRAM_ACCESS_TOKEN\"]\n    ig_user_id = os.environ[\"INSTAGRAM_USER_ID\"]\n\n    results = []\n    for post in generated_content[\"posts\"]:\n        try:\n            # Step 1: Create media container (requires media URL)\n            # GOTCHA: Media must be publicly accessible HTTPS URL\n            container_resp = create_media_container(post, ig_user_id, access_token)\n            container_id = container_resp[\"id\"]\n\n            # Step 2: Publish media container\n            publish_resp = publish_media(container_id, ig_user_id, access_token)\n\n            results.append({\n                \"post_id\": post[\"post_id\"],\n                \"status\": \"success\",\n                \"ig_media_id\": publish_resp[\"id\"],\n                \"ig_permalink\": f\"https://instagram.com/p/{publish_resp['id']}\"\n            })\n        except HTTPError as e:\n            if e.response.status_code == 429:\n                # Rate limit hit\n                logging.error(\"Rate limit exceeded, halting publish\")\n                results.append({\n                    \"post_id\": post[\"post_id\"],\n                    \"status\": \"failed\",\n                    \"error\": \"Rate limit exceeded\"\n                })\n                break\n            else:\n                results.append({\n                    \"post_id\": post[\"post_id\"],\n                    \"status\": \"failed\",\n                    \"error\": str(e)\n                })\n\n    print(json.dumps({\"results\": results}, indent=2))\n\n# generate_content_pack.py\ndef main():\n    # PATTERN: json_read_write\n    args = parse_args()\n    generated_content = json.loads(args.generated_content)\n    review_report = json.loads(args.review_report)\n    output_dir = Path(args.output_dir)\n\n    # Write JSON\n    json_path = output_dir / f\"content_pack_{datetime.now().strftime('%Y-%m-%d')}.json\"\n    json_path.write_text(json.dumps(generated_content, indent=2))\n\n    # Write Markdown\n    md_path = output_dir / f\"content_pack_{datetime.now().strftime('%Y-%m-%d')}.md\"\n    md_content = format_content_pack_markdown(generated_content, review_report)\n    md_path.write_text(md_content)\n\n    print(json.dumps({\"json_path\": str(json_path), \"md_path\": str(md_path)}, indent=2))\n```\n\n### Integration Points\n```yaml\nSECRETS:\n  - name: \"FIRECRAWL_API_KEY\"\n    purpose: \"Firecrawl API for reference content extraction\"\n    required: true\n\n  - name: \"ANTHROPIC_API_KEY\"\n    purpose: \"Claude API for content generation and review\"\n    required: true\n\n  - name: \"INSTAGRAM_ACCESS_TOKEN\"\n    purpose: \"Instagram Graph API long-lived access token\"\n    required: true (only for auto_publish mode)\n\n  - name: \"INSTAGRAM_USER_ID\"\n    purpose: \"Instagram Business Account ID for Graph API\"\n    required: true (only for auto_publish mode)\n\n  - name: \"SLACK_WEBHOOK_URL\"\n    purpose: \"Slack notifications (optional)\"\n    required: false\n\nENVIRONMENT:\n  - file: \".env.example\"\n    vars:\n      - \"FIRECRAWL_API_KEY=your_firecrawl_key_here  # Get from firecrawl.dev\"\n      - \"ANTHROPIC_API_KEY=your_anthropic_key_here  # Claude API key\"\n      - \"INSTAGRAM_ACCESS_TOKEN=your_ig_token_here  # Long-lived token (60 days)\"\n      - \"INSTAGRAM_USER_ID=your_ig_user_id_here  # Business account ID\"\n      - \"SLACK_WEBHOOK_URL=https://hooks.slack.com/...  # Optional\"\n\nDEPENDENCIES:\n  - file: \"requirements.txt\"\n    packages:\n      - \"anthropic>=0.28.0  # Claude API client\"\n      - \"firecrawl-py>=0.0.16  # Firecrawl scraping\"\n      - \"httpx>=0.27.0  # HTTP client with retry logic\"\n      - \"beautifulsoup4>=4.12.0  # HTML parsing (fallback)\"\n      - \"tenacity>=8.2.0  # Retry logic\"\n      - \"jsonschema>=4.20.0  # JSON validation\"\n      - \"python-dateutil>=2.8.2  # Date parsing\"\n\nGITHUB_ACTIONS:\n  - trigger: \"schedule (cron: 0 9 * * 1)\"\n    config: \"Weekly Monday 09:00 UTC\"\n  - trigger: \"workflow_dispatch\"\n    config: \"Manual with inputs: brand_profile_path, weekly_theme, post_plan, reference_links, publishing_mode\"\n  - trigger: \"issues (labeled: instagram-content-request)\"\n    config: \"Agent HQ pattern - parse issue body for inputs\"\n```\n\n---\n\n## Validation Loop\n\n### Level 1: Syntax & Structure\n```bash\n# Run FIRST — every tool must pass before proceeding to Level 2\n# AST parse — verify valid Python syntax\npython -c \"import ast; ast.parse(open('tools/validate_inputs.py').read())\"\npython -c \"import ast; ast.parse(open('tools/setup_output.py').read())\"\npython -c \"import ast; ast.parse(open('tools/fetch_reference_content.py').read())\"\npython -c \"import ast; ast.parse(open('tools/generate_content_strategy.py').read())\"\npython -c \"import ast; ast.parse(open('tools/generate_post_content.py').read())\"\npython -c \"import ast; ast.parse(open('tools/review_content.py').read())\"\npython -c \"import ast; ast.parse(open('tools/gate_decision.py').read())\"\npython -c \"import ast; ast.parse(open('tools/publish_to_instagram.py').read())\"\npython -c \"import ast; ast.parse(open('tools/generate_content_pack.py').read())\"\npython -c \"import ast; ast.parse(open('tools/generate_upload_checklist.py').read())\"\npython -c \"import ast; ast.parse(open('tools/update_latest_index.py').read())\"\npython -c \"import ast; ast.parse(open('tools/archive_cleanup.py').read())\"\npython -c \"import ast; ast.parse(open('tools/send_notification.py').read())\"\n\n# Import check — verify no missing dependencies\npython -c \"import importlib; importlib.import_module('tools.validate_inputs')\"\npython -c \"import importlib; importlib.import_module('tools.setup_output')\"\n# ... repeat for all tools\n\n# Structure check — verify main() exists\npython -c \"from tools.validate_inputs import main; assert callable(main)\"\npython -c \"from tools.setup_output import main; assert callable(main)\"\n# ... repeat for all tools\n\n# Subagent file validation — verify YAML frontmatter\npython -c \"import yaml; yaml.safe_load(open('.claude/agents/content-strategist.md').read().split('---')[1])\"\npython -c \"import yaml; yaml.safe_load(open('.claude/agents/copywriter-specialist.md').read().split('---')[1])\"\npython -c \"import yaml; yaml.safe_load(open('.claude/agents/hashtag-specialist.md').read().split('---')[1])\"\npython -c \"import yaml; yaml.safe_load(open('.claude/agents/creative-director.md').read().split('---')[1])\"\npython -c \"import yaml; yaml.safe_load(open('.claude/agents/reviewer-specialist.md').read().split('---')[1])\"\npython -c \"import yaml; yaml.safe_load(open('.claude/agents/instagram-publisher.md').read().split('---')[1])\"\n\n# Expected: All pass with no errors. If any fail, fix before proceeding.\n```\n\n### Level 2: Unit Tests\n```bash\n# Run SECOND — each tool must produce correct output for sample inputs\n# Test validate_inputs with sample data\necho '{\"brand_name\": \"TestBrand\", \"tone\": \"friendly\", \"target_audience\": \"test\", \"products\": [\"test\"], \"banned_topics\": [], \"preferred_cta\": [\"test\"]}' > /tmp/brand_profile.json\npython tools/validate_inputs.py \\\n  --brand_profile_path /tmp/brand_profile.json \\\n  --weekly_theme \"Test Theme\" \\\n  --post_plan '{\"reels\": 2, \"carousels\": 1}' \\\n  --reference_links '[]' \\\n  --publishing_mode \"content_pack_only\"\n# Expected: Valid JSON output with parsed inputs\n\n# Test setup_output\npython tools/setup_output.py --base_path /tmp/test_output\n# Expected: Directories created, no errors\n\n# Test fetch_reference_content with mock URL\npython tools/fetch_reference_content.py \\\n  --reference_links '[{\"url\": \"https://example.com\", \"purpose\": \"test\"}]'\n# Expected: JSON with success/failure per URL (might fail due to network, but should not crash)\n\n# Test generate_content_strategy with sample inputs\npython tools/generate_content_strategy.py \\\n  --brand_profile \"$(cat /tmp/brand_profile.json)\" \\\n  --weekly_theme \"Test Theme\" \\\n  --post_plan '{\"reels\": 2}' \\\n  --reference_content '{}'\n# Expected: JSON with post_briefs array\n\n# Test review_content with sample generated content\necho '{\"posts\": [{\"post_id\": 1, \"caption\": \"Test\", \"hashtags\": [\"#test\"]}]}' > /tmp/generated.json\npython tools/review_content.py \\\n  --generated_content \"$(cat /tmp/generated.json)\" \\\n  --brand_profile \"$(cat /tmp/brand_profile.json)\" \\\n  --reference_content '{}'\n# Expected: JSON with scores and pass/fail decision\n\n# If any tool fails: Read the error, fix the root cause, re-run.\n# NEVER mock to make tests pass — fix the actual code.\n```\n\n### Level 3: Integration Tests\n```bash\n# Run THIRD — verify tools work together as a pipeline\n# Simulate the full workflow with sample data\n\n# Step 1: Validate inputs\npython tools/validate_inputs.py \\\n  --brand_profile_path config/brand_profile.json \\\n  --weekly_theme \"Integration Test\" \\\n  --post_plan '{\"reels\": 3, \"carousels\": 2}' \\\n  --reference_links '[]' \\\n  --publishing_mode \"content_pack_only\" > /tmp/validated.json\n\n# Step 2: Setup output directory\npython tools/setup_output.py --base_path /tmp/integration_test > /tmp/output_dir.json\n\n# Step 3: Generate content strategy\npython tools/generate_content_strategy.py \\\n  --brand_profile \"$(jq -r '.brand_profile' /tmp/validated.json)\" \\\n  --weekly_theme \"$(jq -r '.weekly_theme' /tmp/validated.json)\" \\\n  --post_plan \"$(jq -c '.post_plan' /tmp/validated.json)\" \\\n  --reference_content '{}' > /tmp/strategy.json\n\n# Step 4: Generate post content (sequential mode for testing)\npython tools/generate_post_content.py \\\n  --content_strategy \"$(cat /tmp/strategy.json)\" \\\n  --brand_profile \"$(jq -r '.brand_profile' /tmp/validated.json)\" \\\n  --use_agent_teams false > /tmp/generated.json\n\n# Step 5: Review content\npython tools/review_content.py \\\n  --generated_content \"$(cat /tmp/generated.json)\" \\\n  --brand_profile \"$(jq -r '.brand_profile' /tmp/validated.json)\" \\\n  --reference_content '{}' > /tmp/review.json\n\n# Step 6: Gate decision\npython tools/gate_decision.py \\\n  --review_report \"$(cat /tmp/review.json)\" \\\n  --publishing_mode \"content_pack_only\" > /tmp/decision.json\n\n# Step 7: Generate content pack\npython tools/generate_content_pack.py \\\n  --generated_content \"$(cat /tmp/generated.json)\" \\\n  --review_report \"$(cat /tmp/review.json)\" \\\n  --output_dir /tmp/integration_test\n\n# Step 8: Verify outputs exist\ntest -f /tmp/integration_test/content_pack_*.md || echo \"ERROR: Markdown content pack missing\"\ntest -f /tmp/integration_test/content_pack_*.json || echo \"ERROR: JSON content pack missing\"\n\n# Verify workflow.md references match actual tool files\ngrep -q \"validate_inputs.py\" workflow.md || echo \"ERROR: validate_inputs.py not in workflow.md\"\ngrep -q \"generate_post_content.py\" workflow.md || echo \"ERROR: generate_post_content.py not in workflow.md\"\n\n# Verify CLAUDE.md documents all tools and subagents\ngrep -q \"content-strategist\" CLAUDE.md || echo \"ERROR: content-strategist subagent not in CLAUDE.md\"\ngrep -q \"review_content.py\" CLAUDE.md || echo \"ERROR: review_content.py not in CLAUDE.md\"\n\n# Verify .github/workflows/ YAML is valid\npython -c \"import yaml; yaml.safe_load(open('.github/workflows/weekly-instagram.yml'))\"\n\necho \"Integration test complete. Check for any ERROR messages above.\"\n```\n\n---\n\n## Final Validation Checklist\n- [ ] All tools pass Level 1 (syntax, imports, structure)\n- [ ] All tools pass Level 2 (unit tests with sample data)\n- [ ] Pipeline passes Level 3 (integration test end-to-end)\n- [ ] workflow.md has failure modes and fallbacks for every step\n- [ ] CLAUDE.md documents all tools, subagents, MCPs, and secrets\n- [ ] .github/workflows/ has timeout-minutes and failure notifications\n- [ ] .env.example lists all required environment variables\n- [ ] .gitignore excludes .env, __pycache__/, credentials, temp files\n- [ ] README.md covers all three execution paths (CLI, Actions, Agent HQ)\n- [ ] No hardcoded secrets anywhere in the codebase\n- [ ] Subagent files (.claude/agents/*.md) have valid YAML frontmatter and specific system prompts\n- [ ] requirements.txt lists all Python dependencies with versions\n- [ ] config/brand_profile.json example exists with all required fields\n- [ ] All API rate limits documented in CLAUDE.md (Instagram: 200/hour, Firecrawl: varies by plan)\n- [ ] Auto-publish gate decision is clearly documented (score >= 80/100 AND mode = auto_publish)\n\n---\n\n## Anti-Patterns to Avoid\n- Do not hardcode API keys, tokens, or credentials — use GitHub Secrets or .env\n- Do not use `git add -A` or `git add .` — stage only output files explicitly\n- Do not skip validation because \"it should work\" — run all three levels\n- Do not catch bare `except:` — always catch specific exception types\n- Do not build tools that require interactive input — all tools must run unattended\n- Do not create MCP-dependent tools without HTTP/API fallbacks (Firecrawl → HTTP+BeautifulSoup)\n- Do not design subagents that call other subagents — only the main agent delegates\n- Do not use Agent Teams when fewer than 3 independent tasks exist — sequential is sufficient\n- Do not commit .env files, credentials, or API keys to the repository\n- Do not ignore failing tests — fix the root cause, never mock to pass\n- Do not generate workflow steps without failure modes and fallback actions\n- Do not write tools without try/except, logging, type hints, and a main() function\n- Do not assume Instagram Graph API media URLs are accessible — must be public HTTPS\n- Do not exceed Instagram rate limits (200 calls/hour) — implement rate limit detection and pausing\n- Do not auto-publish without quality review passing — this is a critical safety gate\n- Do not generate more than 12 hashtags per post — Instagram optimal is 8-12, not 30\n- Do not exceed 2200 character caption limit — optimal is 125-150 for full visibility\n\n---\n\n## Confidence Score: 8/10\n\n**Score rationale:**\n- **Workflow pattern match**: High confidence. This is a textbook Generate > Review > Publish pattern with clear two-pass process and quality gates. The pattern is proven in similar systems (lead-gen-machine, marketing-pipeline, content-tone-repurposer). Confidence: **high**\n\n- **Instagram Graph API integration**: Medium-high confidence. The API is well-documented, but has specific requirements (public HTTPS URLs for media, rate limits, auth token lifecycle). The PRP accounts for these, but first-time implementation may encounter edge cases. Fallback to manual content packs mitigates risk. Confidence: **medium-high**\n\n- **Multi-dimensional quality review**: High confidence. The 5-dimension scoring (brand voice, compliance, hashtags, format, claims) is based on proven multi_dimension_scorer pattern from marketing-pipeline. Claude excels at brand voice analysis. The pattern is proven. Confidence: **high**\n\n- **Agent Teams parallelization**: High confidence. Post generation is perfectly suited for Agent Teams (3-7 independent tasks, no data dependencies). The pattern is proven in content-tone-repurposer. Sequential fallback ensures the system works regardless. Confidence: **high**\n\n- **Subagent architecture**: High confidence. 6 specialist subagents (strategist, copywriter, hashtag, creative-director, reviewer, publisher) with clear scopes. Follows the \"one subagent per domain\" rule. No cross-subagent calls. Proven pattern. Confidence: **high**\n\n- **Brand voice matching**: Medium confidence. Brand voice alignment requires examples of on-brand vs off-brand content in the prompt. The PRP includes brand_profile with tone/style, but first run may require prompt tuning. The review step catches mismatches. Confidence: **medium**\n\n- **Reference content extraction**: High confidence. Firecrawl + HTTP fallback pattern is proven (lead-gen-machine, marketing-pipeline). Graceful degradation handles missing references. Confidence: **high**\n\n- **Compliance checks**: Medium confidence. Banned topics and prohibited claims scanning is straightforward keyword/phrase matching + Claude analysis. However, regulatory compliance (FTC guidelines) may require domain expertise. The PRP flags this but doesn't implement full legal review. Confidence: **medium**\n\n**Ambiguity flags** (areas requiring clarification before building):\n- [ ] **Instagram media handling**: The PRP assumes this system does NOT generate images/videos (it generates captions/hashtags/creative briefs only). Image prompts and creative briefs are guidance for manual content creation or briefing designers. Confirm this is correct. If images must be generated, add image generation tools (Replicate, DALL-E) and update workflow.\n\n- [ ] **Instagram authentication flow**: The PRP requires a long-lived Instagram access token (60 days). Does the user already have this token, or does the system need to implement the OAuth flow to obtain it? If OAuth is required, add auth flow tools and documentation.\n\n- [ ] **Scheduled posting**: The PRP mentions \"scheduled posts\" in the Instagram API section. Does the user want immediate publishing or scheduled future publishing? Scheduled publishing has additional API requirements (Business/Creator account, max 75 scheduled posts). Clarify and document in workflow.\n\n**If any ambiguity flag is checked, ask the user to clarify before proceeding. Otherwise, confidence is 8/10 — ready to build.**\n\n---\n\n## Factory Build Prompt\n\nWhen this PRP is ready, execute the build with:\n\n```\n/execute-prp PRPs/weekly-instagram-content-publisher.md\n```\n\nThis will trigger `factory/workflow.md` with the above requirements as structured input.\n"}}],"api":"anthropic-messages","provider":"anthropic","model":"claude-sonnet-4-5-20250929","usage":{"input":14,"output":19019,"cacheRead":9365,"cacheWrite":37191,"totalTokens":65589,"cost":{"input":0.000042000000000000004,"output":0.285285,"cacheRead":0.0028095,"cacheWrite":0.13946625,"total":0.42760275000000003}},"stopReason":"toolUse","timestamp":1771029073211}}
{"type":"message","id":"e00f8c5d","parentId":"d88f09de","timestamp":"2026-02-14T00:36:44.166Z","message":{"role":"toolResult","toolCallId":"toolu_01Tsay1HofnGKsotjjpLFw62","toolName":"write","content":[{"type":"text","text":"Successfully wrote 66479 bytes to PRPs/weekly-instagram-content-publisher.md"}],"isError":false,"timestamp":1771029404165}}
{"type":"message","id":"1f6701d9","parentId":"e00f8c5d","timestamp":"2026-02-14T00:36:53.804Z","message":{"role":"assistant","content":[{"type":"thinking","thinking":"Perfect! I've generated a comprehensive PRP for the weekly Instagram content publisher system. Now I need to commit this to the repository with a commit message that includes the confidence score at the end (as specified in the job requirements).\n\nThe PRP includes:\n- Comprehensive problem description and success criteria\n- Detailed input/output specifications\n- Full context (documentation, MCP registry, patterns, tools)\n- Workflow pattern selection (Generate > Review > Publish + Agent Teams)\n- Complete subagent architecture (6 specialists)\n- Agent Teams analysis (3+ independent tasks rule)\n- Implementation blueprint with 9 workflow steps\n- 13 tool specifications with pseudocode\n- 3-level validation loop\n- Confidence score: 8/10\n- 3 ambiguity flags for user clarification\n\nLet me commit this with the proper format.","thinkingSignature":"Et4HCkYICxgCKkC0jA+RN241KPGQGwBMhrr6/GLW9JuT2DIm+BXDVTrEx79u5aUvXQJmBtpcZuv4u2EMslkDOup5LISyC8xeWCdIEgypW3Xp9hoECetMF40aDISBg2Rd+W7YYP4g7CIw3tsq9ZI3oPcameBG9D1YPOBWiKrTGcw0nyfwxCPzMmtv5lljgusCniSizpjUERBbKsUGEOn06JJo+VUltyO0vBCbhTFFpgUrWGt/aGCL2wR2HVmMdf9ow3kL0Ga2gjsl391719d6KChRJdHZsmb9Hb+RSsoIBG/Hc4dmj/dYkIcjC12wsAVeDFEqlE3C8ryQG1YdYm0VTxxKe8HdSEIddzRoY4Xknl5Nf14/50qzBYWT9Ybr91jWZZYfnTXUzFBvmZmAGFNalFs58XFr9ngfQ6igaBULjvdt+u1cdlUtuFxLDIRowmJSOlX3go1lchSZ9v/0sg617FDKEgOdhEeFF/LzxAorXTdovEUusFN3jxDCL718BMb383b3G7O/UESARik5FV+/iHmQ8NG4OHt+cCThYT18dXoKnFIRI/BOPzSrR7lxU6l6ZZ7FRVIOuF2ImnuWtf5SNC66UhU10zis3Gaml65of1sMXlyFPzjFt+/YJqfOjQ6+IXMTURqtg8sKcf4JoZgmm9qhVRrPHwOVU3z8MU6X3hwlLMFGJ0TTkHAahsyuH/Ijwzp7sUwy1756j0onmNL2Uc2LMGMYDZB7awA6JaodwLUCZXTTThlxFoshFMSuOGO/zZ6Jy1CR7DAAyktuIgTgQLOnf5C4wa2KAPu9emKQnzrMnr1tLX6ceVHctxI2zqOx/83eqaNi0cKVgXB6B7WaANBS4m9gV/cAn1eZCdCIZBxc9sphbnX6qN9WOYUG8w+2xqoi6tBlrGvb2jPS/xCMyfus+TFhqBh+ZXGNE9sim5YD4Oe3HrMtPZ/fHsvC1rqtgZ0Wcg/oDx3D+VCGtthlWHxs6mp1lCC6U8GkTOIee5e+DjMcQi7+13/fEto2XlU8hOzr0ZRJ8ZFoU6YPh16JijkKjV3r77iheY5kLyCE2h+krvCohb5wbovhB9QPSWgc1IUOGrGPtcdyT1ZirPjwfTbiedKqtEWbph9Gp65m8f+Hd2PUyCw5dS7ltI9Xlp76PZIiG0XCMSC5cK1HFznTIm95XAntLV1NZD6Nb7pubq6F5+Us/ZjdcodPLtER1+VF2g00zmkm9P4R/8CGCOp2ubqD7p2lOpZ9reJp9p+zNC91lI1YbWKrn5gVkkXQ/J5aNx7L0uhA6ftSpXGiA2Jc//7iibky5jbsWkPX4muv2akeGAE="},{"type":"text","text":"Excellent! I've generated a comprehensive PRP for the weekly Instagram content publisher system. Let me commit it to the repository."},{"type":"toolCall","id":"toolu_01PRpkiqrBHMffeDmGs5oYXT","name":"bash","arguments":{"command":"cd /job && git add PRPs/weekly-instagram-content-publisher.md"}}],"api":"anthropic-messages","provider":"anthropic","model":"claude-sonnet-4-5-20250929","usage":{"input":13,"output":290,"cacheRead":0,"cacheWrite":65603,"totalTokens":65906,"cost":{"input":0.000039,"output":0.00435,"cacheRead":0,"cacheWrite":0.24601125000000001,"total":0.25040025}},"stopReason":"toolUse","timestamp":1771029404167}}
{"type":"message","id":"b812df7c","parentId":"1f6701d9","timestamp":"2026-02-14T00:36:53.879Z","message":{"role":"toolResult","toolCallId":"toolu_01PRpkiqrBHMffeDmGs5oYXT","toolName":"bash","content":[{"type":"text","text":"(no output)"}],"isError":false,"timestamp":1771029413878}}
{"type":"message","id":"16696e92","parentId":"b812df7c","timestamp":"2026-02-14T00:37:07.235Z","message":{"role":"assistant","content":[{"type":"thinking","thinking":"Good, the file is staged. Now let me commit it with a proper commit message that includes the confidence score at the end as required.","thinkingSignature":"Eq8CCkYICxgCKkAvinjPZ8zY+zrmmEmCfui66c+KMizIlc4DiXHIXIx7tMPsn3A4faKqfzYVqzBngeA6sFkNc4hNx7c17Dulxp8SEgxPnF0D8Y5WqsylKFgaDDvV78NdXzSCEKkf2SIwvvbUcagdibCn5RByzaXFkqB+aP9ewe9v7oUA5U7nm4OWQa7ZCg5ASD1AFN4H46S4KpYBU5BTziLB0xXFuFh9LqSd0XwjGyCH4gCUNiisD5OktfeDPImL6Rtzv+Z0iwVNAoqjZgTWqgsA7GFVM+U9zeIxUtY0szIbTdFWCOlBK9PlFPdu2uOkLH7nbAgZu7c1XP2u0E0CvYZIpu7NCofEjpzr3sdKyMUinNY3bfRDibntT2tspZkBxntjym3/Qf4vE4cJPUyp/qpBGAE="},{"type":"toolCall","id":"toolu_01CayiNGi6DKWytaXebxhBdU","name":"bash","arguments":{"command":"cd /job && git commit -m \"Generate PRP: weekly-instagram-content-publisher\n\nA comprehensive Product Requirements Prompt for an AI-powered weekly Instagram content generation system. The system generates Instagram-ready content packs (captions, hashtags, creative briefs, image prompts) based on brand voice and weekly themes, runs multi-dimensional quality review checks, and either auto-publishes via Instagram Graph API or prepares manual upload packages.\n\nKey features:\n- Two-pass generation with quality gates (Generate > Review > Publish pattern)\n- 6 specialist subagents (strategist, copywriter, hashtag, creative-director, reviewer, publisher)\n- Agent Teams parallelization for 3+ posts (5x speedup)\n- 5-dimension quality review (brand voice, compliance, hashtags, format, claims)\n- Auto-publish with fallback to manual content packs\n- Instagram Graph API integration with rate limiting\n- Reference content extraction via Firecrawl\n- Rolling archive with 12-week retention\n\nWorkflow:\n1. Input validation (brand profile, theme, post plan, references, mode)\n2. Reference content extraction (Firecrawl + HTTP fallback)\n3. Content strategy generation (post briefs, schedule)\n4. Post generation (parallel via Agent Teams if 3+ posts)\n5. Quality review (5-dimension scoring)\n6. Gate decision (auto-publish if score >= 80/100)\n7a. Auto-publish to Instagram OR 7b. Generate manual content pack\n8. Archive management and latest index update\n9. Notification (GitHub comment or Slack)\n\nTools: 13 Python tools covering validation, scraping, generation, review, publishing\nSubagents: 6 specialists with clear scopes and no cross-delegation\nMCPs: Firecrawl (scraping), Anthropic (generation + review)\nSecrets: FIRECRAWL_API_KEY, ANTHROPIC_API_KEY, INSTAGRAM_ACCESS_TOKEN\n\nAmbiguity flags (3):\n- Instagram media handling (assumes NO image generation - only captions/briefs)\n- Instagram auth flow (assumes long-lived token provided by user)\n- Scheduled vs immediate posting (clarify timing requirements)\n\nCONFIDENCE_SCORE: 8/10\""}}],"api":"anthropic-messages","provider":"anthropic","model":"claude-sonnet-4-5-20250929","usage":{"input":12,"output":601,"cacheRead":65603,"cacheWrite":309,"totalTokens":66525,"cost":{"input":0.000036,"output":0.009015,"cacheRead":0.019680899999999998,"cacheWrite":0.00115875,"total":0.029890649999999998}},"stopReason":"toolUse","timestamp":1771029413879}}
{"type":"message","id":"9eedb549","parentId":"16696e92","timestamp":"2026-02-14T00:37:07.251Z","message":{"role":"toolResult","toolCallId":"toolu_01CayiNGi6DKWytaXebxhBdU","toolName":"bash","content":[{"type":"text","text":"[job/fc309de0-da2f-4709-bc2e-acf598fa3b07 3567356] Generate PRP: weekly-instagram-content-publisher\n 1 file changed, 1030 insertions(+), 765 deletions(-)\n"}],"isError":false,"timestamp":1771029427250}}
{"type":"message","id":"84a5f034","parentId":"9eedb549","timestamp":"2026-02-14T00:37:31.696Z","message":{"role":"assistant","content":[{"type":"thinking","thinking":"Perfect! The commit was successful and includes the CONFIDENCE_SCORE: 8/10 at the end of the commit message as required. Now let me create a summary of what was generated and the next steps.","thinkingSignature":"EucCCkYICxgCKkC4VFp6tsgJvusskNR9npibwyS4RVE30BHuUHnUexCsKj7gbsVu+SUkZkfuI6OIqXSXJD5RL3Yda6FZGOgjfw7FEgyFSjkOO4KU+yxR5jMaDGHdCo3FhEUNfy5jYSIwD9Q5ERpFR6ZVy2TG+IRVS/nfqa2SEJ7bVm3d8xTBCX4rCGa9WWO1djDeIGlJg+EaKs4BsAriX8hXV5I9hty7/JCUvOaB2f9pdBva1fw9G6u4bhQJX3/IQxZXZ7bhFXUgVWvqsaq4fDtZCTrqRwTcGtoDSkhyUwWy0QHJV80KbQssEQpNncYRAlkt2jRwmwsugxjvNLlywqoGEAquQ1k0EctOS3nlyK79Vkq1NyhBznqRIUWKrbyYcPTKKmt7W8injad6E7+VLNHP1EoDTiaMZlumHsqRM7YpSLmH2t3qSKF5u7p5Xas34iwXBXamuwVS3aKCubxxOMWiwnHsu6XSPRsYAQ=="},{"type":"text","text":"Perfect! I've successfully generated a comprehensive PRP for the **weekly-instagram-content-publisher** system and committed it to the repository.\n\n## Summary\n\n**PRP Location:** `PRPs/weekly-instagram-content-publisher.md`\n\n**Confidence Score:** 8/10\n\n### What Was Generated\n\nA 1,030-line Product Requirements Prompt that provides complete specifications for building an AI-powered weekly Instagram content generation system with:\n\n#### Core Architecture\n- **Primary Pattern:** Generate > Review > Publish (two-pass with quality gates)\n- **Secondary Pattern:** Fan-Out > Process > Merge (Agent Teams for 3+ posts)\n- **6 Specialist Subagents:** content-strategist, copywriter-specialist, hashtag-specialist, creative-director, reviewer-specialist, instagram-publisher\n- **13 Python Tools:** Full pipeline from input validation to Instagram publishing\n- **3 Execution Paths:** CLI, GitHub Actions (cron + manual), Agent HQ (issue-driven)\n\n#### Key Features\n- **Two-pass quality control:** Generate → Review → Publish with 80/100 score gate\n- **Multi-dimensional review:** 5 dimensions (brand voice, compliance, hashtags, format, claims)\n- **Agent Teams parallelization:** 5x speedup for 3+ posts (parallel generation)\n- **Auto-publish with fallback:** Instagram Graph API with manual content pack fallback\n- **Reference content extraction:** Firecrawl + HTTP fallback for blog posts/product pages\n- **Rolling archive:** 12-week retention with latest.md index\n\n#### Workflow (9 Steps)\n1. Input validation (brand profile, theme, post plan, references, mode)\n2. Reference content extraction (Firecrawl → HTTP fallback)\n3. Content strategy generation (post briefs, schedule)\n4. Post generation (parallel via Agent Teams if 3+ posts, sequential fallback)\n5. Quality review (5-dimension scoring with detailed report)\n6. Gate decision (auto-publish if score ≥ 80/100)\n7. Either publish to Instagram OR generate manual content pack\n8. Archive management and latest index update\n9. Notification (GitHub comment or Slack)\n\n#### Outputs\n- `content_pack_YYYY-MM-DD.md` - Human-readable content pack\n- `content_pack_YYYY-MM-DD.json` - Structured machine-readable data\n- `review_report_YYYY-MM-DD.md` - Quality review results\n- `publish_log_YYYY-MM-DD.json` - Instagram API results (if auto-publish)\n- `upload_checklist_YYYY-MM-DD.md` - Manual upload instructions (if content_pack_only)\n- `latest.md` - Rolling index to most recent content\n\n### Ambiguity Flags (Require Clarification)\n\n**3 areas flagged for user clarification before building:**\n\n1. **Instagram media handling:** The PRP assumes this system generates captions/hashtags/creative briefs ONLY (not actual images/videos). Image prompts and creative briefs are guidance for manual content creation. Confirm this is correct, or add image generation tools if needed.\n\n2. **Instagram authentication:** Requires long-lived Instagram access token (60 days). Does the user have this token, or does the system need to implement OAuth flow?\n\n3. **Scheduled vs immediate posting:** Does the user want immediate publishing or scheduled future publishing? Scheduled posts have additional requirements (Business/Creator account, max 75 scheduled).\n\n### Technical Specifications\n\n**MCPs Used:**\n- Firecrawl (reference content extraction)\n- Anthropic (content generation + quality review)\n\n**API Integrations:**\n- Instagram Graph API (auto-publishing)\n- Direct HTTP fallbacks for all MCPs\n\n**Secrets Required:**\n- `FIRECRAWL_API_KEY`\n- `ANTHROPIC_API_KEY`\n- `INSTAGRAM_ACCESS_TOKEN` (only for auto_publish mode)\n- `INSTAGRAM_USER_ID` (only for auto_publish mode)\n- `SLACK_WEBHOOK_URL` (optional notifications)\n\n**Critical Constraints:**\n- Instagram API rate limit: 200 calls/hour\n- Caption max: 2200 chars (optimal: 125-150 for full visibility)\n- Hashtag optimal: 8-12 per post (not 30)\n- Media must be public HTTPS URLs for Graph API\n\n### Next Steps\n\n**Option 1: Clarify ambiguities first (recommended)**\nUser should clarify the 3 ambiguity flags before building to ensure the system matches exact requirements.\n\n**Option 2: Proceed with assumptions**\nIf the assumptions in the PRP are correct (no image generation, user provides token, immediate publishing), execute:\n\n```bash\n/execute-prp PRPs/weekly-instagram-content-publisher.md\n```\n\nThis will trigger the factory workflow to build the complete WAT system based on this PRP.\n\n### Validation Ready\n\nThe PRP includes complete 3-level validation:\n- **Level 1:** Syntax checks for all 13 tools + 6 subagent files\n- **Level 2:** Unit tests with sample data for each tool\n- **Level 3:** End-to-end integration test simulating full workflow\n\nAll validation commands are documented and ready to execute during the build phase.\n\n**Confidence: 8/10** - Ready to build with high confidence, pending clarification of the 3 ambiguity flags."}],"api":"anthropic-messages","provider":"anthropic","model":"claude-sonnet-4-5-20250929","usage":{"input":12,"output":1317,"cacheRead":65912,"cacheWrite":685,"totalTokens":67926,"cost":{"input":0.000036,"output":0.019755,"cacheRead":0.0197736,"cacheWrite":0.00256875,"total":0.04213335000000001}},"stopReason":"stop","timestamp":1771029427251}}
