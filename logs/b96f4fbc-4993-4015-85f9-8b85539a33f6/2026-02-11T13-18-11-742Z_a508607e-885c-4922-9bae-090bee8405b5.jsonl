{"type":"session","version":3,"id":"a508607e-885c-4922-9bae-090bee8405b5","timestamp":"2026-02-11T13:18:11.742Z","cwd":"/job"}
{"type":"model_change","id":"6962f831","parentId":null,"timestamp":"2026-02-11T13:18:11.743Z","provider":"anthropic","modelId":"claude-sonnet-4-5-20250929"}
{"type":"thinking_level_change","id":"77c0f54f","parentId":"6962f831","timestamp":"2026-02-11T13:18:11.743Z","thinkingLevel":"medium"}
{"type":"message","id":"0a2853a0","parentId":"77c0f54f","timestamp":"2026-02-11T13:18:11.756Z","message":{"role":"user","content":[{"type":"text","text":"\n\n# Your Job\n\nRead the WAT factory instructions in CLAUDE.md and factory/workflow.md.\n\nGenerate a PRP (Product Requirements Prompt) for a system called \"content-repurposer\".\n\nProblem description:\nBuild a multi-channel content repurposing system that takes a blog post URL, extracts the content, and generates a Twitter thread, LinkedIn post, email newsletter section, and Instagram caption — each optimized for the platform's format and character limits. Include tone analysis of the source content and style matching per platform. Generate content only (no auto-posting). Analyze and match the source blog's tone automatically. Suggest hashtags and mentions per platform. Save all outputs to a single JSON file with one key per platform.\n\nFollow the PRP generation process:\n1. Read library/patterns.md and library/tool_catalog.md for reusable patterns\n2. Read config/mcp_registry.md for available MCPs\n3. Read PRPs/templates/prp_base.md for the PRP template\n4. Generate the PRP and save it to PRPs/content-repurposer.md\n5. Log the confidence score and any ambiguity flags\n\nIMPORTANT: End your final commit message with exactly this line:\nCONFIDENCE_SCORE: X/10\nwhere X is the PRP's confidence score. This is used for automated pipeline decisions."}],"timestamp":1770815891750}}
{"type":"message","id":"cbddc8cb","parentId":"0a2853a0","timestamp":"2026-02-11T13:18:19.746Z","message":{"role":"assistant","content":[{"type":"thinking","thinking":"Alright, let me break down this job:\n\n1. I need to read the WAT factory instructions from CLAUDE.md and factory/workflow.md\n2. Generate a PRP for a \"content-repurposer\" system\n3. Follow the PRP generation process:\n   - Read library/patterns.md\n   - Read library/tool_catalog.md\n   - Read config/mcp_registry.md\n   - Read PRPs/templates/prp_base.md\n   - Generate the PRP at PRPs/content-repurposer.md\n   - Log confidence score and ambiguity flags\n4. Include confidence score in commit message\n\nLet me start by reading the necessary files.","thinkingSignature":"Er8FCkYICxgCKkC7+31weK7vkQM4oU+yB4IlBLXw9ikOXZ6OfWR2VCcOmfgHZ+fybQijJZNQv8BpqZ/t9Th7Xxl7Cp3NTcprqUPsEgwDop3wHESOTkvfelMaDCyes7w20VGS8DueuiIwLS00bOyRXklwqEeqtPIW9rO/VzSjHr0yEVCbZrR+8odL6pwqyIFYztG1O6tTZhD/KqYE+GDplbRK5DgdKe+IMP4CJI8YVkxG/GKJw7dj7aKzQy5PAwH7USqBP++OSrk7OhnfqnDX/Ljdl4oLPKVMJbSXLlyIR5/wxhVuFhhBPeJ3w07as1p/KDWdJ77cnZQ6zfcP+oMtGBFL/LN2epl+bYsFQNrNgZk68+yEM4r18eOoFOvsSKZkHPEgxb+iLbKrPe+BwX1wc/hOAnSNXkz+PsCgSenlMgsvBu7FPBo1hvbM6/MVPgf6Z2sYGybk+6v8kEGd3Ln7ZbAlE/gEG6v73iP39Mu+5K7F32Dd48rGA5XwxhEdPiMto35WDTyQOzHTgHeQFkxzVXWe03wk++1yMe33/bGn8pJBTVfL6vRsX1635yaSKGEynTN91NJPyE/N6nSUWG+B91vLDstAZDJlv0eveh9i2dbRbex1cvW1JQSF2TF3nFRxb6PfhztyjuJJ8REGjlZmefQe7i/yQEh7V/Yks6o6r+6zWdmcUpQ86RHVLV9zRG6XzYxyMcwaOb+gCNeHB2qCv6JQHZ7aLLH34c0TXPAJ3c/pCcgMsldcy+/npzb3UROF+0Id+/PdG6mtujkhxV4AMsoEC1cg2qsjt0XIh/iim/sM3Wj1Gsur9moDcKnr2VdFm8S6lXHq9Uz6IAXpN8xTN+XGQ5zeD+seYd5Y9ZJa3ZES1xJl5htgcTqipUsnMAn/R8FBfBUyAQevQo6CECizVcPjKB/KinytxCFXK7AS8sDtjhgB"},{"type":"text","text":"I'll generate a comprehensive PRP for the content-repurposer system. Let me start by reading the necessary reference files."},{"type":"toolCall","id":"toolu_016aYJxLDNhUPFiuSAQzsk5U","name":"read","arguments":{"path":"CLAUDE.md"}},{"type":"toolCall","id":"toolu_01Pdu7VsQKS7Pv9Gdtupa6zK","name":"read","arguments":{"path":"factory/workflow.md"}},{"type":"toolCall","id":"toolu_01HBgjPCHa7gACixRbgY2UXD","name":"read","arguments":{"path":"library/patterns.md"}},{"type":"toolCall","id":"toolu_011Kz6FdQ8sApbvd9MLENidX","name":"read","arguments":{"path":"library/tool_catalog.md"}},{"type":"toolCall","id":"toolu_012DUCPo8zoejFH1syCv5hm7","name":"read","arguments":{"path":"config/mcp_registry.md"}}],"api":"anthropic-messages","provider":"anthropic","model":"claude-sonnet-4-5-20250929","usage":{"input":10,"output":405,"cacheRead":0,"cacheWrite":8662,"totalTokens":9077,"cost":{"input":0.00003,"output":0.0060750000000000005,"cacheRead":0,"cacheWrite":0.0324825,"total":0.0385875}},"stopReason":"toolUse","timestamp":1770815891752}}
{"type":"message","id":"d7292592","parentId":"cbddc8cb","timestamp":"2026-02-11T13:18:19.753Z","message":{"role":"toolResult","toolCallId":"toolu_016aYJxLDNhUPFiuSAQzsk5U","toolName":"read","content":[{"type":"text","text":"# WAT Factory Bot\n\nThis document explains the WAT Factory Bot codebase for AI assistants working on this project. The WAT Factory Bot merges two systems: **thepopebot** (autonomous AI agent with Telegram chat, cron scheduling, and Docker-based job execution) and the **WAT Systems Factory** (a meta-system that builds deployable AI agent systems from natural language descriptions).\n\nThe bot can chat, handle tasks, run autonomous jobs -- AND build complete WAT systems (Workflows, Agents, Tools) when asked.\n\n---\n\n## Part 1: Agent Architecture (thepopebot)\n\n### Two-Layer Architecture\n\n```\n┌──────────────────────────────────────────────────────────────────────────┐\n│                       WAT Factory Bot Architecture                        │\n├──────────────────────────────────────────────────────────────────────────┤\n│                                                                          │\n│   ┌──────────────────┐                                                   │\n│   │  Event Handler   │                                                   │\n│   │  ┌────────────┐  │         1. create-job                            │\n│   │  │  Telegram  │  │ ─────────────────────────►  ┌──────────────────┐ │\n│   │  │   Cron     │  │                             │      GitHub      │ │\n│   │  │   Chat     │  │ ◄─────────────────────────  │  (job/* branch)  │ │\n│   │  └────────────┘  │   5. update-event-handler.yml calls             │ │\n│   │                  │      /github/webhook        └────────┬─────────┘ │\n│   └──────────────────┘                                      │           │\n│            │                                                │           │\n│            │                           2. run-job.yml       │           │\n│            ▼                              triggers          │           │\n│   ┌──────────────────┐                                      │           │\n│   │ Telegram notifies│                                      ▼           │\n│   │ user of job done │                         ┌──────────────────────┐ │\n│   └──────────────────┘                         │    Docker Agent      │ │\n│                                                │  ┌────────────────┐  │ │\n│                                                │  │ 1. Clone       │  │ │\n│                                                │  │ 2. Run Pi      │  │ │\n│                                                │  │ 3. Commit      │  │ │\n│                                                │  │ 4. Create PR   │  │ │\n│                                                │  └────────────────┘  │ │\n│                                                └──────────┬───────────┘ │\n│                                                           │             │\n│                                                           │ 3. PR opens │\n│                                                           ▼             │\n│                                                ┌──────────────────────┐ │\n│                                                │       GitHub         │ │\n│                                                │    (PR opened)       │ │\n│                                                │ 4. auto-merge.yml    │ │\n│                                                │ 5. update-event-     │ │\n│                                                │    handler.yml       │ │\n│                                                └──────────────────────┘ │\n│                                                                          │\n└──────────────────────────────────────────────────────────────────────────┘\n```\n\n### Directory Structure\n\n```\n/\n├── .github/workflows/\n│   ├── auto-merge.yml           # Auto-merges job PRs (checks AUTO_MERGE + ALLOWED_PATHS)\n│   ├── docker-build.yml         # Builds and pushes Docker image to GHCR\n│   ├── run-job.yml              # Runs Docker agent when job/* branch created\n│   └── update-event-handler.yml # Notifies event handler when PR opened\n├── .claude/\n│   ├── commands/                # Slash commands for Claude Code\n│   │   ├── generate-prp.md     # /generate-prp -- creates PRP from problem description\n│   │   └── execute-prp.md      # /execute-prp -- builds system from PRP\n│   └── skills/                  # Reusable skill modules\n├── .pi/\n│   ├── extensions/              # Pi extensions (env-sanitizer for secret filtering)\n│   └── skills/                  # Custom skills for the Pi agent\n├── config/                      # MCP registry, GitHub config\n├── converters/                  # n8n-to-WAT format converters\n├── docs/                        # Additional documentation\n├── event_handler/               # Event Handler orchestration layer (DO NOT MODIFY)\n│   ├── server.js                # Express HTTP server (webhooks, Telegram, GitHub)\n│   ├── actions.js               # Shared action executor (agent, command, http)\n│   ├── cron.js                  # Cron scheduler (loads CRONS.json)\n│   ├── cron/                    # Working directory for command-type cron jobs\n│   ├── triggers.js              # Webhook trigger middleware (loads TRIGGERS.json)\n│   ├── triggers/                # Working directory for command-type trigger scripts\n│   ├── claude/\n│   │   ├── index.js             # Claude API integration for chat\n│   │   ├── tools.js             # Tool definitions (create_job, get_job_status)\n│   │   └── conversation.js      # Chat history management\n│   └── tools/\n│       ├── create-job.js        # Job creation via GitHub API\n│       ├── github.js            # GitHub REST API helper + job status\n│       └── telegram.js          # Telegram bot integration\n├── factory/                     # WAT Systems Factory build engine\n│   ├── workflow.md              # The factory's own build process\n│   ├── tools/                   # Factory tool implementations (Python)\n│   └── templates/               # Templates for generated artifacts\n├── library/                     # Learned patterns and reusable tool catalog\n│   ├── patterns.md              # Proven workflow patterns\n│   └── tool_catalog.md          # Reusable tool patterns\n├── operating_system/            # Agent character and behavior\n│   ├── SOUL.md                  # Agent personality and identity\n│   ├── CHATBOT.md               # Telegram chat system prompt\n│   ├── JOB_SUMMARY.md           # Job completion summary prompt\n│   ├── HEARTBEAT.md             # Periodic check instructions\n│   ├── CRONS.json               # Scheduled job definitions\n│   └── TRIGGERS.json            # Webhook trigger definitions\n├── PRPs/                        # Product Requirements Prompts\n│   ├── templates/\n│   │   └── prp_base.md          # Base PRP template\n│   └── {system-name}.md         # Generated PRPs (one per system request)\n├── systems/                     # Output: completed WAT systems\n│   └── {system}/                # Each system is self-contained\n├── setup/                       # Interactive setup wizard\n├── logs/                        # Per-job directories (job.md + session logs)\n├── INITIAL.md                   # Simple intake template for system requests\n├── Dockerfile                   # Container definition (DO NOT MODIFY)\n├── entrypoint.sh                # Container startup script (DO NOT MODIFY)\n└── SECURITY.md                  # Security documentation\n```\n\n### Key Files\n\n| File | Purpose |\n|------|---------|\n| `operating_system/SOUL.md` | Agent personality and identity |\n| `operating_system/CHATBOT.md` | System prompt for Telegram chat |\n| `operating_system/JOB_SUMMARY.md` | Prompt for summarizing completed jobs |\n| `logs/<JOB_ID>/job.md` | The specific task for the agent to execute |\n| `Dockerfile` | Builds the agent container (Node.js 22, Playwright, Pi) |\n| `entrypoint.sh` | Container startup script - clones repo, runs agent, commits results |\n| `.pi/extensions/env-sanitizer/` | Filters secrets from LLM's bash subprocess environment |\n| `factory/workflow.md` | The factory's build process for generating WAT systems |\n| `library/patterns.md` | Proven workflow patterns for system generation |\n| `library/tool_catalog.md` | Reusable tool patterns |\n| `PRPs/templates/prp_base.md` | Base PRP template for system specifications |\n| `INITIAL.md` | Simple intake template for non-technical users |\n\n### Event Handler Layer\n\nThe Event Handler is a Node.js Express server that provides orchestration capabilities.\n\n**DO NOT MODIFY** any files in `event_handler/`, `.github/workflows/`, `entrypoint.sh`, or `Dockerfile`.\n\n#### Endpoints\n\n| Endpoint | Method | Purpose |\n|----------|--------|---------|\n| `/webhook` | POST | Generic webhook for job creation (requires API_KEY) |\n| `/telegram/webhook` | POST | Telegram bot webhook for conversational interface |\n| `/telegram/register` | POST | Register Telegram webhook URL |\n| `/github/webhook` | POST | Receives notifications from GitHub Actions |\n| `/jobs/status` | GET | Check status of a running job |\n\n#### Components\n\n- **server.js** - Express HTTP server handling all webhook routes\n- **cron.js** - Loads CRONS.json and schedules jobs using node-cron\n- **triggers.js** - Loads TRIGGERS.json and returns Express middleware for webhook triggers\n- **claude/** - Claude API integration for Telegram chat with tool use\n- **tools/** - Job creation, GitHub API, and Telegram utilities\n\n#### Action Types: `agent`, `command`, and `http`\n\nBoth cron jobs and webhook triggers use the same shared dispatch system (`event_handler/actions.js`). Every action has a `type` field -- `\"agent\"` (default), `\"command\"`, or `\"http\"`.\n\n| | `agent` | `command` | `http` |\n|---|---------|-----------|--------|\n| **Uses LLM** | Yes -- spins up Pi in a Docker container | No -- runs a shell command directly | No -- makes an HTTP request |\n| **Thinking** | Can reason, make decisions, write code | No thinking, just executes | No thinking, just sends a request |\n| **Runtime** | Minutes to hours (full agent lifecycle) | Milliseconds to seconds | Milliseconds to seconds |\n| **Cost** | LLM API calls + GitHub Actions minutes | Free (runs on event handler) | Free (runs on event handler) |\n\nIf the task needs to *think*, use `agent`. If it just needs to *do*, use `command`. If it needs to *call an external service*, use `http`.\n\n##### Type: `agent` (default)\n\nCreates a full Docker Agent job via `createJob()`. This pushes a `job/*` branch to GitHub, which triggers `run-job.yml` to spin up the Docker container with Pi. The `job` string is passed directly as-is to the LLM as its task prompt (written to `logs/<JOB_ID>/job.md` on the job branch).\n\n**Best practice:** Keep the `job` field short. Put detailed task instructions in a dedicated markdown file in `operating_system/` and reference it by path:\n\n```json\n\"job\": \"Read the file at operating_system/MY_TASK.md and complete the tasks described there.\"\n```\n\n##### Type: `command`\n\nRuns a shell command directly on the event handler server. No Docker container, no GitHub branch, no LLM. Working directories:\n- **Crons**: `event_handler/cron/`\n- **Triggers**: `event_handler/triggers/`\n\n##### Type: `http`\n\nMakes an HTTP request to an external URL. No Docker container, no LLM.\n\n**Outgoing body logic:**\n- `GET` requests skip the body entirely\n- `POST` (default) sends `{ ...vars }` if no incoming data, or `{ ...vars, data: <incoming payload> }` when triggered by a webhook\n\n### Cron Jobs\n\nCron jobs are defined in `operating_system/CRONS.json` and loaded by `event_handler/cron.js` at startup using `node-cron`.\n\n| Field | Description | Required |\n|-------|-------------|----------|\n| `name` | Display name for logging | Yes |\n| `schedule` | Cron expression (e.g., `*/30 * * * *`) | Yes |\n| `type` | `agent` (default), `command`, or `http` | No |\n| `job` | Task description for agent type | For `agent` |\n| `command` | Shell command for command type | For `command` |\n| `url` | Target URL for http type | For `http` |\n| `enabled` | Set to `false` to disable without deleting | No |\n\n### Webhook Triggers\n\nWebhook triggers are defined in `operating_system/TRIGGERS.json` and loaded by `event_handler/triggers.js` as Express middleware. They fire actions when existing endpoints are hit.\n\nTemplate tokens for `job` and `command` strings:\n- `{{body}}` -- full request body as JSON\n- `{{body.field}}` -- a specific field from the body\n- `{{query}}` / `{{query.field}}` -- query string params\n- `{{headers}}` / `{{headers.field}}` -- request headers\n\n### Environment Variables (Event Handler)\n\n| Variable | Description | Required |\n|----------|-------------|----------|\n| `API_KEY` | Authentication key for /webhook endpoint | Yes |\n| `GH_TOKEN` | GitHub PAT for creating branches/files | Yes |\n| `GH_OWNER` | GitHub repository owner | Yes |\n| `GH_REPO` | GitHub repository name | Yes |\n| `PORT` | Server port (default: 3000) | No |\n| `TELEGRAM_BOT_TOKEN` | Telegram bot token from BotFather | For Telegram |\n| `TELEGRAM_WEBHOOK_SECRET` | Secret for webhook validation | No |\n| `GH_WEBHOOK_SECRET` | Secret for GitHub Actions webhook auth | For notifications |\n| `ANTHROPIC_API_KEY` | Claude API key for chat functionality | For chat |\n| `EVENT_HANDLER_MODEL` | Claude model for chat (default: claude-sonnet-4) | No |\n\n### Docker Agent Layer\n\nThe Dockerfile creates a container with Node.js 22, Pi coding agent, Playwright + Chromium, and Git + GitHub CLI.\n\n#### Runtime Flow (entrypoint.sh)\n\n1. Extract Job ID from branch name (job/uuid -> uuid) or generate UUID\n2. Start headless Chrome (CDP on port 9222)\n3. Decode `SECRETS` from base64, parse JSON, export each key as env var (filtered from LLM's bash)\n4. Decode `LLM_SECRETS` from base64, parse JSON, export each key as env var (LLM can access these)\n5. Configure Git credentials via `gh auth setup-git`\n6. Clone repository branch to `/job`\n7. Run Pi with SOUL.md + job.md as prompt\n8. Save session log to `logs/{JOB_ID}/`\n9. Commit all changes\n10. Create PR via `gh pr create` (auto-merge handled by `auto-merge.yml`)\n\n### GitHub Actions\n\nGitHub Actions automate the job lifecycle. **DO NOT MODIFY** these workflow files.\n\n| Workflow | Trigger | Purpose |\n|----------|---------|---------|\n| `docker-build.yml` | Push to `main` | Builds Docker image, pushes to GHCR |\n| `run-job.yml` | `job/*` branch created | Runs Docker agent container |\n| `auto-merge.yml` | PR opened from `job/*` | Checks ALLOWED_PATHS, merges PR |\n| `update-event-handler.yml` | After auto-merge completes | Notifies event handler |\n\n#### GitHub Repository Variables\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `GH_WEBHOOK_URL` | Event handler URL | -- |\n| `AUTO_MERGE` | Set to `false` to disable auto-merge | Enabled |\n| `ALLOWED_PATHS` | Comma-separated path prefixes for auto-merge | `/logs` |\n| `IMAGE_URL` | Full Docker image path | Not set (uses `stephengpope/thepopebot:latest`) |\n| `MODEL` | Anthropic model ID for the Pi agent | Not set (Pi default) |\n\n**IMPORTANT:** For the WAT Factory Bot, set `ALLOWED_PATHS` to `/systems,/library,/PRPs,/logs,/factory` so that factory-generated system files can be auto-merged.\n\n### How Credentials Work\n\nCredentials are passed via base64-encoded JSON in the `SECRETS` environment variable. The `env-sanitizer` extension filters these from the LLM's bash subprocess. For credentials the LLM needs access to (browser logins, skill API keys), use `LLM_SECRETS` instead.\n\n### The Operating System\n\nFiles in `operating_system/` define the agent's character and behavior:\n\n- **SOUL.md** - Personality, identity, and values\n- **CHATBOT.md** - System prompt for Telegram chat\n- **JOB_SUMMARY.md** - Prompt for summarizing completed jobs\n- **HEARTBEAT.md** - Self-monitoring behavior\n- **CRONS.json** - Scheduled job definitions\n- **TRIGGERS.json** - Webhook trigger definitions\n\n### Session Logs\n\nEach job gets its own directory at `logs/{JOB_ID}/` containing job.md and session logs (.jsonl).\n\n### Markdown File Includes\n\nMarkdown files in `operating_system/` support `{{filepath}}` include syntax, powered by `event_handler/utils/render-md.js`. Paths resolve relative to the repo root. Recursive includes supported with circular protection.\n\n---\n\n## Part 2: WAT Systems Factory\n\nThe WAT Systems Factory is a meta-system that builds deployable AI agent systems. Every system it produces follows the WAT framework (Workflows, Agents, Tools) and runs autonomously via GitHub Actions.\n\n### WAT Framework\n\nThree components make up every WAT system:\n\n- **Agent**: Claude Code. The AI brain that executes workflows, makes decisions, and runs tools.\n- **Workflows**: Markdown (.md) files. Natural language process instructions.\n- **Tools**: Python (.py) files. Executable actions the agent performs.\n\n### Factory Build Process\n\nTo build a new WAT system, follow `factory/workflow.md`:\n\n1. **Intake** -- Receive PRP file, problem description, or n8n JSON\n2. **Research** -- Check MCP registry and tool catalog\n3. **Design** -- Architect the workflow, identify subagents, analyze Agent Teams\n4. **Generate Workflow** -- Write workflow.md\n5. **Generate Tools** -- Write Python tools\n5b. **Generate Subagents** -- Write .claude/agents/ files\n6. **Generate GitHub Actions** -- Create .github/workflows/ files\n7. **Generate CLAUDE.md** -- Write operating instructions\n8. **Test** -- 3-level validation gates (syntax, unit, integration)\n9. **Package** -- Bundle into self-contained system\n10. **Learn** -- Extract patterns, update library\n\n### PRP Workflow (Recommended Intake Path)\n\nThe PRP (Product Requirements Prompt) is a structured intake layer. It expands a simple problem description into a comprehensive, validated specification before the factory builds anything.\n\n**Step 1: Generate PRP** -- Takes a problem description, researches patterns/tools/MCPs, designs subagent architecture, produces a full PRP at `PRPs/{system-name}.md` with a confidence score (1-10).\n\n**Step 2: Review, then Build** -- Reads the PRP, validates confidence score (halts if below 7/10), checks ambiguity flags, executes `factory/workflow.md`, runs 3-level validation, packages into `systems/{system-name}/`.\n\n### WAT Framework Rules\n\n#### Workflow Rules\n- All workflows MUST be Markdown (.md) files\n- Use clear headers (##) for each phase\n- Every workflow must declare: **Inputs**, **Outputs**, **Failure Modes**, and **Fallback Steps**\n\n#### Tool Rules\n- All tools MUST be Python (.py) files\n- Every tool MUST have: module-level docstring, `main()` function, `try/except` error handling, logging, type hints\n- Tools must be independently executable: `python tool_name.py`\n\n#### System Rules\nEvery generated WAT system MUST be a self-contained repo containing:\n- `CLAUDE.md`, `workflow.md`, `tools/`, `.claude/agents/`, `.github/workflows/`, `requirements.txt`, `README.md`, `.env.example`, `.gitignore`\n\n### Subagent Rules\n\nCustom subagents are the DEFAULT delegation mechanism in every WAT system. Each system gets specialist subagent definitions as `.claude/agents/` markdown files.\n\n- One subagent per domain\n- Minimal tool access (principle of least privilege)\n- Detailed system prompts\n- Naming: `{function}-specialist`, `{function}-agent`, `{function}-reviewer`\n- No cross-subagent calls -- only the main agent delegates\n\n### Agent Teams Rules\n\nAgent Teams is for parallel multi-agent execution. Use the **3+ Independent Tasks Rule**: if the system has 3+ tasks with no data dependencies, recommend Agent Teams. Otherwise, sequential execution is sufficient.\n\n- ALWAYS provide a sequential fallback\n- Each teammate gets a focused, scoped task\n- The team lead merges results and does final quality control\n\n### Self-Improvement Mechanism\n\nThe factory learns from every system it builds:\n1. After each build, update `library/patterns.md` with new patterns\n2. After each build, update `library/tool_catalog.md` with new tools\n3. Track what works and what fails\n\n### Quality Standards\n\n#### Tool Quality Gates\n- Every tool must have `try/except` error handling\n- Every tool must log via `logging`\n- Every tool must have clear docstrings\n- Every tool should exit 0 on success, non-zero on failure\n\n#### Workflow Quality Gates\n- Every workflow must define failure modes and fallbacks\n- Every workflow must declare inputs and outputs\n\n### File References (Factory)\n\n| File | Purpose |\n|------|---------|\n| `INITIAL.md` | Simple intake template -- fill out and generate PRP |\n| `.claude/commands/generate-prp.md` | Slash command: generates PRP from problem description |\n| `.claude/commands/execute-prp.md` | Slash command: builds WAT system from PRP |\n| `PRPs/templates/prp_base.md` | Base template for generated PRPs |\n| `factory/workflow.md` | The factory's build process |\n| `factory/templates/*` | Templates for generated artifacts |\n| `factory/tools/*` | Factory tool implementations |\n| `config/mcp_registry.md` | Available MCPs and capabilities |\n| `library/patterns.md` | Proven workflow patterns |\n| `library/tool_catalog.md` | Reusable tool patterns |\n| `converters/n8n_node_map.md` | n8n node to WAT mapping reference |\n\n---\n\n## Part 3: Operational Rules\n\n### CRITICAL Git Rules\n\n- ONLY commit files the agent changed in the current session\n- ALWAYS use `git add <specific-file-paths>` -- NEVER `git add -A` or `git add .`\n- Before committing, run `git status` and verify only the agent's files are staged\n- NEVER commit unless explicitly instructed by the human\n- NEVER use `git reset --hard`, `git checkout .`, `git clean -fd`, `git stash`, `git push --force`\n\n### Style Rules\n\n- No emojis in commits, issues, or code comments\n- No fluff or cheerful filler text\n- Technical prose only -- be kind but direct\n- Keep answers short and concise\n- Code comments explain why, not what\n\n### Anti-Patterns to Avoid\n\n- Do not hardcode API keys, tokens, or credentials\n- Do not use `git add -A` or `git add .`\n- Do not skip validation because \"it should work\"\n- Do not catch bare `except:` -- always catch specific exception types\n- Do not build tools that require interactive input\n- Do not create MCP-dependent tools without HTTP/API fallbacks\n- Do not design subagents that call other subagents\n- Do not use Agent Teams when fewer than 3 independent tasks exist\n- Do not build from a PRP with unresolved ambiguity flags\n- Do not modify a PRP during build execution\n\n### Files That Must NOT Be Modified\n\nThese files are part of the core agent infrastructure and must not be changed:\n- `event_handler/**` -- All event handler code\n- `.github/workflows/**` -- All GitHub Actions workflows\n- `entrypoint.sh` -- Container startup script\n- `Dockerfile` -- Container definition\n"}],"isError":false,"timestamp":1770815899753}}
{"type":"message","id":"125dfcb6","parentId":"d7292592","timestamp":"2026-02-11T13:18:19.755Z","message":{"role":"toolResult","toolCallId":"toolu_01Pdu7VsQKS7Pv9Gdtupa6zK","toolName":"read","content":[{"type":"text","text":"# WAT Systems Factory — Build Workflow\n\nThis is the factory's own WAT workflow. Follow these steps to build a new WAT system from a problem description or n8n workflow JSON.\n\n## Inputs\n\n- **problem_description**: A natural language description of the system to build, OR an n8n workflow JSON file\n- **system_name**: A lowercase-with-dashes name for the new system\n- **enable_agent_teams**: Boolean — whether to use Agent Teams for parallelizable tasks (optional, default false)\n\n## Outputs\n\n- A complete WAT system in `systems/{system_name}/` containing:\n  - `CLAUDE.md` — operating instructions\n  - `workflow.md` — the system's process workflow\n  - `tools/` — Python tool files\n  - `.github/workflows/` — GitHub Actions workflow files\n  - `requirements.txt` — Python dependencies\n  - `README.md` — setup and usage instructions\n\n---\n\n## Step 1: Intake\n\nReceive and classify the input.\n\n1. Read the `problem_description` input\n2. **Is this an n8n workflow JSON?**\n   - Check if the input is valid JSON containing n8n workflow structure (nodes, connections)\n   - **If yes**: Route to Step 1b (n8n Conversion)\n   - **If no**: Continue to Step 2 with the problem description as-is\n\n### Step 1b: n8n Conversion\n\nConvert an existing n8n workflow into WAT format.\n\n1. Parse the n8n JSON using `factory/tools/convert_n8n.py`\n2. Reference `converters/n8n_node_map.md` to translate every node type\n3. Map each n8n element:\n   - **Trigger nodes** (Webhook, Schedule, Manual) → GitHub Actions triggers\n   - **Action nodes** (HTTP Request, Function, etc.) → Python tools\n   - **Logic nodes** (IF, Switch, Merge) → Decision points in workflow.md\n   - **Connections** → Step ordering and data flow in workflow.md\n4. Extract the underlying workflow logic as a structured design\n5. Preserve the original n8n JSON as `reference/original_n8n.json` in the output system\n6. Continue to Step 3 with the translated design\n\n**Failure mode**: If the n8n JSON is malformed or contains unsupported node types, log the specific issues and generate a partial conversion with TODO markers for unsupported elements.\n\n---\n\n## Step 2: Research\n\nIdentify available tools and resources for the system.\n\n1. Read `config/mcp_registry.md` — what MCPs are available?\n2. Read `library/tool_catalog.md` — what reusable tool patterns exist?\n3. Read `library/patterns.md` — does this problem match a known pattern?\n4. For each capability the system needs:\n   - Check if an MCP provides it (preferred — less code to maintain)\n   - Check if a tool pattern exists in the catalog (reusable — faster to implement)\n   - Plan a custom tool if nothing exists (document as a new pattern candidate)\n5. Document the research findings: which MCPs, patterns, and tools will be used\n\n**Failure mode**: If the MCP registry is empty or missing, fall back to direct HTTP/API tools. The system must work without any specific MCP.\n\n---\n\n## Step 3: Design\n\nArchitect the workflow for the new system.\n\n1. Break the problem into discrete steps — each step should do one thing clearly\n2. Define the order: which steps depend on which?\n3. Identify decision points: where does the workflow branch based on conditions?\n4. Define failure modes for each step: what can go wrong and what's the fallback?\n5. Define inputs and outputs for each step\n6. **Design Subagents** — identify specialist subagents for the system:\n   - **Every system gets subagents**. Subagents are the DEFAULT delegation mechanism. For each major phase or capability in the workflow, define a specialist subagent.\n   - For each subagent, define:\n     - **name**: lowercase-with-hyphens identifier (e.g., `search-specialist`, `data-processor`)\n     - **description**: When Claude should delegate to this subagent (used for automatic delegation)\n     - **tools**: Which Claude Code tools this subagent needs (Read, Write, Edit, Bash, Grep, Glob, etc.) — grant only what's needed\n     - **model**: `sonnet` for most tasks, `haiku` for simple/fast tasks, `opus` for complex reasoning\n     - **permissionMode**: `default` unless the subagent needs autonomous writes (`acceptEdits`)\n     - **system_prompt**: Detailed instructions — the subagent's domain expertise, how to use its tools, expected inputs/outputs, error handling\n   - **Delegation hierarchy** (critical):\n     - **Subagents are the default** for all task delegation within a system. When the workflow reaches a phase, delegate to its specialist subagent.\n     - **Agent Teams is ONLY used** when the design identifies 3+ truly independent tasks that can run in parallel. Agent Teams is a parallelization optimization, not a delegation mechanism.\n     - A system can use BOTH: subagents for delegation AND Agent Teams for parallel execution of independent subagent tasks.\n   - Document all subagents in the design output — they will be generated in Step 5b\n7. **Agent Teams Analysis** — evaluate whether this system ALSO benefits from Agent Teams:\n   - **Count independent tasks**: List all tasks that have no data dependencies on each other\n   - **Apply the 3+ Independent Tasks Rule**:\n     - **3 or more independent tasks** → Recommend Agent Teams. Flag these tasks for parallel execution with the native team lead/teammate pattern. Set `enable_agent_teams` to true.\n     - **Fewer than 3 independent tasks** → Sequential execution is sufficient. Note in the design that Agent Teams is not recommended.\n   - **Assess parallelization benefit**: Will concurrent execution meaningfully reduce total time? (e.g., 3 API calls at 10s each → 10s parallel vs 30s sequential = meaningful)\n   - **Check merge complexity**: Can the parallel results be combined without complex reconciliation? If merging is harder than the parallel speedup, prefer sequential.\n   - **Document the decision**: Record in the design whether Agent Teams is recommended, which tasks would run in parallel, and the rationale\n   - **If Agent Teams is recommended**: Define team roles:\n     - **Team Lead responsibilities**: What the coordinator does (create task list, spawn teammates, merge results, quality control)\n     - **Teammate definitions**: For each parallel task, define the scoped instructions, expected inputs, and expected outputs\n     - **Shared task list structure**: What tasks appear on the list, their dependencies (blockedBy/blocks), and completion criteria\n   - **If Agent Teams is NOT recommended**: Design for sequential execution only. Note why (e.g., \"all tasks depend on previous output\" or \"only 2 independent tasks — overhead not justified\")\n   - **Regardless of recommendation**: The design MUST include a sequential execution path. Agent Teams is always an optimization, never a requirement.\n8. Select the workflow pattern from `library/patterns.md` that best fits (or define a new one)\n\n**Failure mode**: If the problem is too vague to design a clear workflow, generate a minimal viable workflow with TODO markers and document what clarification is needed.\n\n---\n\n## Step 3b: Factory Parallel Build (Agent Teams)\n\n**This step applies to the factory's own build process, not the generated system.**\n\nWhen the factory is building a complex system (4+ tools or multiple independent workflow phases), it can use Agent Teams to generate artifacts in parallel. This accelerates the build without affecting the output.\n\n1. **Check if Agent Teams is enabled**: Read `CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS` from the environment\n2. **Assess build complexity**:\n   - Count the number of tools to generate\n   - Count the number of independent generation steps (workflow, tools, Actions, CLAUDE.md)\n   - **If 4+ generation artifacts AND Agent Teams enabled**: Use parallel build\n   - **Otherwise**: Continue to Steps 4–7 sequentially (skip this step)\n3. **If parallel build**: The factory team lead creates a shared task list and spawns teammates:\n   - **Task: Generate Workflow** — Teammate generates `workflow.md` from the design (Step 4)\n   - **Task: Generate Tools** — Teammate generates all Python tools from the design (Step 5)\n   - **Task: Generate GitHub Actions** — Teammate generates `.github/workflows/` files (Step 6)\n   - **Task: Generate CLAUDE.md** — Teammate generates the system's CLAUDE.md (Step 7)\n   - Each teammate receives the full design from Step 3 as context\n   - Each teammate writes its output to the system's output directory\n4. **Team lead collects and validates**: After all teammates complete, the team lead:\n   - Verifies all expected files were generated\n   - Checks cross-references (e.g., workflow.md mentions tools that exist in `tools/`)\n   - Resolves any conflicts or inconsistencies\n   - Proceeds to Step 8 (Test)\n\n**Sequential fallback**: If Agent Teams is not enabled, Steps 4–7 execute sequentially as defined below. Results are identical.\n\n**Failure mode**: If a teammate fails, the team lead falls back to generating that artifact sequentially. A partial parallel build is still faster than fully sequential.\n\n---\n\n## Step 4: Generate Workflow\n\nWrite the system's `workflow.md`.\n\n1. Run `factory/tools/generate_workflow.py` with the design from Step 3\n2. Follow the structure in `factory/templates/workflow_template.md`\n3. Include:\n   - Clear header with system name and purpose\n   - Inputs section with expected parameters\n   - Outputs section with deliverables\n   - Numbered steps with sub-steps\n   - Decision points with bold conditional text\n   - Failure modes and fallback steps for every step\n4. **If Agent Teams applies**: Include parallel execution sections using `factory/templates/agent_teams_template.md`\n   - Define which steps run as sub-agents\n   - Define the merge/coordination logic\n   - Include a sequential fallback section\n\n**Failure mode**: If workflow generation fails, write a minimal skeleton workflow.md with the key steps and TODO markers.\n\n---\n\n## Step 5: Generate Tools\n\nWrite Python tools for each action step.\n\n1. Run `factory/tools/generate_tools.py` with the workflow steps from Step 4\n2. For each action step in the workflow:\n   - Check `library/tool_catalog.md` for a reusable pattern\n   - If found: adapt the pattern for this system's specific needs\n   - If not found: generate a new tool following `factory/templates/tool_template.py`\n3. Every tool MUST have:\n   - Module docstring (purpose, inputs, outputs)\n   - `main()` entry point\n   - `try/except` error handling\n   - `logging` integration\n   - Type hints\n4. **If Agent Teams is used**: Generate sub-agent task definitions using `factory/tools/generate_agent_teams.py`\n5. Generate `requirements.txt` listing all Python dependencies\n\n**Failure mode**: If a tool cannot be generated (e.g., requires an unknown API), create a stub tool with clear TODO markers and document what's missing.\n\n---\n\n## Step 5b: Generate Subagents\n\nCreate `.claude/agents/` markdown files for the system's specialist subagents.\n\n1. Run `factory/tools/generate_subagents.py` with the subagent definitions from Step 3\n2. For each subagent defined in the design:\n   - Generate a markdown file following `factory/templates/subagent_template.md`\n   - YAML frontmatter with: `name`, `description`, `tools`, `model`, `permissionMode`\n   - Detailed system prompt in the markdown body covering:\n     - What the subagent specializes in\n     - Which tools to use and how\n     - Expected inputs and outputs\n     - Step-by-step execution instructions\n     - Failure handling procedures\n3. Place all subagent files in `{system_dir}/.claude/agents/`\n4. Validate each subagent file:\n   - Frontmatter parses as valid YAML\n   - `name` is lowercase-with-hyphens only\n   - `tools` lists only valid Claude Code tools\n   - `model` is one of: sonnet, opus, haiku, inherit\n   - System prompt is non-empty and specific to the domain\n\n**Failure mode**: If a subagent file fails validation, fix the issue and regenerate. If generation fails entirely, create stub subagent files with TODO markers — the system should still work without subagents (Claude will use tools directly).\n\n---\n\n## Step 6: Generate GitHub Actions\n\nCreate `.github/workflows/` files for the new system.\n\n1. Run `factory/tools/generate_github_actions.py` with the system design\n2. Using `factory/templates/github_action_template.yml`, create the main workflow:\n   - Configure triggers based on the system's needs (dispatch, cron, webhook)\n   - Set up Python environment and dependencies\n   - Inject required secrets (reference GitHub Secrets, NEVER hardcode)\n   - Run Claude Code with the system's workflow.md\n   - Commit results back to the repo\n   - Configure failure notifications\n   - Set `timeout-minutes` on every job\n3. Using `factory/templates/agent_hq_template.yml`, create `agent_hq.yml`:\n   - Respond to issue assignment and @claude mentions\n   - Parse issue body as task input\n   - Submit results as draft PR\n   - Support iterative feedback\n\n**Failure mode**: If trigger configuration is ambiguous, default to `workflow_dispatch` with a manual input parameter.\n\n---\n\n## Step 7: Generate CLAUDE.md\n\nWrite the system's operating instructions.\n\n1. Using `factory/templates/claude_md_template.md`, generate CLAUDE.md with:\n   - System identity and purpose\n   - Required MCPs and their alternatives\n   - Expected inputs and outputs\n   - How to execute the workflow\n   - **Subagents section**: List all subagents, when to delegate to each, and how to chain them\n   - **Delegation hierarchy**: Subagents are the default, Agent Teams only for parallel execution\n   - Agent Teams configuration (if any) with token cost notes\n   - Agent HQ usage instructions\n   - Secret requirements (names only, never values)\n   - Troubleshooting common issues\n2. This file MUST serve double duty:\n   - Instructions for Claude Code when running via CLI or GitHub Actions\n   - Context for GitHub Agent HQ when running via issue assignment\n\n**Failure mode**: If unsure about MCP requirements, list all possible MCPs with notes on which are required vs. optional.\n\n---\n\n## Step 8: Test (3-Level Validation Gates)\n\nValidate the generated system through three progressive levels. **Each level must pass before proceeding to the next.** Do not skip levels. Do not proceed past a failing gate.\n\n### Level 1: Syntax & Structure\n\nVerify every generated file is syntactically valid and structurally correct.\n\n1. Run `factory/tools/test_tools.py` in syntax-only mode against each generated tool\n2. For each Python tool file:\n   - **AST parse**: Verify the file is valid Python (`python -c \"import ast; ast.parse(open('{tool}').read())\"`)\n   - **Import check**: Verify the file imports without errors (`python -c \"import importlib; importlib.import_module('{tool_module}')\"`)\n   - **Structure check**: Verify `main()` exists and is callable\n   - **Docstring check**: Verify module-level docstring exists\n   - **Error handling check**: Verify at least one `try/except` block exists (AST inspection)\n3. For each workflow/markdown file:\n   - Verify it is non-empty\n   - Verify it has required sections (Inputs, Outputs for workflow.md)\n4. For each YAML file (.github/workflows/):\n   - Verify valid YAML syntax\n   - Verify `timeout-minutes` is set on every job\n5. For each subagent file (.claude/agents/):\n   - Verify YAML frontmatter parses correctly\n   - Verify required fields (`name`, `description`) are present\n   - Verify `name` is lowercase-with-hyphens only\n   - Verify system prompt body is non-empty\n6. Log all Level 1 results\n\n**Gate**: ALL files must pass Level 1. If any file fails:\n- Attempt automatic fix (syntax errors, missing docstrings, missing main())\n- Re-run Level 1 on the fixed files\n- If auto-fix fails after 2 attempts, log the failure and halt the build\n\n**Failure mode**: If the testing infrastructure itself fails, generate a manual Level 1 checklist in README.md and proceed to Level 2 with a warning.\n\n### Level 2: Unit Tests\n\nVerify each tool produces correct output for sample inputs.\n\n1. For each Python tool:\n   - Run with sample/mock inputs: `python tools/{tool_name}.py --help` (verify argparse works)\n   - Run with minimal valid input and verify:\n     - Exit code is 0 on success\n     - Output is valid JSON (if tool outputs JSON)\n     - Output contains expected keys/structure\n   - Run with invalid input and verify:\n     - Exit code is non-zero\n     - Error message is meaningful (not a raw traceback)\n     - The tool does not hang or crash silently\n2. For tools that call external APIs:\n   - Verify the tool handles missing API keys gracefully (clear error message, non-zero exit)\n   - If the PRP provides mock data, use it for testing\n   - If no mock data, verify the tool's error path (missing credentials) works correctly\n3. Verify `requirements.txt` contains all dependencies used by the tools\n4. Log all Level 2 results\n\n**Gate**: ALL tools must pass Level 2. If any tool fails:\n- Read the error output, identify the root cause\n- Fix the tool code (not the test)\n- Re-run Level 2 on the fixed tool\n- If fix fails after 2 attempts, log the failure and halt the build\n\n**Failure mode**: If external APIs are unreachable and no mock data exists, verify error handling paths only and document untested happy paths in README.md.\n\n### Level 3: Integration Tests\n\nVerify tools work together as a pipeline and the system is complete.\n\n1. **Pipeline test**: Simulate the workflow end-to-end with sample data\n   - Run tools in workflow order, passing each tool's output as the next tool's input\n   - Verify the final output matches expected format and contains expected data\n   - Verify no tool drops data or produces incompatible output for the next step\n2. **Cross-reference validation**:\n   - Verify workflow.md references only tools that exist in `tools/`\n   - Verify CLAUDE.md documents every tool, subagent, and secret\n   - Verify .github/workflows/ YAML references correct tool paths and secret names\n   - Verify .env.example lists every secret referenced in CLAUDE.md and GitHub Actions\n   - Verify README.md covers all three execution paths\n3. **Subagent integration**:\n   - Verify each subagent file in `.claude/agents/` is referenced in CLAUDE.md\n   - Verify subagent tool lists contain only valid Claude Code tools\n   - Verify subagent descriptions match the workflow phases they serve\n4. **Package completeness**:\n   - Verify all required system files exist (CLAUDE.md, workflow.md, tools/, .github/workflows/, requirements.txt, README.md, .env.example, .gitignore)\n   - Verify no hardcoded secrets in any file (scan for common patterns: `sk-`, `api_key=`, bearer tokens)\n5. Log all Level 3 results\n\n**Gate**: ALL integration checks must pass. If any fail:\n- Identify the inconsistency (e.g., workflow.md references a tool that does not exist)\n- Fix the source file\n- Re-run Level 3\n- If fix fails after 2 attempts, log the failure and halt the build\n\n**Failure mode**: If the full pipeline cannot be tested (e.g., requires live API access), test each tool independently with sample data and document the untested integration path in README.md.\n\n---\n\n## Step 9: Package\n\nBundle everything into a deployable system.\n\n1. Run `factory/tools/package_system.py`\n2. Create the output directory: `systems/{system_name}/`\n3. Place all files:\n   - `CLAUDE.md` (from Step 7)\n   - `workflow.md` (from Step 4)\n   - `tools/` (from Step 5)\n   - `.claude/agents/` (from Step 5b)\n   - `.github/workflows/` (from Step 6)\n   - `requirements.txt` (from Step 5)\n4. Generate `README.md` with:\n   - System overview\n   - Setup instructions for ALL THREE execution paths:\n     - **(a) Claude Code CLI**: Clone repo, set env vars, run `claude` with workflow.md\n     - **(b) GitHub Actions dispatch**: Push to GitHub, configure secrets, trigger via Actions UI or API\n     - **(c) GitHub Agent HQ**: Assign issue to @claude with task description in issue body\n   - Required secrets list\n   - Example usage for each path\n5. Verify the package is complete: all required files present, no hardcoded secrets\n\n**Failure mode**: If any files are missing, log what's missing and package what exists with a TODO list in README.md.\n\n---\n\n## Step 10: Learn\n\nExtract learnings and update the factory's knowledge base.\n\n1. Review the system that was just built — what patterns were used?\n2. **Update `library/patterns.md`**:\n   - If a new pattern was discovered, add it with a description and example\n   - If an existing pattern was refined, update its entry\n3. **Update `library/tool_catalog.md`**:\n   - If new tool patterns were created, catalog them for reuse\n   - If existing tool patterns were improved, update their entries\n4. **If this was an n8n conversion**:\n   - Save the n8n-to-WAT mapping in `converters/n8n_examples/` for future reference\n5. Log what worked well and what was difficult — this informs future improvements\n6. Commit all library updates\n\n**Failure mode**: If learning extraction fails, the system build is still valid — learning is a bonus, not a requirement.\n"}],"isError":false,"timestamp":1770815899754}}
{"type":"message","id":"4357685e","parentId":"125dfcb6","timestamp":"2026-02-11T13:18:19.755Z","message":{"role":"toolResult","toolCallId":"toolu_01HBgjPCHa7gACixRbgY2UXD","toolName":"read","content":[{"type":"text","text":"# WAT Systems Factory -- Workflow Pattern Catalog\n\n> Proven, reusable workflow patterns for building GitHub-native autonomous systems with Claude Code and MCP tools.\n\nEach pattern below is a composable building block. Patterns can be combined: a **Monitor > Detect > Alert** system might trigger an **Issue > Execute > PR** workflow, which internally uses **Fan-Out > Process > Merge** for the heavy lifting.\n\n---\n\n## 1. Scrape > Process > Output\n\n**Summary:** A web scraping pipeline that fetches raw content from one or more URLs via a web-access MCP, processes and structures the data with Python (or Claude itself), and writes clean output to a target destination.\n\n### When to Use\n\n- You need structured data extracted from websites that lack APIs.\n- You want to convert messy HTML into clean JSON, CSV, or Markdown on a recurring schedule.\n- You are building a dataset from publicly available web content.\n\n### Steps\n\n1. **Define targets** -- Specify URLs, selectors, or search queries in a config file or workflow input.\n2. **Fetch raw content** -- Use a web MCP (Fetch, Puppeteer, or Playwright) to retrieve page content. Handle pagination if needed.\n3. **Parse and extract** -- Strip irrelevant markup. Pull target fields using CSS selectors, XPath, or Claude-driven semantic extraction.\n4. **Transform** -- Normalize data types, deduplicate, validate schema. Python scripts or inline Claude processing.\n5. **Output** -- Write results to a file in the repo (`data/`, `output/`), post to an API, or commit as a structured artifact.\n\n### Key Tools / MCPs\n\n- **Fetch MCP** -- lightweight HTTP retrieval, returns Markdown-converted content.\n- **Puppeteer / Playwright MCP** -- for JavaScript-rendered pages requiring a headless browser.\n- **Python** -- pandas, BeautifulSoup, or custom scripts for heavy transformation.\n- **filesystem MCP** -- write results to disk.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  schedule:\n    - cron: '0 6 * * *'   # daily at 06:00 UTC\n  workflow_dispatch:        # manual trigger\n```\n\n### Example Use Cases\n\n- Scrape competitor pricing pages nightly; output a CSV diff showing changes.\n- Collect job postings from three sites; normalize into a single JSON feed committed to the repo.\n- Pull public government data tables and convert to machine-readable Parquet files.\n\n### Failure Handling\n\n- **Retry with backoff** on transient HTTP errors (429, 503).\n- **Schema validation** after extraction -- if required fields are missing, flag the record rather than silently dropping it.\n- **Stale-data guard** -- if fetch returns identical content to last run, skip processing and log \"no change.\"\n- **Alert on structural change** -- if selectors return zero matches (site redesign), open a GitHub Issue for human review.\n\n---\n\n## 2. Research > Analyze > Report\n\n**Summary:** A multi-source research workflow that gathers information from diverse sources (web, APIs, local files), synthesizes findings with Claude, and produces a formatted report committed to the repository.\n\n### When to Use\n\n- You need to compile information from multiple sources into a single coherent document.\n- Periodic market research, competitive analysis, or literature reviews.\n- Any task where a human would spend hours reading and summarizing.\n\n### Steps\n\n1. **Define research brief** -- Specify the topic, questions to answer, source priorities, and output format.\n2. **Gather sources** -- Query web search, fetch specific URLs, read local reference files, call APIs.\n3. **Extract key findings** -- For each source, pull relevant facts, quotes, data points. Tag each with provenance.\n4. **Analyze and synthesize** -- Claude cross-references findings, identifies patterns, contradictions, and gaps.\n5. **Generate report** -- Produce a structured Markdown document with sections, citations, and an executive summary.\n6. **Commit and notify** -- Commit the report to the repo. Optionally open a PR or send a notification.\n\n### Key Tools / MCPs\n\n- **Web Search MCP / Fetch MCP** -- gather online sources.\n- **filesystem MCP** -- read local reference documents.\n- **GitHub MCP** -- commit output, open issues or PRs.\n- **Claude (extended thinking)** -- deep analysis and synthesis.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  schedule:\n    - cron: '0 9 * * 1'   # weekly on Monday at 09:00 UTC\n  workflow_dispatch:\n    inputs:\n      topic:\n        description: 'Research topic'\n        required: true\n```\n\n### Example Use Cases\n\n- Weekly competitive landscape report that tracks new features announced by three competitors.\n- Monthly technology radar summarizing emerging tools relevant to the team's stack.\n- Ad-hoc deep-dive report triggered manually via `workflow_dispatch` with a topic input.\n\n### Failure Handling\n\n- **Source fallback** -- if a primary source is unreachable, log it and proceed with remaining sources. Note gaps in the report.\n- **Confidence scoring** -- Claude flags low-confidence claims; the report includes a \"Caveats\" section.\n- **Minimum-source threshold** -- if fewer than N sources are successfully retrieved, abort and open an issue rather than publishing a thin report.\n\n---\n\n## 3. Monitor > Detect > Alert\n\n**Summary:** A continuous monitoring system that periodically checks a target (website, API, dataset, repo) for changes, applies detection logic, and fires alerts or triggers downstream workflows when significant changes are found.\n\n### When to Use\n\n- You want to be notified when something changes: a webpage, an API response, a competitor's pricing, a regulatory filing.\n- You need a lightweight, Git-native alternative to commercial monitoring SaaS.\n- Ideal for cron-triggered GitHub Actions that run on a schedule.\n\n### Steps\n\n1. **Fetch current state** -- Retrieve the current snapshot of the monitored target.\n2. **Load previous state** -- Read the last-known state from a file committed in the repo (e.g., `state/last_snapshot.json`).\n3. **Diff** -- Compare current vs. previous. Use structural diff for JSON, text diff for prose, hash comparison for binary.\n4. **Evaluate significance** -- Apply rules or Claude-based judgment to determine if the change is meaningful (filter noise).\n5. **Alert** -- If significant: open a GitHub Issue, send a Slack message, trigger a downstream workflow via `repository_dispatch`, or all three.\n6. **Persist new state** -- Commit the current snapshot as the new baseline.\n\n### Key Tools / MCPs\n\n- **Fetch MCP / Puppeteer MCP** -- retrieve current state from web targets.\n- **filesystem MCP** -- read/write state files.\n- **GitHub MCP** -- open issues, trigger dispatches.\n- **Slack MCP** -- send alert messages.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  schedule:\n    - cron: '*/30 * * * *'  # every 30 minutes\n```\n\n### Example Use Cases\n\n- Monitor a government regulatory page for new filings; alert the legal team via Slack when a new document appears.\n- Watch a competitor's changelog page; open a GitHub Issue summarizing each detected update.\n- Track an API's response schema for breaking changes; alert the engineering team.\n\n### Failure Handling\n\n- **Transient failure tolerance** -- if the fetch fails, do NOT overwrite previous state. Log the failure. After N consecutive failures, escalate to an alert.\n- **False-positive suppression** -- use Claude to evaluate whether a detected \"change\" is meaningful (e.g., ignore timestamp-only diffs).\n- **State corruption recovery** -- if state file is missing or unparseable, re-initialize from scratch and log a warning instead of crashing.\n\n---\n\n## 4. Intake > Enrich > Deliver\n\n**Summary:** A data enrichment pipeline that receives raw input (from a form, webhook, file drop, or API), augments it with additional data from external sources, and delivers the enriched result to its destination.\n\n### When to Use\n\n- Incoming data is incomplete and needs augmentation before it is useful.\n- You are building a lead enrichment, content tagging, or metadata enhancement system.\n- Raw data arrives in one format and must be transformed and enriched before storage or delivery.\n\n### Steps\n\n1. **Intake** -- Receive raw data. Sources: webhook payload, new file in repo, manual trigger with JSON input, or a watched directory.\n2. **Validate** -- Check that required fields exist and data types are correct. Reject malformed input early.\n3. **Enrich** -- For each record, call enrichment sources: web search for context, APIs for supplementary data, Claude for classification or summarization.\n4. **Merge** -- Combine original data with enrichment results into a unified schema.\n5. **Deliver** -- Write enriched data to its destination: commit to repo, POST to API, append to a dataset file, or open a PR.\n\n### Key Tools / MCPs\n\n- **Web Search MCP** -- find supplementary information.\n- **Fetch MCP** -- call enrichment APIs.\n- **Claude** -- classify, tag, summarize, or score records.\n- **filesystem MCP** -- read input, write output.\n- **GitHub MCP** -- commit results or open PRs.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  push:\n    paths:\n      - 'inbox/**'          # new file dropped in inbox\n  workflow_dispatch:\n    inputs:\n      payload:\n        description: 'JSON data to enrich'\n        required: true\n```\n\n### Example Use Cases\n\n- A sales team drops a CSV of company names into `inbox/`; the pipeline enriches each with industry, size, website, and LinkedIn URL, then writes to `data/enriched_leads.csv`.\n- Incoming support tickets are enriched with customer history and sentiment score before routing.\n- Raw RSS feed items are enriched with summaries, topic tags, and relevance scores.\n\n### Failure Handling\n\n- **Per-record error isolation** -- if enrichment fails for one record, mark it as `enrichment_failed` and continue processing the rest. Do not let one bad record kill the batch.\n- **Rate-limit awareness** -- track API call counts; pause and retry when rate-limited rather than failing fast.\n- **Partial delivery** -- deliver successfully enriched records on schedule; queue failed records for retry in the next run.\n\n---\n\n## 5. Generate > Review > Publish\n\n**Summary:** A content generation pipeline with built-in quality gates. Claude generates draft content, a review step validates quality and compliance, and approved content is published to its destination.\n\n### When to Use\n\n- Automated content creation (blog posts, documentation, social media, release notes) where quality must be verified before publishing.\n- Any workflow where you want a \"human-in-the-loop\" approval step or an automated quality check before output goes live.\n\n### Steps\n\n1. **Brief** -- Define the content requirements: topic, audience, tone, length, format, and any reference material.\n2. **Generate** -- Claude produces a first draft using the brief and any provided reference material.\n3. **Automated review** -- Run quality checks: spelling/grammar, factual consistency against source material, tone analysis, compliance rules (e.g., no prohibited claims).\n4. **Gate decision** -- If automated review passes, proceed. If it fails, loop back to step 2 with feedback (max N retries). Optionally, require human approval via a PR review.\n5. **Publish** -- Commit final content to the repo, deploy to a CMS via API, or post to a platform.\n\n### Key Tools / MCPs\n\n- **Claude** -- generation and review (use separate system prompts for generator vs. reviewer to create genuine separation).\n- **filesystem MCP** -- read briefs, write drafts and final output.\n- **GitHub MCP** -- open a draft PR for human review; merge on approval.\n- **Fetch MCP** -- publish to external CMS or platform APIs.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  issues:\n    types: [labeled]        # label \"content-request\" triggers generation\n  pull_request_review:\n    types: [submitted]      # human approval triggers publish\n  workflow_dispatch:\n```\n\n### Example Use Cases\n\n- Weekly changelog: gather merged PRs, generate a formatted changelog, open a PR for team review, publish on approval.\n- Blog post pipeline: issue labeled \"draft-request\" triggers generation; editor reviews the PR; merge triggers deployment.\n- Social media batch: generate a week's worth of posts, run brand-voice review, output approved posts to a scheduling queue.\n\n### Failure Handling\n\n- **Retry budget** -- if automated review rejects the draft, regenerate with targeted feedback up to 3 times. After that, escalate to a human by opening an issue.\n- **Rollback on publish failure** -- if the publish step fails (API error), keep the approved content in the repo and log the failure for manual retry.\n- **Diff guard** -- before publishing, diff against previously published content to prevent accidental overwrite of manual edits.\n\n---\n\n## 6. Listen > Decide > Act\n\n**Summary:** An event-driven agent that listens for incoming events (webhooks, GitHub events, messages), applies decision logic to determine the appropriate response, and takes autonomous action.\n\n### When to Use\n\n- You need a system that reacts to external events in near-real-time.\n- Webhook-triggered GitHub Actions are the execution engine.\n- The appropriate response depends on the content of the event (routing, triage, conditional logic).\n\n### Steps\n\n1. **Receive event** -- GitHub Actions triggers on the event (webhook, issue creation, PR comment, `repository_dispatch`).\n2. **Parse and classify** -- Extract relevant data from the event payload. Use Claude to classify intent or severity if the input is unstructured (e.g., natural language).\n3. **Decide** -- Apply decision logic: rule-based routing, Claude-driven triage, or a lookup table. Determine which action(s) to take.\n4. **Act** -- Execute the chosen action: respond to the issue, trigger another workflow, call an API, update a file, or escalate to a human.\n5. **Log** -- Record the event, decision, and action taken for auditability.\n\n### Key Tools / MCPs\n\n- **GitHub MCP** -- read event payloads, respond to issues/PRs, trigger dispatches.\n- **Claude** -- classify, triage, and decide.\n- **Slack MCP** -- notify humans when escalation is needed.\n- **Fetch MCP** -- call external APIs as part of the action.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  issues:\n    types: [opened, labeled]\n  issue_comment:\n    types: [created]\n  repository_dispatch:\n    types: [incoming_webhook]\n```\n\n### Example Use Cases\n\n- Triage bot: new issues are classified by Claude (bug, feature, question) and auto-labeled, assigned, and replied to with relevant templates.\n- Deployment gatekeeper: a PR comment like \"/deploy staging\" triggers a deployment workflow after validating the commenter's permissions.\n- Customer feedback router: webhook delivers feedback; Claude scores sentiment and routes critical items to Slack.\n\n### Failure Handling\n\n- **Idempotency** -- design actions to be safe to retry. Use event IDs to detect and skip duplicate deliveries.\n- **Decision logging** -- log the full decision chain (event > classification > decision > action) so failures can be diagnosed.\n- **Safe defaults** -- if classification confidence is low, default to escalation (notify a human) rather than taking an autonomous action that might be wrong.\n\n---\n\n## 7. Collect > Transform > Store\n\n**Summary:** A classic ETL (Extract, Transform, Load) pipeline where Git serves as the data store. Data is collected from external sources, transformed into a canonical format, and committed to the repository as versioned, queryable files.\n\n### When to Use\n\n- You want a version-controlled data lake without standing up a database.\n- Small-to-medium datasets (tens of MB) that benefit from Git's diff, history, and branching.\n- Data pipelines where auditability and reproducibility matter more than query speed.\n\n### Steps\n\n1. **Collect** -- Pull data from source(s): APIs, web scrapes, file uploads, other repos.\n2. **Validate** -- Check data integrity: schema validation, null checks, type enforcement.\n3. **Transform** -- Normalize, clean, reshape. Convert to the canonical schema. Deduplicate against existing data in the store.\n4. **Diff** -- Compare transformed data against the current version in the repo. Compute what is new, changed, or deleted.\n5. **Store** -- Commit the updated dataset files to the repo. Use meaningful commit messages describing what changed (e.g., \"Add 14 new records from source X\").\n\n### Key Tools / MCPs\n\n- **Fetch MCP** -- pull data from APIs and web sources.\n- **Python** -- pandas or custom scripts for transformation.\n- **filesystem MCP** -- read/write data files.\n- **GitHub MCP / Git CLI** -- commit, push, branch as needed.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  schedule:\n    - cron: '0 */6 * * *'  # every 6 hours\n  workflow_dispatch:\n```\n\n### Example Use Cases\n\n- Collect cryptocurrency prices from three APIs every 6 hours; store as daily CSV files in `data/prices/`.\n- Pull open government datasets weekly; transform into a normalized schema; maintain a living dataset in the repo.\n- Aggregate internal metrics from multiple microservice health endpoints; store as JSON for a dashboard.\n\n### Failure Handling\n\n- **Atomic commits** -- only commit if the entire pipeline succeeds. Never commit partial or corrupted data.\n- **Schema drift detection** -- if source data no longer matches the expected schema, abort and alert rather than storing malformed data.\n- **Rollback via Git** -- if bad data is committed, revert the commit. The previous state is always recoverable.\n- **Dedup safety** -- idempotent writes ensure that re-running the pipeline does not create duplicate records.\n\n---\n\n## 8. Fan-Out > Process > Merge\n\n**Summary:** The Agent Teams pattern. A coordinator agent splits a large task into independent sub-tasks, dispatches them to parallel sub-agents (or sequential iterations), collects all results, and merges them into a unified output.\n\n### When to Use\n\n- A task is naturally decomposable into independent units of work (e.g., process each file, research each topic, analyze each competitor).\n- You want to reduce wall-clock time by parallelizing work.\n- The final deliverable requires combining results from multiple sub-tasks.\n\n### Steps\n\n1. **Plan** -- The coordinator analyzes the task and determines how to decompose it. Produces a work manifest: a list of sub-tasks with their inputs.\n2. **Fan-Out** -- Dispatch each sub-task. Options: parallel GitHub Actions jobs via a matrix strategy, sequential Claude tool calls, or `repository_dispatch` events to separate workflows.\n3. **Process** -- Each sub-agent executes its sub-task independently. Results are written to a known location (e.g., `temp/results/task_01.json`).\n4. **Collect** -- The coordinator waits for all sub-tasks to complete and gathers their outputs.\n5. **Merge** -- Combine results: concatenate, deduplicate, reconcile conflicts, and produce the final unified output.\n6. **Deliver** -- Commit the merged output or pass it to the next stage in a larger pipeline.\n\n### Key Tools / MCPs\n\n- **Claude** -- coordinator logic, sub-task execution, merge logic.\n- **GitHub Actions matrix** -- parallel job execution.\n- **filesystem MCP** -- read/write intermediate and final results.\n- **GitHub MCP** -- coordinate via issues, artifacts, or dispatch events.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  workflow_dispatch:\n    inputs:\n      task_manifest:\n        description: 'JSON array of sub-tasks'\n        required: true\n  repository_dispatch:\n    types: [fan_out_request]\n```\n\n### Example Use Cases\n\n- Analyze 20 competitor websites: fan out one sub-agent per competitor, each produces a profile, merge into a single competitive analysis report.\n- Process a batch of 50 documents: each sub-agent summarizes one document, merge into a master summary with cross-references.\n- Multi-language translation: fan out one sub-agent per target language, merge all translations into a localized content bundle.\n\n### Failure Handling\n\n- **Partial success tolerance** -- if some sub-tasks fail, merge the successful results and report failures. Do not discard all work because of one failure.\n- **Retry failed sub-tasks** -- the coordinator can re-dispatch failed sub-tasks up to N times before giving up.\n- **Timeout enforcement** -- set a maximum wall-clock time for the fan-out phase. Collect whatever is done when the timer expires.\n- **Conflict resolution** -- if sub-task results conflict, the merge step uses Claude to reconcile or flags the conflict for human review.\n\n---\n\n## 9. Issue > Execute > PR\n\n**Summary:** The GitHub Agent HQ pattern. An agent receives a task specification via a GitHub Issue, autonomously executes the required workflow, and delivers the results as a draft Pull Request for human review.\n\n### When to Use\n\n- You want a human-in-the-loop task queue where work is requested via Issues and delivered via PRs.\n- The agent should be fully autonomous in execution but the output requires human approval before merging.\n- Building an internal tool where non-technical stakeholders can request work by opening an Issue.\n\n### Steps\n\n1. **Receive** -- A new Issue is opened (or an existing Issue is labeled with a trigger label like `agent-task`). GitHub Actions fires.\n2. **Parse task** -- Claude reads the Issue body and extracts the task specification: what to do, inputs, constraints, expected output.\n3. **Create branch** -- Create a feature branch named after the Issue (e.g., `agent/issue-42`).\n4. **Execute** -- Run the appropriate workflow or pattern to complete the task. This might invoke any other pattern in this catalog.\n5. **Commit results** -- Commit all output files to the feature branch with clear commit messages.\n6. **Open draft PR** -- Open a draft PR linked to the original Issue. The PR body includes a summary of what was done, key decisions made, and anything that needs human attention.\n7. **Comment on Issue** -- Post a comment on the Issue linking to the PR and summarizing the outcome.\n\n### Key Tools / MCPs\n\n- **GitHub MCP** -- read issues, create branches, commit, open PRs, post comments.\n- **Claude** -- parse task, execute logic, generate summaries.\n- **Any other MCP** -- depending on the task being executed.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  issues:\n    types: [opened, labeled]\n  issue_comment:\n    types: [created]       # support \"/execute\" command in comments\n```\n\n### Example Use Cases\n\n- A product manager opens an Issue: \"Research top 5 CRM tools and compare features.\" The agent researches, builds a comparison table, and opens a PR with the report.\n- An engineer opens an Issue: \"Add unit tests for the auth module.\" The agent generates tests and delivers them as a PR.\n- A stakeholder opens an Issue: \"Update the pricing page data for Q1.\" The agent pulls new data, updates the files, and opens a PR for review.\n\n### Failure Handling\n\n- **Status updates** -- the agent comments on the Issue at each major step (\"Starting research...\", \"Generating report...\", \"Opening PR...\") so progress is visible.\n- **Graceful failure** -- if the agent cannot complete the task, it comments on the Issue with a clear explanation of what went wrong and what it tried, then labels the Issue `agent-failed` for human triage.\n- **Scope guard** -- if Claude determines the task is ambiguous or out of scope, it asks a clarifying question in the Issue comments rather than guessing.\n- **Draft-only PRs** -- all PRs are opened as drafts. The agent never merges its own work.\n\n---\n\n## 10. n8n Import > Translate > Deploy\n\n**Summary:** A conversion pattern that ingests an n8n workflow JSON export, translates it into a WAT-native system (Claude Code + MCP + GitHub Actions), and deploys it as a fully functional GitHub-native workflow.\n\n### When to Use\n\n- You are migrating from n8n (or a similar visual workflow tool) to a GitHub-native architecture.\n- You want to leverage existing workflow logic without manually rewriting it.\n- You are evaluating whether an n8n workflow can run as a WAT system to reduce infrastructure dependencies.\n\n### Steps\n\n1. **Import** -- Read the n8n workflow JSON file. This contains nodes, connections, credentials references, and configuration.\n2. **Parse and map** -- Walk the node graph. For each n8n node type, identify the equivalent WAT component:\n   - HTTP Request node -> Fetch MCP call\n   - Code (JavaScript/Python) node -> inline script or Python file\n   - IF/Switch node -> Claude decision logic or conditional in workflow\n   - Webhook trigger -> `repository_dispatch` or `workflow_dispatch`\n   - Cron trigger -> `schedule` trigger in GitHub Actions\n   - Set/Merge nodes -> data transformation in Claude or Python\n3. **Translate** -- Generate the WAT system files:\n   - `.github/workflows/*.yml` -- GitHub Actions workflow definition.\n   - `prompts/` -- Claude system prompts for nodes that require AI logic.\n   - `scripts/` -- Python or shell scripts for code nodes.\n   - `config/` -- Configuration extracted from the n8n workflow.\n4. **Validate** -- Dry-run the generated workflow. Check that all node types were mapped, no credentials are hardcoded, and the control flow is correct.\n5. **Deploy** -- Commit the generated files to the repo. Open a PR with a detailed summary of the translation, noting any nodes that required manual review or could not be auto-translated.\n\n### Key Tools / MCPs\n\n- **filesystem MCP** -- read n8n JSON, write generated files.\n- **Claude** -- interpret node logic, generate prompts, translate code, produce workflow YAML.\n- **GitHub MCP** -- commit files, open PR.\n- **Python** -- parse n8n JSON, generate boilerplate.\n\n### GitHub Actions Trigger\n\n```yaml\non:\n  push:\n    paths:\n      - 'imports/n8n/*.json'   # drop an n8n export to trigger conversion\n  workflow_dispatch:\n    inputs:\n      n8n_file:\n        description: 'Path to n8n JSON file in the repo'\n        required: true\n```\n\n### Example Use Cases\n\n- Migrate a 15-node n8n lead-scoring workflow to a Git-versioned, GitHub Actions-powered system.\n- Convert an n8n social media posting pipeline into a WAT **Generate > Review > Publish** system.\n- Translate an n8n monitoring workflow into a WAT **Monitor > Detect > Alert** pattern, eliminating the need for a running n8n instance.\n\n### Failure Handling\n\n- **Unsupported node catalog** -- maintain a list of n8n node types that cannot be auto-translated. When encountered, flag them in the PR description with a `TODO` and a suggested manual implementation.\n- **Credential safety** -- never translate credentials. Replace all credential references with GitHub Secrets placeholders (`${{ secrets.SERVICE_API_KEY }}`) and document which secrets need to be configured.\n- **Fidelity check** -- generate a side-by-side comparison of the n8n flow and the translated WAT flow so a human reviewer can verify correctness.\n- **Incremental migration** -- support translating a subset of the n8n workflow, allowing teams to migrate node-by-node rather than all-at-once.\n\n---\n\n## Pattern Composition Quick Reference\n\nPatterns are building blocks. Here are common compositions:\n\n| Composition | How It Works |\n|---|---|\n| **Monitor > Detect > Alert** + **Issue > Execute > PR** | Monitor detects a change, opens an Issue, which triggers the agent to act and deliver a PR. |\n| **Issue > Execute > PR** using **Fan-Out > Process > Merge** | A complex Issue task is decomposed and parallelized internally before the PR is opened. |\n| **Scrape > Process > Output** feeding **Collect > Transform > Store** | Scraping output is piped into the ETL pattern for versioned storage. |\n| **Listen > Decide > Act** routing to any pattern | An event-driven dispatcher that selects and triggers the appropriate pattern based on the event. |\n| **n8n Import > Translate > Deploy** producing any pattern | The translator identifies which WAT pattern best fits the n8n workflow and generates accordingly. |\n\n---\n\n## Choosing a Pattern\n\n| Question | Recommended Pattern |\n|---|---|\n| Do I need to pull data from websites? | **Scrape > Process > Output** |\n| Do I need to synthesize information from many sources? | **Research > Analyze > Report** |\n| Do I need to watch for changes over time? | **Monitor > Detect > Alert** |\n| Do I need to augment incoming data? | **Intake > Enrich > Deliver** |\n| Do I need AI-generated content with quality control? | **Generate > Review > Publish** |\n| Do I need to react to external events? | **Listen > Decide > Act** |\n| Do I need versioned data storage without a database? | **Collect > Transform > Store** |\n| Do I need to parallelize a large task? | **Fan-Out > Process > Merge** |\n| Do I need a task queue with human approval? | **Issue > Execute > PR** |\n| Do I need to migrate an n8n workflow? | **n8n Import > Translate > Deploy** |\n| Do I need B2B lead generation from web data? | **Intake > Enrich > Deliver** (lead-gen variant) |\n\n---\n\n## Proven Compositions (from builds)\n\n### Lead Generation Pipeline (lead-gen-machine)\n- **Pattern**: Intake > Enrich > Deliver + Scrape > Process > Output\n- **Flow**: Parse profile > Search web > Scrape sites > Extract contacts (Claude) > Score leads > Output CSV\n- **Key insight**: The Intake > Enrich > Deliver pattern works well when the \"enrichment\" involves multi-step web scraping + AI extraction. Each company is an independent unit of work, making this a strong candidate for Fan-Out > Process > Merge parallelization.\n- **Scoring pattern**: Decompose match criteria into independent dimensions (industry, size, location, keywords) with weighted point values. This makes the scoring transparent and tunable.\n- **Fallback chains**: Firecrawl > HTTP+BeautifulSoup for scraping, Claude > regex for extraction. Always have a degraded-but-functional fallback.\n\n### Marketing Pipeline (marketing-pipeline)\n- **Pattern**: Intake > Enrich > Deliver + Generate > Review > Publish\n- **Flow**: Ingest CSV > Enrich via Apollo/Hunter/Brave/Claude > Deep-score (5 dimensions) > Segment (Hot/Warm/Cold) > Generate outreach (Hot) > Generate nurture (Warm) > Output CSVs + emails\n- **Key insight**: Multi-API enrichment with graceful degradation. Apollo (primary) > Hunter (email verification) > Brave (research) > Claude (pain analysis). Each API layer adds value but the system works with only Anthropic API.\n- **Scoring pattern**: 5-dimension scoring (size fit, tech stack compatibility, budget signals, accessibility, pain signals) with transparent point breakdown. More dimensions = more nuanced than lead-gen-machine's 4-dimension approach.\n- **Content generation pattern**: Outreach and nurture are independent post-segmentation, making them natural Agent Teams candidates. Outreach is per-lead personalized; nurture is aggregate-informed.\n- **API fallback chain**: Apollo > Hunter > Brave > Claude > HTTP scraping. Four-level fallback with clear degradation at each level.\n- **CSV-in/CSV-out**: Accepts CSV input (compatible with lead-gen-machine output), produces segmented CSVs + markdown email sequences. Easy to chain systems.\n\n### Website Uptime Monitor (website-uptime-monitor)\n- **Pattern**: Monitor > Log (simplified from Monitor > Detect > Alert)\n- **Flow**: Check URL via HTTP GET > Measure response time > Determine up/down status > Append to CSV > Commit to Git\n- **Key insight**: Simplest possible monitoring system - no detection logic, no alerting, just raw data collection. Every check produces a log entry regardless of status. Git history IS the audit trail.\n- **No MCPs needed**: Direct Python `requests` library is simpler and more reliable than Fetch MCP for a single GET request. CSV append uses stdlib. Zero external dependencies beyond GitHub Actions.\n- **Exit code signaling**: Tool exits 0 if up, 1 if down - GitHub Actions UI shows workflow as \"failed\" when site is down, providing at-a-glance status without additional alerting logic.\n- **Concurrency handling**: GitHub Actions `concurrency` setting prevents simultaneous runs. Git push includes retry with rebase to handle rare concurrent commits.\n- **CSV as database**: Append-only CSV committed to Git provides versioned, auditable, queryable dataset. No external database needed. Git handles MB-scale data easily.\n- **Cron variance accepted**: GitHub Actions cron has ±5 minute variance - documented as expected behavior rather than a bug to fix.\n- **Three execution paths**: (1) Scheduled cron (primary), (2) Manual dispatch (testing), (3) Local CLI (development). Agent HQ (4) optional for issue-driven tasks.\n- **Cost awareness**: ~1,440 GitHub Actions minutes/month for 5-minute checks. Fits within free tier (2,000/month private, unlimited public).\n- **Extensibility points**: Add alerting by extending monitor.yml (Slack webhook, GitHub Issue). Add multiple URLs via matrix strategy or separate workflows. Add authentication via tool arguments + GitHub Secrets.\n\n### CSV-to-JSON Converter (csv-to-json-converter)\n- **Pattern**: Collect > Transform > Store\n- **Flow**: Collect CSV files > Analyze structure > Parse CSV > Infer types > Validate data > Convert to JSON/JSONL > Generate reports > Commit\n- **Key insight**: Multi-phase transformation with specialist subagents. Each phase (analyze, infer, validate, write) has a dedicated subagent with focused responsibilities. Main agent orchestrates the pipeline.\n- **Subagent architecture**: Four specialist subagents (csv-analyzer, type-inference-specialist, data-validator, json-writer) with minimal tool access. Subagents are the DEFAULT delegation mechanism, not Agent Teams.\n- **Type inference pattern**: Statistical analysis with confidence scoring (80% threshold). Detects int, float, boolean, datetime, string. Falls back to string for ambiguous columns. Handles 10+ null value representations.\n- **Encoding detection**: chardet for automatic encoding detection with UTF-8 fallback. Strips BOM (byte order mark) automatically. Supports UTF-8, Latin-1, Windows-1252, etc.\n- **Ragged row handling**: Pad short rows with null, truncate long rows with warning. Log all issues in validation report but continue processing (graceful degradation).\n- **Validation without failure**: Validator NEVER raises exceptions. Always returns a report. Strict mode controls whether issues halt the pipeline, but validation itself is fail-safe.\n- **JSONL streaming**: For large files (1M+ rows), JSONL format writes one record at a time. No array wrapper. Memory-efficient for multi-GB datasets.\n- **Batch processing modes**: Sequential (1-2 files) or parallel (3+ files with Agent Teams). Parallel is 5x faster wall time but same token cost. Always has sequential fallback.\n- **Three execution paths**: (1) CLI for local development, (2) GitHub Actions for production, (3) Agent HQ for issue-driven tasks. All three produce identical output.\n- **Metadata reporting**: run_summary.json (machine-readable) + validation_report.md (human-readable). Includes per-file stats, type inference results, validation issues, recommendations.\n- **No secrets required**: Uses only stdlib (csv, json, pathlib) + chardet + python-dateutil. No external APIs. Pure data transformation pipeline.\n- **Git commit discipline**: Stage ONLY output files (never `git add -A`). Commit message includes file count, row count, timestamp. Output directory is parameterized.\n\n### RSS Digest Monitor (rss-digest-monitor)\n- **Pattern**: Monitor > Collect > Transform > Deliver (with state persistence)\n- **Flow**: Load state > Fetch RSS feeds > Filter new posts > Generate HTML digest > Send email > Update state > Commit\n- **Key insight**: State persistence in Git prevents post duplication across runs. Composite GUID keys (feed_url::guid) prevent false deduplication. State is ONLY updated after successful email delivery, ensuring retry on failure.\n- **Subagent architecture**: Three specialist subagents (rss-fetcher-specialist, digest-generator-specialist, state-manager-specialist). Subagents are the DEFAULT delegation mechanism. Sequential execution is correct due to clear data dependencies.\n- **Per-feed error isolation**: One failed feed does NOT kill the entire run. Each feed is fetched independently with try/except. Failed feeds are logged and skipped. Graceful degradation ensures partial success.\n- **Composite GUID pattern**: Use `feed_url::entry_guid` as deduplication key instead of guid alone. Prevents false positives when different feeds use the same GUID format.\n- **State file size management**: Enforce maximum 10,000 GUIDs by keeping most recent entries. Prevents unbounded state growth. Log warning if limit is hit.\n- **Email with fallback**: MIME multipart with HTML primary and plain-text fallback. Ensures compatibility with all email clients. HTML is styled with inline CSS for maximum rendering support.\n- **Conditional email send**: Skip email generation if no new posts found. Silent run with state update only. Saves API calls and prevents empty emails.\n- **SMTP retry logic**: Retry once with 30s delay on transient failures (auth, network, timeout). Do NOT update state if email send fails -- posts retry on next run.\n- **First-run initialization**: Missing or corrupted state file initializes empty state. All posts are processed on first run. Log warning but continue (better to duplicate once than crash).\n- **Date parsing with fallback**: Use python-dateutil for robust parsing of varied date formats. If unparseable, use current timestamp. Include post anyway (better to duplicate than miss).\n- **Three execution paths**: (1) Scheduled cron at 8 AM UTC (primary), (2) Manual dispatch with force_send (testing), (3) Local CLI (development). All produce identical output.\n- **Run summary logging**: Each run produces `logs/YYYY-MM-DD_run.json` with feeds_checked, feeds_failed, new_posts, email_sent, failed_feeds array. Machine-readable audit trail.\n- **Git commit discipline**: Stage ONLY state/rss_state.json and logs/*.json. NEVER `git add -A`. Commit message includes date and post count for easy history review.\n- **No MCP dependencies**: Uses standard Python libraries (feedparser, smtplib, json, pathlib). Works out-of-the-box with only Python and requirements.txt. No external MCPs required.\n- **Cost awareness**: ~3-5 minutes per day = ~90-150 GitHub Actions minutes/month. Well within free tier (2,000/month private, unlimited public). Minimal token cost for state management only.\n"}],"isError":false,"timestamp":1770815899755}}
{"type":"message","id":"ce9d3267","parentId":"4357685e","timestamp":"2026-02-11T13:18:19.757Z","message":{"role":"toolResult","toolCallId":"toolu_011Kz6FdQ8sApbvd9MLENidX","toolName":"read","content":[{"type":"text","text":"# WAT Systems Factory -- Reusable Tool Pattern Catalog\n\n> **Version:** 0.1.0-seed\n> **Maintainer:** WAT Systems Factory / Agent HQ\n> **Purpose:** Canonical reference of composable tool patterns that factory-generated workflows can assemble into pipelines. Each pattern is self-contained, testable, and convertible to an MCP tool or n8n node.\n\n---\n\n## Table of Contents\n\n1. [Web Scraping & Search](#web-scraping--search)\n2. [File I/O](#file-io)\n3. [API Calls](#api-calls)\n4. [Data Processing](#data-processing)\n5. [AI Processing](#ai-processing)\n6. [Notifications](#notifications)\n7. [Git Operations](#git-operations)\n8. [GitHub API](#github-api)\n9. [Database Operations](#database-operations)\n10. [Media Processing](#media-processing)\n11. [n8n Conversion](#n8n-conversion)\n12. [Agent Teams Coordination](#agent-teams-coordination)\n13. [GitHub Agent HQ Interaction](#github-agent-hq-interaction)\n\n---\n\n## Web Scraping & Search\n\n### `firecrawl_scrape`\n\n**Description:** Scrapes a single URL or crawls an entire domain via the Firecrawl API, returning clean Markdown-formatted content suitable for downstream LLM consumption.\n\n| Field | Detail |\n|---|---|\n| **Input** | `url: str`, `mode: Literal[\"scrape\", \"crawl\"]`, `max_pages: int = 50`, `selectors: list[str] = []` |\n| **Output** | `dict` with keys `markdown: str`, `metadata: dict`, `links: list[str]` |\n| **Key Dependencies** | `firecrawl-py`, `httpx` |\n| **MCP Alternative** | `@anthropic/firecrawl-mcp` server -- exposes `firecrawl_scrape` and `firecrawl_crawl` tools directly |\n\n```python\nimport os\nfrom firecrawl import FirecrawlApp\n\ndef main(url: str, mode: str = \"scrape\", max_pages: int = 50,\n         selectors: list[str] | None = None) -> dict:\n    \"\"\"Scrape or crawl a URL via Firecrawl and return Markdown content.\"\"\"\n    app = FirecrawlApp(api_key=os.environ[\"FIRECRAWL_API_KEY\"])\n\n    if mode == \"crawl\":\n        result = app.crawl_url(url, params={\n            \"limit\": max_pages,\n            \"scrapeOptions\": {\"formats\": [\"markdown\"]},\n        })\n        pages = [{\"markdown\": p.get(\"markdown\", \"\"), \"url\": p.get(\"url\")}\n                 for p in result.get(\"data\", [])]\n        return {\"pages\": pages, \"count\": len(pages)}\n\n    # Default: single-page scrape\n    result = app.scrape_url(url, params={\"formats\": [\"markdown\"]})\n    return {\n        \"markdown\": result.get(\"markdown\", \"\"),\n        \"metadata\": result.get(\"metadata\", {}),\n        \"links\": result.get(\"links\", []),\n    }\n```\n\n---\n\n### `brave_search`\n\n**Description:** Runs a web search query through the Brave Search API and returns structured result objects with titles, URLs, and snippets.\n\n| Field | Detail |\n|---|---|\n| **Input** | `query: str`, `count: int = 10`, `freshness: str = \"\"` |\n| **Output** | `list[dict]` each with `title`, `url`, `description`, `age` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | `@anthropic/brave-search-mcp` server -- exposes `brave_web_search` and `brave_local_search` |\n\n```python\nimport os, httpx\n\nBRAVE_ENDPOINT = \"https://api.search.brave.com/res/v1/web/search\"\n\ndef main(query: str, count: int = 10, freshness: str = \"\") -> list[dict]:\n    \"\"\"Execute a Brave web search and return structured results.\"\"\"\n    headers = {\n        \"Accept\": \"application/json\",\n        \"X-Subscription-Token\": os.environ[\"BRAVE_API_KEY\"],\n    }\n    params = {\"q\": query, \"count\": min(count, 20)}\n    if freshness:\n        params[\"freshness\"] = freshness  # e.g. \"pd\" (past day), \"pw\", \"pm\"\n\n    resp = httpx.get(BRAVE_ENDPOINT, headers=headers, params=params, timeout=15)\n    resp.raise_for_status()\n\n    results = []\n    for item in resp.json().get(\"web\", {}).get(\"results\", []):\n        results.append({\n            \"title\": item.get(\"title\"),\n            \"url\": item.get(\"url\"),\n            \"description\": item.get(\"description\"),\n            \"age\": item.get(\"age\", \"\"),\n        })\n    return results\n```\n\n---\n\n### `puppeteer_screenshot`\n\n**Description:** Launches a headless browser via Puppeteer (through its MCP server or a local Node bridge) to capture a screenshot or extract JavaScript-rendered DOM content from a page.\n\n| Field | Detail |\n|---|---|\n| **Input** | `url: str`, `action: Literal[\"screenshot\", \"extract\"]`, `selector: str = \"body\"`, `viewport: dict = {\"width\": 1280, \"height\": 720}` |\n| **Output** | `dict` with `base64_image: str` (for screenshot) or `html: str` (for extract) |\n| **Key Dependencies** | `playwright` (Python) or Node `puppeteer` via subprocess |\n| **MCP Alternative** | `@anthropic/puppeteer-mcp` server -- exposes `puppeteer_navigate`, `puppeteer_screenshot`, `puppeteer_click`, `puppeteer_fill` |\n\n```python\nfrom playwright.async_api import async_playwright\nimport asyncio, base64\n\nasync def _run(url: str, action: str, selector: str, viewport: dict) -> dict:\n    async with async_playwright() as pw:\n        browser = await pw.chromium.launch(headless=True)\n        page = await browser.new_page(viewport_size=viewport)\n        await page.goto(url, wait_until=\"networkidle\")\n\n        if action == \"screenshot\":\n            buf = await page.screenshot(full_page=True)\n            await browser.close()\n            return {\"base64_image\": base64.b64encode(buf).decode()}\n\n        content = await page.inner_html(selector)\n        await browser.close()\n        return {\"html\": content}\n\ndef main(url: str, action: str = \"screenshot\", selector: str = \"body\",\n         viewport: dict | None = None) -> dict:\n    \"\"\"Capture a screenshot or extract rendered HTML from a URL.\"\"\"\n    vp = viewport or {\"width\": 1280, \"height\": 720}\n    return asyncio.run(_run(url, action, selector, vp))\n```\n\n---\n\n## File I/O\n\n### `csv_read_write`\n\n**Description:** Reads a CSV file into a list of dictionaries or writes a list of dictionaries out to CSV, handling encoding detection and BOM stripping automatically.\n\n| Field | Detail |\n|---|---|\n| **Input** | `path: str`, `mode: Literal[\"read\", \"write\"]`, `data: list[dict] = []`, `encoding: str = \"utf-8-sig\"` |\n| **Output** | `list[dict]` on read; `dict` with `rows_written: int, path: str` on write |\n| **Key Dependencies** | `csv` (stdlib), `chardet` (optional encoding detection) |\n| **MCP Alternative** | `@anthropic/filesystem-mcp` for raw read/write; CSV parsing done in-tool |\n\n```python\nimport csv, io, pathlib\n\ndef main(path: str, mode: str = \"read\", data: list[dict] | None = None,\n         encoding: str = \"utf-8-sig\") -> dict | list[dict]:\n    \"\"\"Read or write CSV files with automatic encoding handling.\"\"\"\n    p = pathlib.Path(path)\n\n    if mode == \"read\":\n        text = p.read_text(encoding=encoding)\n        reader = csv.DictReader(io.StringIO(text))\n        return [dict(row) for row in reader]\n\n    # Write mode\n    if not data:\n        raise ValueError(\"data is required for write mode\")\n    fieldnames = list(data[0].keys())\n    with p.open(\"w\", newline=\"\", encoding=encoding) as f:\n        writer = csv.DictWriter(f, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(data)\n    return {\"rows_written\": len(data), \"path\": str(p.resolve())}\n```\n\n---\n\n### `json_read_write`\n\n**Description:** Reads a JSON or JSONL file into Python objects or writes Python objects out to JSON/JSONL, with optional pretty-printing and schema validation.\n\n| Field | Detail |\n|---|---|\n| **Input** | `path: str`, `mode: Literal[\"read\", \"write\"]`, `data: Any = None`, `jsonl: bool = False`, `indent: int = 2` |\n| **Output** | Parsed object(s) on read; `dict` with `path: str, size_bytes: int` on write |\n| **Key Dependencies** | `json` (stdlib), `jsonschema` (optional validation) |\n| **MCP Alternative** | `@anthropic/filesystem-mcp` for raw I/O |\n\n```python\nimport json, pathlib\n\ndef main(path: str, mode: str = \"read\", data=None,\n         jsonl: bool = False, indent: int = 2):\n    \"\"\"Read or write JSON / JSONL files.\"\"\"\n    p = pathlib.Path(path)\n\n    if mode == \"read\":\n        text = p.read_text(encoding=\"utf-8\")\n        if jsonl:\n            return [json.loads(line) for line in text.splitlines() if line.strip()]\n        return json.loads(text)\n\n    # Write mode\n    if jsonl:\n        lines = [json.dumps(item, ensure_ascii=False) for item in data]\n        p.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n    else:\n        p.write_text(json.dumps(data, indent=indent, ensure_ascii=False),\n                      encoding=\"utf-8\")\n    return {\"path\": str(p.resolve()), \"size_bytes\": p.stat().st_size}\n```\n\n---\n\n### `apollo_enrich`\n\n**Description:** Enriches a company name/domain with company data and decision-maker contacts via the Apollo.io API. Returns structured company info, tech stack, and key contacts.\n\n| Field | Detail |\n|---|---|\n| **Input** | `company_name: str`, `domain: str = None` |\n| **Output** | `dict` with company data, tech stack, decision makers |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | None; direct HTTP API |\n\n---\n\n### `hunter_email_search`\n\n**Description:** Searches for email addresses at a given domain via the Hunter.io API. Returns verified contacts with confidence scores.\n\n| Field | Detail |\n|---|---|\n| **Input** | `domain: str`, `limit: int = 5` |\n| **Output** | `dict` with emails, patterns, organization info |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | None; direct HTTP API |\n\n---\n\n### `multi_dimension_scorer`\n\n**Description:** Scores a data record across N independent dimensions with configurable point values. Used for lead scoring, content quality assessment, or any multi-criteria evaluation.\n\n| Field | Detail |\n|---|---|\n| **Input** | `record: dict`, `dimensions: list[dict]` (each with name, max_points, scoring_function) |\n| **Output** | `dict` with dimension breakdown and total score |\n| **Key Dependencies** | None (stdlib only) |\n| **MCP Alternative** | None |\n\n---\n\n### `email_sequence_generator`\n\n**Description:** Generates a multi-email sequence (outreach or nurture) using Claude API. Takes company/audience context and a sequence template, returns structured JSON with subject lines, bodies, and send timing.\n\n| Field | Detail |\n|---|---|\n| **Input** | `context: dict`, `sequence_template: str`, `email_count: int` |\n| **Output** | `dict` with sequence of emails (subject, body, send_day) |\n| **Key Dependencies** | `anthropic` |\n| **MCP Alternative** | Anthropic MCP |\n\n---\n\n### `pdf_extract`\n\n**Description:** Extracts text content, metadata, and optionally tables from a PDF file, returning structured output ready for LLM ingestion or data-pipeline processing.\n\n| Field | Detail |\n|---|---|\n| **Input** | `path: str`, `pages: str = \"all\"`, `extract_tables: bool = False` |\n| **Output** | `dict` with `text: str`, `pages: list[dict]`, `metadata: dict`, `tables: list` |\n| **Key Dependencies** | `pymupdf` (fitz), `pdfplumber` (for tables) |\n| **MCP Alternative** | None standard; wrap as custom MCP tool |\n\n```python\nimport fitz  # pymupdf\n\ndef main(path: str, pages: str = \"all\", extract_tables: bool = False) -> dict:\n    \"\"\"Extract text and metadata from a PDF file.\"\"\"\n    doc = fitz.open(path)\n    metadata = dict(doc.metadata)\n\n    page_range = range(len(doc))\n    if pages != \"all\":\n        # Parse ranges like \"1-5\" or \"3,7,10-12\"\n        page_range = _parse_page_range(pages, len(doc))\n\n    extracted_pages = []\n    for i in page_range:\n        page = doc[i]\n        extracted_pages.append({\n            \"page_number\": i + 1,\n            \"text\": page.get_text(\"text\"),\n        })\n\n    result = {\n        \"text\": \"\\n\\n\".join(p[\"text\"] for p in extracted_pages),\n        \"pages\": extracted_pages,\n        \"metadata\": metadata,\n    }\n\n    if extract_tables:\n        import pdfplumber\n        with pdfplumber.open(path) as pdf:\n            result[\"tables\"] = []\n            for i in page_range:\n                for table in pdf.pages[i].extract_tables():\n                    result[\"tables\"].append({\"page\": i + 1, \"rows\": table})\n\n    return result\n\ndef _parse_page_range(spec: str, total: int) -> list[int]:\n    indices = []\n    for part in spec.split(\",\"):\n        if \"-\" in part:\n            start, end = part.split(\"-\", 1)\n            indices.extend(range(int(start) - 1, min(int(end), total)))\n        else:\n            indices.append(int(part) - 1)\n    return [i for i in indices if 0 <= i < total]\n```\n\n---\n\n## API Calls\n\n### `rest_client`\n\n**Description:** A generic REST API client with configurable retry logic, exponential backoff, header injection, and response normalization -- the universal building block for any HTTP-based integration.\n\n| Field | Detail |\n|---|---|\n| **Input** | `url: str`, `method: str = \"GET\"`, `headers: dict = {}`, `body: Any = None`, `retries: int = 3`, `backoff: float = 1.0`, `timeout: int = 30` |\n| **Output** | `dict` with `status: int`, `headers: dict`, `body: Any`, `elapsed_ms: float` |\n| **Key Dependencies** | `httpx`, `tenacity` |\n| **MCP Alternative** | `@anthropic/fetch-mcp` for simple GET/POST |\n\n```python\nimport httpx, time\nfrom tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception\n\ndef _is_retryable(exc):\n    if isinstance(exc, httpx.HTTPStatusError):\n        return exc.response.status_code in (429, 500, 502, 503, 504)\n    return isinstance(exc, (httpx.ConnectTimeout, httpx.ReadTimeout))\n\ndef main(url: str, method: str = \"GET\", headers: dict | None = None,\n         body=None, retries: int = 3, backoff: float = 1.0,\n         timeout: int = 30) -> dict:\n    \"\"\"Make an HTTP request with retry logic and exponential backoff.\"\"\"\n\n    @retry(\n        stop=stop_after_attempt(retries),\n        wait=wait_exponential(multiplier=backoff, min=1, max=30),\n        retry=retry_if_exception(_is_retryable),\n        reraise=True,\n    )\n    def _do_request():\n        t0 = time.monotonic()\n        resp = httpx.request(\n            method=method.upper(),\n            url=url,\n            headers=headers or {},\n            json=body if isinstance(body, (dict, list)) else None,\n            content=body if isinstance(body, (str, bytes)) else None,\n            timeout=timeout,\n        )\n        elapsed = (time.monotonic() - t0) * 1000\n        resp.raise_for_status()\n        try:\n            resp_body = resp.json()\n        except Exception:\n            resp_body = resp.text\n        return {\n            \"status\": resp.status_code,\n            \"headers\": dict(resp.headers),\n            \"body\": resp_body,\n            \"elapsed_ms\": round(elapsed, 2),\n        }\n\n    return _do_request()\n```\n\n---\n\n### `graphql_client`\n\n**Description:** Sends GraphQL queries and mutations to any endpoint, with variable substitution, error extraction, and the same retry semantics as `rest_client`.\n\n| Field | Detail |\n|---|---|\n| **Input** | `endpoint: str`, `query: str`, `variables: dict = {}`, `headers: dict = {}`, `retries: int = 3` |\n| **Output** | `dict` with `data: Any`, `errors: list | None`, `status: int` |\n| **Key Dependencies** | `httpx`, `tenacity` |\n| **MCP Alternative** | None standard; wrap as custom MCP tool |\n\n```python\nimport httpx\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\ndef main(endpoint: str, query: str, variables: dict | None = None,\n         headers: dict | None = None, retries: int = 3) -> dict:\n    \"\"\"Execute a GraphQL query or mutation with retry logic.\"\"\"\n\n    @retry(stop=stop_after_attempt(retries),\n           wait=wait_exponential(multiplier=1, min=1, max=15))\n    def _execute():\n        payload = {\"query\": query}\n        if variables:\n            payload[\"variables\"] = variables\n        resp = httpx.post(\n            endpoint,\n            json=payload,\n            headers={\"Content-Type\": \"application/json\", **(headers or {})},\n            timeout=30,\n        )\n        resp.raise_for_status()\n        result = resp.json()\n        return {\n            \"data\": result.get(\"data\"),\n            \"errors\": result.get(\"errors\"),\n            \"status\": resp.status_code,\n        }\n\n    return _execute()\n```\n\n---\n\n### `oauth_token_refresh`\n\n**Description:** Manages OAuth 2.0 token lifecycle -- obtains tokens via client_credentials or authorization_code grants, caches them, and transparently refreshes before expiry.\n\n| Field | Detail |\n|---|---|\n| **Input** | `token_url: str`, `client_id: str`, `client_secret: str`, `grant_type: str = \"client_credentials\"`, `refresh_token: str = \"\"`, `scopes: list[str] = []` |\n| **Output** | `dict` with `access_token: str`, `expires_in: int`, `token_type: str` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | None; typically embedded within other tools |\n\n```python\nimport httpx, time\n\n_token_cache: dict = {}\n\ndef main(token_url: str, client_id: str, client_secret: str,\n         grant_type: str = \"client_credentials\",\n         refresh_token: str = \"\", scopes: list[str] | None = None) -> dict:\n    \"\"\"Obtain or refresh an OAuth 2.0 access token.\"\"\"\n    cache_key = f\"{token_url}:{client_id}\"\n\n    # Return cached token if still valid\n    cached = _token_cache.get(cache_key)\n    if cached and cached[\"expires_at\"] > time.time() + 60:\n        return {\n            \"access_token\": cached[\"access_token\"],\n            \"expires_in\": int(cached[\"expires_at\"] - time.time()),\n            \"token_type\": cached[\"token_type\"],\n        }\n\n    data = {\"grant_type\": grant_type, \"client_id\": client_id,\n            \"client_secret\": client_secret}\n    if refresh_token:\n        data[\"refresh_token\"] = refresh_token\n    if scopes:\n        data[\"scope\"] = \" \".join(scopes)\n\n    resp = httpx.post(token_url, data=data, timeout=15)\n    resp.raise_for_status()\n    token_data = resp.json()\n\n    _token_cache[cache_key] = {\n        \"access_token\": token_data[\"access_token\"],\n        \"token_type\": token_data.get(\"token_type\", \"Bearer\"),\n        \"expires_at\": time.time() + token_data.get(\"expires_in\", 3600),\n    }\n    return {\n        \"access_token\": token_data[\"access_token\"],\n        \"expires_in\": token_data.get(\"expires_in\", 3600),\n        \"token_type\": token_data.get(\"token_type\", \"Bearer\"),\n    }\n```\n\n---\n\n## Data Processing\n\n### `filter_sort`\n\n**Description:** Applies filter predicates and multi-key sorting to a list of dictionaries, supporting comparison operators, regex matching, and null handling.\n\n| Field | Detail |\n|---|---|\n| **Input** | `data: list[dict]`, `filters: list[dict]` (each with `field`, `op`, `value`), `sort_by: list[dict]` (each with `field`, `order`) |\n| **Output** | `list[dict]` -- filtered and sorted records |\n| **Key Dependencies** | `re` (stdlib) |\n| **MCP Alternative** | None; pure logic tool |\n\n```python\nimport re, operator\n\nOPS = {\n    \"eq\": operator.eq, \"ne\": operator.ne,\n    \"gt\": operator.gt, \"ge\": operator.ge,\n    \"lt\": operator.lt, \"le\": operator.le,\n    \"contains\": lambda a, b: b in str(a),\n    \"regex\": lambda a, b: bool(re.search(b, str(a))),\n    \"in\": lambda a, b: a in b,\n}\n\ndef main(data: list[dict], filters: list[dict] | None = None,\n         sort_by: list[dict] | None = None) -> list[dict]:\n    \"\"\"Filter and sort a list of dictionaries.\"\"\"\n    result = data\n\n    # Apply filters\n    for f in (filters or []):\n        op_fn = OPS.get(f[\"op\"], operator.eq)\n        result = [row for row in result if op_fn(row.get(f[\"field\"]), f[\"value\"])]\n\n    # Apply sorting\n    if sort_by:\n        for s in reversed(sort_by):\n            reverse = s.get(\"order\", \"asc\") == \"desc\"\n            result.sort(key=lambda row: (row.get(s[\"field\"]) is None,\n                                          row.get(s[\"field\"], \"\")),\n                        reverse=reverse)\n\n    return result\n```\n\n---\n\n### `dedup`\n\n**Description:** Removes duplicate records from a dataset based on one or more key fields, with configurable strategies for which duplicate to keep (first, last, or merge).\n\n| Field | Detail |\n|---|---|\n| **Input** | `data: list[dict]`, `keys: list[str]`, `strategy: Literal[\"first\", \"last\", \"merge\"] = \"first\"` |\n| **Output** | `dict` with `unique: list[dict]`, `duplicates_removed: int` |\n| **Key Dependencies** | None (stdlib only) |\n| **MCP Alternative** | None; pure logic tool |\n\n```python\ndef main(data: list[dict], keys: list[str],\n         strategy: str = \"first\") -> dict:\n    \"\"\"Deduplicate records by key fields.\"\"\"\n    seen: dict[tuple, dict] = {}\n    order: list[tuple] = []\n\n    for row in data:\n        key = tuple(row.get(k) for k in keys)\n        if key not in seen:\n            seen[key] = row\n            order.append(key)\n        elif strategy == \"last\":\n            seen[key] = row\n        elif strategy == \"merge\":\n            # Merge: later values fill in blanks only\n            for k, v in row.items():\n                if seen[key].get(k) in (None, \"\"):\n                    seen[key][k] = v\n\n    unique = [seen[k] for k in order]\n    return {\n        \"unique\": unique,\n        \"duplicates_removed\": len(data) - len(unique),\n    }\n```\n\n---\n\n### `transform_map`\n\n**Description:** Applies field-level transformations to every record in a dataset -- renaming columns, computing derived fields, type-casting, and applying template expressions.\n\n| Field | Detail |\n|---|---|\n| **Input** | `data: list[dict]`, `mappings: list[dict]` (each with `source`, `target`, `transform`), `drop_unmapped: bool = False` |\n| **Output** | `list[dict]` -- transformed records |\n| **Key Dependencies** | None (stdlib only) |\n| **MCP Alternative** | None; pure logic tool |\n\n```python\nfrom datetime import datetime\n\nTRANSFORMS = {\n    \"upper\": lambda v: str(v).upper(),\n    \"lower\": lambda v: str(v).lower(),\n    \"strip\": lambda v: str(v).strip(),\n    \"int\": lambda v: int(v) if v not in (None, \"\") else None,\n    \"float\": lambda v: float(v) if v not in (None, \"\") else None,\n    \"isodate\": lambda v: datetime.fromisoformat(str(v)).isoformat(),\n    \"bool\": lambda v: str(v).lower() in (\"true\", \"1\", \"yes\"),\n    \"default\": lambda v: v,\n}\n\ndef main(data: list[dict], mappings: list[dict],\n         drop_unmapped: bool = False) -> list[dict]:\n    \"\"\"Apply field-level transformations to each record.\"\"\"\n    results = []\n    for row in data:\n        new_row = {} if drop_unmapped else dict(row)\n        for m in mappings:\n            source_val = row.get(m[\"source\"])\n            transform_fn = TRANSFORMS.get(m.get(\"transform\", \"default\"),\n                                           TRANSFORMS[\"default\"])\n            new_row[m[\"target\"]] = transform_fn(source_val)\n        results.append(new_row)\n    return results\n```\n\n---\n\n## AI Processing\n\n### `llm_prompt`\n\n**Description:** Sends a prompt to an LLM provider (OpenAI, Anthropic, or compatible endpoint) with system instructions, returning the raw text completion and token usage metadata.\n\n| Field | Detail |\n|---|---|\n| **Input** | `prompt: str`, `system: str = \"\"`, `provider: Literal[\"openai\", \"anthropic\"] = \"anthropic\"`, `model: str = \"\"`, `max_tokens: int = 4096`, `temperature: float = 0.3` |\n| **Output** | `dict` with `text: str`, `model: str`, `usage: dict` |\n| **Key Dependencies** | `anthropic`, `openai` |\n| **MCP Alternative** | None; this IS the core LLM call |\n\n```python\nimport os\n\ndef main(prompt: str, system: str = \"\", provider: str = \"anthropic\",\n         model: str = \"\", max_tokens: int = 4096,\n         temperature: float = 0.3) -> dict:\n    \"\"\"Send a prompt to an LLM and return the completion.\"\"\"\n\n    if provider == \"anthropic\":\n        import anthropic\n        client = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n        resolved_model = model or \"claude-sonnet-4-20250514\"\n        msg = client.messages.create(\n            model=resolved_model,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            system=system or \"You are a helpful assistant.\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n        )\n        return {\n            \"text\": msg.content[0].text,\n            \"model\": resolved_model,\n            \"usage\": {\"input_tokens\": msg.usage.input_tokens,\n                      \"output_tokens\": msg.usage.output_tokens},\n        }\n\n    # OpenAI / compatible\n    import openai\n    client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n    resolved_model = model or \"gpt-4o\"\n    messages = []\n    if system:\n        messages.append({\"role\": \"system\", \"content\": system})\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n    resp = client.chat.completions.create(\n        model=resolved_model, messages=messages,\n        max_tokens=max_tokens, temperature=temperature,\n    )\n    choice = resp.choices[0]\n    return {\n        \"text\": choice.message.content,\n        \"model\": resolved_model,\n        \"usage\": {\"input_tokens\": resp.usage.prompt_tokens,\n                  \"output_tokens\": resp.usage.completion_tokens},\n    }\n```\n\n---\n\n### `structured_extract`\n\n**Description:** Sends content to an LLM with a JSON schema instruction, parses the response into a validated Python object, and retries with corrective prompts if the response fails schema validation.\n\n| Field | Detail |\n|---|---|\n| **Input** | `content: str`, `schema: dict` (JSON Schema), `instructions: str = \"\"`, `retries: int = 2` |\n| **Output** | `dict` with `data: Any` (validated), `raw: str`, `attempts: int` |\n| **Key Dependencies** | `anthropic` or `openai`, `jsonschema` |\n| **MCP Alternative** | None; core LLM extraction pattern |\n\n```python\nimport json, os\nfrom jsonschema import validate, ValidationError\n\ndef main(content: str, schema: dict, instructions: str = \"\",\n         retries: int = 2) -> dict:\n    \"\"\"Extract structured data from content using an LLM with schema validation.\"\"\"\n    import anthropic\n    client = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n\n    system_prompt = (\n        \"Extract information from the provided content and return ONLY valid JSON \"\n        \"matching the schema below. No markdown fences, no commentary.\\n\\n\"\n        f\"Schema:\\n```json\\n{json.dumps(schema, indent=2)}\\n```\"\n    )\n    if instructions:\n        system_prompt += f\"\\n\\nAdditional instructions: {instructions}\"\n\n    user_content = content\n    for attempt in range(1, retries + 2):\n        msg = client.messages.create(\n            model=\"claude-sonnet-4-20250514\",\n            max_tokens=4096,\n            temperature=0.0,\n            system=system_prompt,\n            messages=[{\"role\": \"user\", \"content\": user_content}],\n        )\n        raw = msg.content[0].text.strip()\n\n        # Strip markdown fences if present\n        if raw.startswith(\"```\"):\n            raw = raw.split(\"\\n\", 1)[1].rsplit(\"```\", 1)[0].strip()\n\n        try:\n            parsed = json.loads(raw)\n            validate(instance=parsed, schema=schema)\n            return {\"data\": parsed, \"raw\": raw, \"attempts\": attempt}\n        except (json.JSONDecodeError, ValidationError) as exc:\n            if attempt > retries:\n                raise ValueError(f\"Failed after {attempt} attempts: {exc}\") from exc\n            user_content = (\n                f\"Your previous response was invalid: {exc}\\n\\n\"\n                f\"Original content:\\n{content}\\n\\n\"\n                \"Please try again, returning ONLY valid JSON.\"\n            )\n\n    raise RuntimeError(\"Unreachable\")\n```\n\n---\n\n### `embedding_generate`\n\n**Description:** Generates vector embeddings for one or more text inputs using OpenAI, Voyage, or a local model, returning arrays ready for storage in a vector database.\n\n| Field | Detail |\n|---|---|\n| **Input** | `texts: list[str]`, `provider: Literal[\"openai\", \"voyage\"] = \"openai\"`, `model: str = \"\"` |\n| **Output** | `dict` with `embeddings: list[list[float]]`, `model: str`, `dimensions: int` |\n| **Key Dependencies** | `openai` or `voyageai` |\n| **MCP Alternative** | None standard |\n\n```python\nimport os\n\ndef main(texts: list[str], provider: str = \"openai\",\n         model: str = \"\") -> dict:\n    \"\"\"Generate vector embeddings for text inputs.\"\"\"\n\n    if provider == \"openai\":\n        import openai\n        client = openai.OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n        resolved = model or \"text-embedding-3-small\"\n        resp = client.embeddings.create(input=texts, model=resolved)\n        vecs = [item.embedding for item in resp.data]\n        return {\n            \"embeddings\": vecs,\n            \"model\": resolved,\n            \"dimensions\": len(vecs[0]) if vecs else 0,\n        }\n\n    if provider == \"voyage\":\n        import voyageai\n        client = voyageai.Client(api_key=os.environ[\"VOYAGE_API_KEY\"])\n        resolved = model or \"voyage-2\"\n        resp = client.embed(texts, model=resolved)\n        return {\n            \"embeddings\": resp.embeddings,\n            \"model\": resolved,\n            \"dimensions\": len(resp.embeddings[0]) if resp.embeddings else 0,\n        }\n\n    raise ValueError(f\"Unsupported provider: {provider}\")\n```\n\n---\n\n## Notifications\n\n### `slack_notify`\n\n**Description:** Sends a message to a Slack channel or thread via the Slack Web API or an incoming webhook, supporting Block Kit for rich formatting.\n\n| Field | Detail |\n|---|---|\n| **Input** | `channel: str`, `text: str`, `blocks: list[dict] = []`, `thread_ts: str = \"\"`, `use_webhook: bool = False` |\n| **Output** | `dict` with `ok: bool`, `ts: str`, `channel: str` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | `@anthropic/slack-mcp` server -- exposes `slack_post_message`, `slack_list_channels`, etc. |\n\n```python\nimport os, httpx\n\ndef main(channel: str, text: str, blocks: list[dict] | None = None,\n         thread_ts: str = \"\", use_webhook: bool = False) -> dict:\n    \"\"\"Send a Slack message to a channel or thread.\"\"\"\n\n    if use_webhook:\n        webhook_url = os.environ[\"SLACK_WEBHOOK_URL\"]\n        payload = {\"text\": text}\n        if blocks:\n            payload[\"blocks\"] = blocks\n        resp = httpx.post(webhook_url, json=payload, timeout=10)\n        resp.raise_for_status()\n        return {\"ok\": True, \"ts\": \"\", \"channel\": channel}\n\n    token = os.environ[\"SLACK_BOT_TOKEN\"]\n    payload = {\"channel\": channel, \"text\": text}\n    if blocks:\n        payload[\"blocks\"] = blocks\n    if thread_ts:\n        payload[\"thread_ts\"] = thread_ts\n\n    resp = httpx.post(\n        \"https://slack.com/api/chat.postMessage\",\n        json=payload,\n        headers={\"Authorization\": f\"Bearer {token}\",\n                 \"Content-Type\": \"application/json\"},\n        timeout=10,\n    )\n    data = resp.json()\n    return {\"ok\": data.get(\"ok\", False), \"ts\": data.get(\"ts\", \"\"),\n            \"channel\": data.get(\"channel\", channel)}\n```\n\n---\n\n### `discord_webhook`\n\n**Description:** Posts a message or embed to a Discord channel via a webhook URL, with support for rich embeds, username overrides, and file attachments.\n\n| Field | Detail |\n|---|---|\n| **Input** | `webhook_url: str`, `content: str = \"\"`, `embeds: list[dict] = []`, `username: str = \"\"` |\n| **Output** | `dict` with `status: int`, `success: bool` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | None standard; wrap as custom MCP tool |\n\n```python\nimport httpx\n\ndef main(webhook_url: str, content: str = \"\", embeds: list[dict] | None = None,\n         username: str = \"\") -> dict:\n    \"\"\"Send a message or embed to Discord via webhook.\"\"\"\n    payload: dict = {}\n    if content:\n        payload[\"content\"] = content\n    if embeds:\n        payload[\"embeds\"] = embeds\n    if username:\n        payload[\"username\"] = username\n\n    if not payload:\n        raise ValueError(\"Must provide content or embeds\")\n\n    resp = httpx.post(webhook_url, json=payload, timeout=10)\n    return {\n        \"status\": resp.status_code,\n        \"success\": resp.status_code in (200, 204),\n    }\n```\n\n---\n\n### `email_send`\n\n**Description:** Sends an email via SMTP or a transactional email API (SendGrid, Resend), supporting HTML body, plain-text fallback, and file attachments.\n\n| Field | Detail |\n|---|---|\n| **Input** | `to: list[str]`, `subject: str`, `body_html: str = \"\"`, `body_text: str = \"\"`, `provider: Literal[\"smtp\", \"sendgrid\", \"resend\"] = \"resend\"`, `from_addr: str = \"\"` |\n| **Output** | `dict` with `success: bool`, `message_id: str` |\n| **Key Dependencies** | `httpx`, `smtplib` (stdlib) |\n| **MCP Alternative** | None standard |\n\n```python\nimport os, httpx\n\ndef main(to: list[str], subject: str, body_html: str = \"\",\n         body_text: str = \"\", provider: str = \"resend\",\n         from_addr: str = \"\") -> dict:\n    \"\"\"Send an email via a transactional provider or SMTP.\"\"\"\n    sender = from_addr or os.environ.get(\"EMAIL_FROM\", \"noreply@example.com\")\n\n    if provider == \"resend\":\n        resp = httpx.post(\n            \"https://api.resend.com/emails\",\n            headers={\"Authorization\": f\"Bearer {os.environ['RESEND_API_KEY']}\",\n                     \"Content-Type\": \"application/json\"},\n            json={\n                \"from\": sender, \"to\": to, \"subject\": subject,\n                \"html\": body_html or f\"<pre>{body_text}</pre>\",\n                \"text\": body_text or \"\",\n            },\n            timeout=15,\n        )\n        resp.raise_for_status()\n        data = resp.json()\n        return {\"success\": True, \"message_id\": data.get(\"id\", \"\")}\n\n    if provider == \"sendgrid\":\n        resp = httpx.post(\n            \"https://api.sendgrid.com/v3/mail/send\",\n            headers={\"Authorization\": f\"Bearer {os.environ['SENDGRID_API_KEY']}\",\n                     \"Content-Type\": \"application/json\"},\n            json={\n                \"personalizations\": [{\"to\": [{\"email\": e} for e in to]}],\n                \"from\": {\"email\": sender},\n                \"subject\": subject,\n                \"content\": [\n                    {\"type\": \"text/plain\", \"value\": body_text or \"\"},\n                    {\"type\": \"text/html\", \"value\": body_html or body_text},\n                ],\n            },\n            timeout=15,\n        )\n        resp.raise_for_status()\n        return {\"success\": True, \"message_id\": resp.headers.get(\"X-Message-Id\", \"\")}\n\n    # SMTP fallback\n    import smtplib\n    from email.mime.text import MIMEText\n    msg = MIMEText(body_html or body_text, \"html\" if body_html else \"plain\")\n    msg[\"Subject\"] = subject\n    msg[\"From\"] = sender\n    msg[\"To\"] = \", \".join(to)\n    with smtplib.SMTP(os.environ[\"SMTP_HOST\"], int(os.environ.get(\"SMTP_PORT\", 587))) as s:\n        s.starttls()\n        s.login(os.environ[\"SMTP_USER\"], os.environ[\"SMTP_PASS\"])\n        s.send_message(msg)\n    return {\"success\": True, \"message_id\": \"\"}\n```\n\n---\n\n## Git Operations\n\n### `git_commit_push`\n\n**Description:** Stages specified files (or all changes), creates a commit with a conventional-commit message, and optionally pushes to the remote -- the bread-and-butter of automated code updates.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo_path: str`, `message: str`, `files: list[str] = []`, `push: bool = True`, `branch: str = \"\"` |\n| **Output** | `dict` with `commit_sha: str`, `branch: str`, `pushed: bool` |\n| **Key Dependencies** | `subprocess` (stdlib), `git` CLI |\n| **MCP Alternative** | `@anthropic/git-mcp` server -- exposes `git_status`, `git_commit`, `git_push`, etc. |\n\n```python\nimport subprocess\n\ndef _git(repo: str, *args: str) -> str:\n    result = subprocess.run(\n        [\"git\", *args], cwd=repo,\n        capture_output=True, text=True, check=True,\n    )\n    return result.stdout.strip()\n\ndef main(repo_path: str, message: str, files: list[str] | None = None,\n         push: bool = True, branch: str = \"\") -> dict:\n    \"\"\"Stage, commit, and optionally push changes in a git repo.\"\"\"\n    if branch:\n        try:\n            _git(repo_path, \"checkout\", branch)\n        except subprocess.CalledProcessError:\n            _git(repo_path, \"checkout\", \"-b\", branch)\n\n    # Stage\n    if files:\n        for f in files:\n            _git(repo_path, \"add\", f)\n    else:\n        _git(repo_path, \"add\", \"-A\")\n\n    # Commit\n    _git(repo_path, \"commit\", \"-m\", message)\n    sha = _git(repo_path, \"rev-parse\", \"HEAD\")\n\n    current_branch = _git(repo_path, \"rev-parse\", \"--abbrev-ref\", \"HEAD\")\n\n    pushed = False\n    if push:\n        _git(repo_path, \"push\", \"-u\", \"origin\", current_branch)\n        pushed = True\n\n    return {\"commit_sha\": sha, \"branch\": current_branch, \"pushed\": pushed}\n```\n\n---\n\n### `git_branch_manage`\n\n**Description:** Creates, lists, switches, or deletes branches in a local repository, with optional remote tracking setup.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo_path: str`, `action: Literal[\"create\", \"switch\", \"delete\", \"list\"]`, `branch: str = \"\"`, `from_ref: str = \"main\"` |\n| **Output** | `dict` with `branch: str`, `action: str`, `branches: list[str]` (for list action) |\n| **Key Dependencies** | `subprocess` (stdlib), `git` CLI |\n| **MCP Alternative** | `@anthropic/git-mcp` |\n\n```python\nimport subprocess\n\ndef _git(repo: str, *args: str) -> str:\n    result = subprocess.run(\n        [\"git\", *args], cwd=repo,\n        capture_output=True, text=True, check=True,\n    )\n    return result.stdout.strip()\n\ndef main(repo_path: str, action: str = \"list\", branch: str = \"\",\n         from_ref: str = \"main\") -> dict:\n    \"\"\"Create, switch, delete, or list branches.\"\"\"\n\n    if action == \"list\":\n        raw = _git(repo_path, \"branch\", \"-a\")\n        branches = [b.strip().lstrip(\"* \") for b in raw.splitlines()]\n        return {\"action\": \"list\", \"branches\": branches, \"branch\": \"\"}\n\n    if not branch:\n        raise ValueError(\"branch name required for create/switch/delete\")\n\n    if action == \"create\":\n        _git(repo_path, \"checkout\", \"-b\", branch, from_ref)\n        return {\"action\": \"create\", \"branch\": branch, \"branches\": []}\n\n    if action == \"switch\":\n        _git(repo_path, \"checkout\", branch)\n        return {\"action\": \"switch\", \"branch\": branch, \"branches\": []}\n\n    if action == \"delete\":\n        _git(repo_path, \"branch\", \"-d\", branch)\n        return {\"action\": \"delete\", \"branch\": branch, \"branches\": []}\n\n    raise ValueError(f\"Unknown action: {action}\")\n```\n\n---\n\n### `git_read_repo`\n\n**Description:** Reads repository structure and file contents without requiring a local clone, using either a local path or the GitHub API for remote repos.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo: str` (local path or `owner/repo`), `path: str = \"\"`, `ref: str = \"main\"`, `depth: int = 2` |\n| **Output** | `dict` with `tree: list[dict]`, `file_content: str` (if path points to a file) |\n| **Key Dependencies** | `httpx`, `subprocess` (stdlib) |\n| **MCP Alternative** | `@anthropic/github-mcp` for remote repos |\n\n```python\nimport os, httpx, subprocess, pathlib\n\ndef main(repo: str, path: str = \"\", ref: str = \"main\",\n         depth: int = 2) -> dict:\n    \"\"\"Read repository tree or file contents.\"\"\"\n\n    # Local repo\n    if os.path.isdir(repo):\n        target = pathlib.Path(repo) / path\n        if target.is_file():\n            return {\"tree\": [], \"file_content\": target.read_text(encoding=\"utf-8\")}\n        tree = []\n        for item in sorted(target.rglob(\"*\")):\n            rel = item.relative_to(pathlib.Path(repo))\n            if len(rel.parts) <= depth and not any(p.startswith(\".\") for p in rel.parts):\n                tree.append({\n                    \"path\": str(rel), \"type\": \"dir\" if item.is_dir() else \"file\",\n                    \"size\": item.stat().st_size if item.is_file() else 0,\n                })\n        return {\"tree\": tree, \"file_content\": \"\"}\n\n    # Remote GitHub repo\n    headers = {\"Authorization\": f\"token {os.environ['GITHUB_TOKEN']}\",\n               \"Accept\": \"application/vnd.github.v3+json\"}\n    base = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n    resp = httpx.get(base, headers=headers, params={\"ref\": ref}, timeout=15)\n    resp.raise_for_status()\n    data = resp.json()\n\n    if isinstance(data, dict) and data.get(\"type\") == \"file\":\n        import base64\n        content = base64.b64decode(data[\"content\"]).decode(\"utf-8\")\n        return {\"tree\": [], \"file_content\": content}\n\n    tree = [{\"path\": item[\"path\"], \"type\": item[\"type\"], \"size\": item.get(\"size\", 0)}\n            for item in data]\n    return {\"tree\": tree, \"file_content\": \"\"}\n```\n\n---\n\n## GitHub API\n\n### `github_create_issue`\n\n**Description:** Creates a new issue on a GitHub repository with title, body, labels, assignees, and milestone, returning the full issue URL and number.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo: str` (`owner/repo`), `title: str`, `body: str = \"\"`, `labels: list[str] = []`, `assignees: list[str] = []`, `milestone: int = 0` |\n| **Output** | `dict` with `number: int`, `url: str`, `html_url: str` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | `@anthropic/github-mcp` -- exposes `create_issue` |\n\n```python\nimport os, httpx\n\ndef main(repo: str, title: str, body: str = \"\",\n         labels: list[str] | None = None,\n         assignees: list[str] | None = None,\n         milestone: int = 0) -> dict:\n    \"\"\"Create a GitHub issue.\"\"\"\n    headers = {\n        \"Authorization\": f\"token {os.environ['GITHUB_TOKEN']}\",\n        \"Accept\": \"application/vnd.github.v3+json\",\n    }\n    payload: dict = {\"title\": title, \"body\": body}\n    if labels:\n        payload[\"labels\"] = labels\n    if assignees:\n        payload[\"assignees\"] = assignees\n    if milestone:\n        payload[\"milestone\"] = milestone\n\n    resp = httpx.post(\n        f\"https://api.github.com/repos/{repo}/issues\",\n        headers=headers, json=payload, timeout=15,\n    )\n    resp.raise_for_status()\n    data = resp.json()\n    return {\n        \"number\": data[\"number\"],\n        \"url\": data[\"url\"],\n        \"html_url\": data[\"html_url\"],\n    }\n```\n\n---\n\n### `github_pull_request`\n\n**Description:** Creates or updates a pull request, with support for draft PRs, auto-merge labels, and reviewer assignment.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo: str`, `title: str`, `body: str`, `head: str`, `base: str = \"main\"`, `draft: bool = False`, `reviewers: list[str] = []` |\n| **Output** | `dict` with `number: int`, `html_url: str`, `state: str` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | `@anthropic/github-mcp` -- exposes `create_pull_request` |\n\n```python\nimport os, httpx\n\ndef main(repo: str, title: str, body: str, head: str,\n         base: str = \"main\", draft: bool = False,\n         reviewers: list[str] | None = None) -> dict:\n    \"\"\"Create a GitHub pull request.\"\"\"\n    headers = {\n        \"Authorization\": f\"token {os.environ['GITHUB_TOKEN']}\",\n        \"Accept\": \"application/vnd.github.v3+json\",\n    }\n    payload = {\n        \"title\": title, \"body\": body,\n        \"head\": head, \"base\": base, \"draft\": draft,\n    }\n\n    resp = httpx.post(\n        f\"https://api.github.com/repos/{repo}/pulls\",\n        headers=headers, json=payload, timeout=15,\n    )\n    resp.raise_for_status()\n    pr = resp.json()\n\n    # Request reviewers if specified\n    if reviewers:\n        httpx.post(\n            f\"https://api.github.com/repos/{repo}/pulls/{pr['number']}/requested_reviewers\",\n            headers=headers,\n            json={\"reviewers\": reviewers},\n            timeout=10,\n        )\n\n    return {\n        \"number\": pr[\"number\"],\n        \"html_url\": pr[\"html_url\"],\n        \"state\": pr[\"state\"],\n    }\n```\n\n---\n\n### `github_workflow_dispatch`\n\n**Description:** Triggers a GitHub Actions workflow via the repository dispatch or workflow_dispatch API, passing custom inputs and waiting optionally for completion status.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo: str`, `workflow_id: str` (filename or ID), `ref: str = \"main\"`, `inputs: dict = {}`, `wait: bool = False`, `timeout: int = 300` |\n| **Output** | `dict` with `triggered: bool`, `run_id: int`, `status: str`, `conclusion: str` |\n| **Key Dependencies** | `httpx`, `time` (stdlib) |\n| **MCP Alternative** | `@anthropic/github-mcp` |\n\n```python\nimport os, httpx, time\n\ndef main(repo: str, workflow_id: str, ref: str = \"main\",\n         inputs: dict | None = None, wait: bool = False,\n         timeout: int = 300) -> dict:\n    \"\"\"Trigger a GitHub Actions workflow and optionally wait for completion.\"\"\"\n    headers = {\n        \"Authorization\": f\"token {os.environ['GITHUB_TOKEN']}\",\n        \"Accept\": \"application/vnd.github.v3+json\",\n    }\n    # Trigger\n    resp = httpx.post(\n        f\"https://api.github.com/repos/{repo}/actions/workflows/{workflow_id}/dispatches\",\n        headers=headers,\n        json={\"ref\": ref, \"inputs\": inputs or {}},\n        timeout=15,\n    )\n    resp.raise_for_status()\n\n    if not wait:\n        return {\"triggered\": True, \"run_id\": 0, \"status\": \"queued\", \"conclusion\": \"\"}\n\n    # Poll for the new run\n    time.sleep(3)\n    deadline = time.time() + timeout\n    while time.time() < deadline:\n        runs_resp = httpx.get(\n            f\"https://api.github.com/repos/{repo}/actions/workflows/{workflow_id}/runs\",\n            headers=headers, params={\"branch\": ref, \"per_page\": 1},\n            timeout=15,\n        )\n        runs = runs_resp.json().get(\"workflow_runs\", [])\n        if runs:\n            run = runs[0]\n            if run[\"status\"] == \"completed\":\n                return {\n                    \"triggered\": True, \"run_id\": run[\"id\"],\n                    \"status\": run[\"status\"], \"conclusion\": run[\"conclusion\"],\n                }\n        time.sleep(10)\n\n    return {\"triggered\": True, \"run_id\": 0, \"status\": \"timeout\", \"conclusion\": \"\"}\n```\n\n---\n\n## Database Operations\n\n### `supabase_query`\n\n**Description:** Executes queries against a Supabase project using the PostgREST API -- supports select, insert, update, upsert, and delete with filter chaining.\n\n| Field | Detail |\n|---|---|\n| **Input** | `table: str`, `action: Literal[\"select\", \"insert\", \"update\", \"upsert\", \"delete\"]`, `data: dict | list[dict] = {}`, `filters: list[dict] = []`, `select_columns: str = \"*\"`, `limit: int = 100` |\n| **Output** | `dict` with `data: list[dict]`, `count: int`, `status: int` |\n| **Key Dependencies** | `supabase` (supabase-py) |\n| **MCP Alternative** | `@anthropic/supabase-mcp` server |\n\n```python\nimport os\nfrom supabase import create_client\n\ndef main(table: str, action: str = \"select\", data=None,\n         filters: list[dict] | None = None,\n         select_columns: str = \"*\", limit: int = 100) -> dict:\n    \"\"\"Execute a Supabase database operation.\"\"\"\n    client = create_client(\n        os.environ[\"SUPABASE_URL\"],\n        os.environ[\"SUPABASE_SERVICE_KEY\"],\n    )\n\n    query = client.table(table)\n\n    if action == \"select\":\n        query = query.select(select_columns)\n        for f in (filters or []):\n            query = getattr(query, f.get(\"op\", \"eq\"))(f[\"field\"], f[\"value\"])\n        result = query.limit(limit).execute()\n        return {\"data\": result.data, \"count\": len(result.data), \"status\": 200}\n\n    if action == \"insert\":\n        result = query.insert(data).execute()\n        return {\"data\": result.data, \"count\": len(result.data), \"status\": 201}\n\n    if action == \"upsert\":\n        result = query.upsert(data).execute()\n        return {\"data\": result.data, \"count\": len(result.data), \"status\": 200}\n\n    if action == \"update\":\n        q = query.update(data)\n        for f in (filters or []):\n            q = getattr(q, f.get(\"op\", \"eq\"))(f[\"field\"], f[\"value\"])\n        result = q.execute()\n        return {\"data\": result.data, \"count\": len(result.data), \"status\": 200}\n\n    if action == \"delete\":\n        q = query.delete()\n        for f in (filters or []):\n            q = getattr(q, f.get(\"op\", \"eq\"))(f[\"field\"], f[\"value\"])\n        result = q.execute()\n        return {\"data\": result.data, \"count\": len(result.data), \"status\": 200}\n\n    raise ValueError(f\"Unknown action: {action}\")\n```\n\n---\n\n### `sqlite_query`\n\n**Description:** Opens (or creates) a local SQLite database and executes arbitrary SQL, returning rows as dictionaries -- ideal for lightweight local state, caching, and intermediate pipeline storage.\n\n| Field | Detail |\n|---|---|\n| **Input** | `db_path: str`, `sql: str`, `params: list = []`, `mode: Literal[\"query\", \"execute\", \"script\"] = \"query\"` |\n| **Output** | `dict` with `rows: list[dict]`, `rowcount: int`, `lastrowid: int` |\n| **Key Dependencies** | `sqlite3` (stdlib) |\n| **MCP Alternative** | `@anthropic/sqlite-mcp` server |\n\n```python\nimport sqlite3\n\ndef main(db_path: str, sql: str, params: list | None = None,\n         mode: str = \"query\") -> dict:\n    \"\"\"Execute SQL against a SQLite database.\"\"\"\n    conn = sqlite3.connect(db_path)\n    conn.row_factory = sqlite3.Row\n    cur = conn.cursor()\n\n    try:\n        if mode == \"script\":\n            cur.executescript(sql)\n            conn.commit()\n            return {\"rows\": [], \"rowcount\": cur.rowcount, \"lastrowid\": cur.lastrowid}\n\n        if mode == \"execute\":\n            cur.execute(sql, params or [])\n            conn.commit()\n            return {\"rows\": [], \"rowcount\": cur.rowcount, \"lastrowid\": cur.lastrowid}\n\n        # query mode\n        cur.execute(sql, params or [])\n        rows = [dict(row) for row in cur.fetchall()]\n        return {\"rows\": rows, \"rowcount\": len(rows), \"lastrowid\": 0}\n\n    finally:\n        conn.close()\n```\n\n---\n\n### `airtable_ops`\n\n**Description:** Reads, creates, updates, or deletes records in an Airtable base, handling pagination and field-type coercion automatically.\n\n| Field | Detail |\n|---|---|\n| **Input** | `base_id: str`, `table_name: str`, `action: Literal[\"list\", \"create\", \"update\", \"delete\"]`, `records: list[dict] = []`, `filter_formula: str = \"\"`, `max_records: int = 100` |\n| **Output** | `dict` with `records: list[dict]`, `count: int` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | None standard; wrap as custom MCP tool |\n\n```python\nimport os, httpx\n\nBASE_URL = \"https://api.airtable.com/v0\"\n\ndef _headers():\n    return {\n        \"Authorization\": f\"Bearer {os.environ['AIRTABLE_API_KEY']}\",\n        \"Content-Type\": \"application/json\",\n    }\n\ndef main(base_id: str, table_name: str, action: str = \"list\",\n         records: list[dict] | None = None,\n         filter_formula: str = \"\", max_records: int = 100) -> dict:\n    \"\"\"Perform CRUD operations on Airtable records.\"\"\"\n    url = f\"{BASE_URL}/{base_id}/{table_name}\"\n\n    if action == \"list\":\n        params: dict = {\"maxRecords\": max_records}\n        if filter_formula:\n            params[\"filterByFormula\"] = filter_formula\n        all_records, offset = [], None\n        while True:\n            if offset:\n                params[\"offset\"] = offset\n            resp = httpx.get(url, headers=_headers(), params=params, timeout=15)\n            resp.raise_for_status()\n            data = resp.json()\n            all_records.extend(data.get(\"records\", []))\n            offset = data.get(\"offset\")\n            if not offset or len(all_records) >= max_records:\n                break\n        return {\"records\": all_records, \"count\": len(all_records)}\n\n    if action == \"create\":\n        payload = {\"records\": [{\"fields\": r} for r in (records or [])]}\n        resp = httpx.post(url, headers=_headers(), json=payload, timeout=15)\n        resp.raise_for_status()\n        created = resp.json().get(\"records\", [])\n        return {\"records\": created, \"count\": len(created)}\n\n    if action == \"update\":\n        payload = {\"records\": records or []}  # expects [{\"id\": ..., \"fields\": {...}}]\n        resp = httpx.patch(url, headers=_headers(), json=payload, timeout=15)\n        resp.raise_for_status()\n        updated = resp.json().get(\"records\", [])\n        return {\"records\": updated, \"count\": len(updated)}\n\n    if action == \"delete\":\n        ids = [r if isinstance(r, str) else r.get(\"id\") for r in (records or [])]\n        resp = httpx.delete(url, headers=_headers(),\n                            params={\"records[]\": ids}, timeout=15)\n        resp.raise_for_status()\n        return {\"records\": resp.json().get(\"records\", []),\n\n[Showing lines 1-1444 of 2869 (50.0KB limit). Use offset=1445 to continue.]"}],"details":{"truncation":{"content":"# WAT Systems Factory -- Reusable Tool Pattern Catalog\n\n> **Version:** 0.1.0-seed\n> **Maintainer:** WAT Systems Factory / Agent HQ\n> **Purpose:** Canonical reference of composable tool patterns that factory-generated workflows can assemble into pipelines. Each pattern is self-contained, testable, and convertible to an MCP tool or n8n node.\n\n---\n\n## Table of Contents\n\n1. [Web Scraping & Search](#web-scraping--search)\n2. [File I/O](#file-io)\n3. [API Calls](#api-calls)\n4. [Data Processing](#data-processing)\n5. [AI Processing](#ai-processing)\n6. [Notifications](#notifications)\n7. [Git Operations](#git-operations)\n8. [GitHub API](#github-api)\n9. [Database Operations](#database-operations)\n10. [Media Processing](#media-processing)\n11. [n8n Conversion](#n8n-conversion)\n12. [Agent Teams Coordination](#agent-teams-coordination)\n13. [GitHub Agent HQ Interaction](#github-agent-hq-interaction)\n\n---\n\n## Web Scraping & Search\n\n### `firecrawl_scrape`\n\n**Description:** Scrapes a single URL or crawls an entire domain via the Firecrawl API, returning clean Markdown-formatted content suitable for downstream LLM consumption.\n\n| Field | Detail |\n|---|---|\n| **Input** | `url: str`, `mode: Literal[\"scrape\", \"crawl\"]`, `max_pages: int = 50`, `selectors: list[str] = []` |\n| **Output** | `dict` with keys `markdown: str`, `metadata: dict`, `links: list[str]` |\n| **Key Dependencies** | `firecrawl-py`, `httpx` |\n| **MCP Alternative** | `@anthropic/firecrawl-mcp` server -- exposes `firecrawl_scrape` and `firecrawl_crawl` tools directly |\n\n```python\nimport os\nfrom firecrawl import FirecrawlApp\n\ndef main(url: str, mode: str = \"scrape\", max_pages: int = 50,\n         selectors: list[str] | None = None) -> dict:\n    \"\"\"Scrape or crawl a URL via Firecrawl and return Markdown content.\"\"\"\n    app = FirecrawlApp(api_key=os.environ[\"FIRECRAWL_API_KEY\"])\n\n    if mode == \"crawl\":\n        result = app.crawl_url(url, params={\n            \"limit\": max_pages,\n            \"scrapeOptions\": {\"formats\": [\"markdown\"]},\n        })\n        pages = [{\"markdown\": p.get(\"markdown\", \"\"), \"url\": p.get(\"url\")}\n                 for p in result.get(\"data\", [])]\n        return {\"pages\": pages, \"count\": len(pages)}\n\n    # Default: single-page scrape\n    result = app.scrape_url(url, params={\"formats\": [\"markdown\"]})\n    return {\n        \"markdown\": result.get(\"markdown\", \"\"),\n        \"metadata\": result.get(\"metadata\", {}),\n        \"links\": result.get(\"links\", []),\n    }\n```\n\n---\n\n### `brave_search`\n\n**Description:** Runs a web search query through the Brave Search API and returns structured result objects with titles, URLs, and snippets.\n\n| Field | Detail |\n|---|---|\n| **Input** | `query: str`, `count: int = 10`, `freshness: str = \"\"` |\n| **Output** | `list[dict]` each with `title`, `url`, `description`, `age` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | `@anthropic/brave-search-mcp` server -- exposes `brave_web_search` and `brave_local_search` |\n\n```python\nimport os, httpx\n\nBRAVE_ENDPOINT = \"https://api.search.brave.com/res/v1/web/search\"\n\ndef main(query: str, count: int = 10, freshness: str = \"\") -> list[dict]:\n    \"\"\"Execute a Brave web search and return structured results.\"\"\"\n    headers = {\n        \"Accept\": \"application/json\",\n        \"X-Subscription-Token\": os.environ[\"BRAVE_API_KEY\"],\n    }\n    params = {\"q\": query, \"count\": min(count, 20)}\n    if freshness:\n        params[\"freshness\"] = freshness  # e.g. \"pd\" (past day), \"pw\", \"pm\"\n\n    resp = httpx.get(BRAVE_ENDPOINT, headers=headers, params=params, timeout=15)\n    resp.raise_for_status()\n\n    results = []\n    for item in resp.json().get(\"web\", {}).get(\"results\", []):\n        results.append({\n            \"title\": item.get(\"title\"),\n            \"url\": item.get(\"url\"),\n            \"description\": item.get(\"description\"),\n            \"age\": item.get(\"age\", \"\"),\n        })\n    return results\n```\n\n---\n\n### `puppeteer_screenshot`\n\n**Description:** Launches a headless browser via Puppeteer (through its MCP server or a local Node bridge) to capture a screenshot or extract JavaScript-rendered DOM content from a page.\n\n| Field | Detail |\n|---|---|\n| **Input** | `url: str`, `action: Literal[\"screenshot\", \"extract\"]`, `selector: str = \"body\"`, `viewport: dict = {\"width\": 1280, \"height\": 720}` |\n| **Output** | `dict` with `base64_image: str` (for screenshot) or `html: str` (for extract) |\n| **Key Dependencies** | `playwright` (Python) or Node `puppeteer` via subprocess |\n| **MCP Alternative** | `@anthropic/puppeteer-mcp` server -- exposes `puppeteer_navigate`, `puppeteer_screenshot`, `puppeteer_click`, `puppeteer_fill` |\n\n```python\nfrom playwright.async_api import async_playwright\nimport asyncio, base64\n\nasync def _run(url: str, action: str, selector: str, viewport: dict) -> dict:\n    async with async_playwright() as pw:\n        browser = await pw.chromium.launch(headless=True)\n        page = await browser.new_page(viewport_size=viewport)\n        await page.goto(url, wait_until=\"networkidle\")\n\n        if action == \"screenshot\":\n            buf = await page.screenshot(full_page=True)\n            await browser.close()\n            return {\"base64_image\": base64.b64encode(buf).decode()}\n\n        content = await page.inner_html(selector)\n        await browser.close()\n        return {\"html\": content}\n\ndef main(url: str, action: str = \"screenshot\", selector: str = \"body\",\n         viewport: dict | None = None) -> dict:\n    \"\"\"Capture a screenshot or extract rendered HTML from a URL.\"\"\"\n    vp = viewport or {\"width\": 1280, \"height\": 720}\n    return asyncio.run(_run(url, action, selector, vp))\n```\n\n---\n\n## File I/O\n\n### `csv_read_write`\n\n**Description:** Reads a CSV file into a list of dictionaries or writes a list of dictionaries out to CSV, handling encoding detection and BOM stripping automatically.\n\n| Field | Detail |\n|---|---|\n| **Input** | `path: str`, `mode: Literal[\"read\", \"write\"]`, `data: list[dict] = []`, `encoding: str = \"utf-8-sig\"` |\n| **Output** | `list[dict]` on read; `dict` with `rows_written: int, path: str` on write |\n| **Key Dependencies** | `csv` (stdlib), `chardet` (optional encoding detection) |\n| **MCP Alternative** | `@anthropic/filesystem-mcp` for raw read/write; CSV parsing done in-tool |\n\n```python\nimport csv, io, pathlib\n\ndef main(path: str, mode: str = \"read\", data: list[dict] | None = None,\n         encoding: str = \"utf-8-sig\") -> dict | list[dict]:\n    \"\"\"Read or write CSV files with automatic encoding handling.\"\"\"\n    p = pathlib.Path(path)\n\n    if mode == \"read\":\n        text = p.read_text(encoding=encoding)\n        reader = csv.DictReader(io.StringIO(text))\n        return [dict(row) for row in reader]\n\n    # Write mode\n    if not data:\n        raise ValueError(\"data is required for write mode\")\n    fieldnames = list(data[0].keys())\n    with p.open(\"w\", newline=\"\", encoding=encoding) as f:\n        writer = csv.DictWriter(f, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(data)\n    return {\"rows_written\": len(data), \"path\": str(p.resolve())}\n```\n\n---\n\n### `json_read_write`\n\n**Description:** Reads a JSON or JSONL file into Python objects or writes Python objects out to JSON/JSONL, with optional pretty-printing and schema validation.\n\n| Field | Detail |\n|---|---|\n| **Input** | `path: str`, `mode: Literal[\"read\", \"write\"]`, `data: Any = None`, `jsonl: bool = False`, `indent: int = 2` |\n| **Output** | Parsed object(s) on read; `dict` with `path: str, size_bytes: int` on write |\n| **Key Dependencies** | `json` (stdlib), `jsonschema` (optional validation) |\n| **MCP Alternative** | `@anthropic/filesystem-mcp` for raw I/O |\n\n```python\nimport json, pathlib\n\ndef main(path: str, mode: str = \"read\", data=None,\n         jsonl: bool = False, indent: int = 2):\n    \"\"\"Read or write JSON / JSONL files.\"\"\"\n    p = pathlib.Path(path)\n\n    if mode == \"read\":\n        text = p.read_text(encoding=\"utf-8\")\n        if jsonl:\n            return [json.loads(line) for line in text.splitlines() if line.strip()]\n        return json.loads(text)\n\n    # Write mode\n    if jsonl:\n        lines = [json.dumps(item, ensure_ascii=False) for item in data]\n        p.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n    else:\n        p.write_text(json.dumps(data, indent=indent, ensure_ascii=False),\n                      encoding=\"utf-8\")\n    return {\"path\": str(p.resolve()), \"size_bytes\": p.stat().st_size}\n```\n\n---\n\n### `apollo_enrich`\n\n**Description:** Enriches a company name/domain with company data and decision-maker contacts via the Apollo.io API. Returns structured company info, tech stack, and key contacts.\n\n| Field | Detail |\n|---|---|\n| **Input** | `company_name: str`, `domain: str = None` |\n| **Output** | `dict` with company data, tech stack, decision makers |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | None; direct HTTP API |\n\n---\n\n### `hunter_email_search`\n\n**Description:** Searches for email addresses at a given domain via the Hunter.io API. Returns verified contacts with confidence scores.\n\n| Field | Detail |\n|---|---|\n| **Input** | `domain: str`, `limit: int = 5` |\n| **Output** | `dict` with emails, patterns, organization info |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | None; direct HTTP API |\n\n---\n\n### `multi_dimension_scorer`\n\n**Description:** Scores a data record across N independent dimensions with configurable point values. Used for lead scoring, content quality assessment, or any multi-criteria evaluation.\n\n| Field | Detail |\n|---|---|\n| **Input** | `record: dict`, `dimensions: list[dict]` (each with name, max_points, scoring_function) |\n| **Output** | `dict` with dimension breakdown and total score |\n| **Key Dependencies** | None (stdlib only) |\n| **MCP Alternative** | None |\n\n---\n\n### `email_sequence_generator`\n\n**Description:** Generates a multi-email sequence (outreach or nurture) using Claude API. Takes company/audience context and a sequence template, returns structured JSON with subject lines, bodies, and send timing.\n\n| Field | Detail |\n|---|---|\n| **Input** | `context: dict`, `sequence_template: str`, `email_count: int` |\n| **Output** | `dict` with sequence of emails (subject, body, send_day) |\n| **Key Dependencies** | `anthropic` |\n| **MCP Alternative** | Anthropic MCP |\n\n---\n\n### `pdf_extract`\n\n**Description:** Extracts text content, metadata, and optionally tables from a PDF file, returning structured output ready for LLM ingestion or data-pipeline processing.\n\n| Field | Detail |\n|---|---|\n| **Input** | `path: str`, `pages: str = \"all\"`, `extract_tables: bool = False` |\n| **Output** | `dict` with `text: str`, `pages: list[dict]`, `metadata: dict`, `tables: list` |\n| **Key Dependencies** | `pymupdf` (fitz), `pdfplumber` (for tables) |\n| **MCP Alternative** | None standard; wrap as custom MCP tool |\n\n```python\nimport fitz  # pymupdf\n\ndef main(path: str, pages: str = \"all\", extract_tables: bool = False) -> dict:\n    \"\"\"Extract text and metadata from a PDF file.\"\"\"\n    doc = fitz.open(path)\n    metadata = dict(doc.metadata)\n\n    page_range = range(len(doc))\n    if pages != \"all\":\n        # Parse ranges like \"1-5\" or \"3,7,10-12\"\n        page_range = _parse_page_range(pages, len(doc))\n\n    extracted_pages = []\n    for i in page_range:\n        page = doc[i]\n        extracted_pages.append({\n            \"page_number\": i + 1,\n            \"text\": page.get_text(\"text\"),\n        })\n\n    result = {\n        \"text\": \"\\n\\n\".join(p[\"text\"] for p in extracted_pages),\n        \"pages\": extracted_pages,\n        \"metadata\": metadata,\n    }\n\n    if extract_tables:\n        import pdfplumber\n        with pdfplumber.open(path) as pdf:\n            result[\"tables\"] = []\n            for i in page_range:\n                for table in pdf.pages[i].extract_tables():\n                    result[\"tables\"].append({\"page\": i + 1, \"rows\": table})\n\n    return result\n\ndef _parse_page_range(spec: str, total: int) -> list[int]:\n    indices = []\n    for part in spec.split(\",\"):\n        if \"-\" in part:\n            start, end = part.split(\"-\", 1)\n            indices.extend(range(int(start) - 1, min(int(end), total)))\n        else:\n            indices.append(int(part) - 1)\n    return [i for i in indices if 0 <= i < total]\n```\n\n---\n\n## API Calls\n\n### `rest_client`\n\n**Description:** A generic REST API client with configurable retry logic, exponential backoff, header injection, and response normalization -- the universal building block for any HTTP-based integration.\n\n| Field | Detail |\n|---|---|\n| **Input** | `url: str`, `method: str = \"GET\"`, `headers: dict = {}`, `body: Any = None`, `retries: int = 3`, `backoff: float = 1.0`, `timeout: int = 30` |\n| **Output** | `dict` with `status: int`, `headers: dict`, `body: Any`, `elapsed_ms: float` |\n| **Key Dependencies** | `httpx`, `tenacity` |\n| **MCP Alternative** | `@anthropic/fetch-mcp` for simple GET/POST |\n\n```python\nimport httpx, time\nfrom tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception\n\ndef _is_retryable(exc):\n    if isinstance(exc, httpx.HTTPStatusError):\n        return exc.response.status_code in (429, 500, 502, 503, 504)\n    return isinstance(exc, (httpx.ConnectTimeout, httpx.ReadTimeout))\n\ndef main(url: str, method: str = \"GET\", headers: dict | None = None,\n         body=None, retries: int = 3, backoff: float = 1.0,\n         timeout: int = 30) -> dict:\n    \"\"\"Make an HTTP request with retry logic and exponential backoff.\"\"\"\n\n    @retry(\n        stop=stop_after_attempt(retries),\n        wait=wait_exponential(multiplier=backoff, min=1, max=30),\n        retry=retry_if_exception(_is_retryable),\n        reraise=True,\n    )\n    def _do_request():\n        t0 = time.monotonic()\n        resp = httpx.request(\n            method=method.upper(),\n            url=url,\n            headers=headers or {},\n            json=body if isinstance(body, (dict, list)) else None,\n            content=body if isinstance(body, (str, bytes)) else None,\n            timeout=timeout,\n        )\n        elapsed = (time.monotonic() - t0) * 1000\n        resp.raise_for_status()\n        try:\n            resp_body = resp.json()\n        except Exception:\n            resp_body = resp.text\n        return {\n            \"status\": resp.status_code,\n            \"headers\": dict(resp.headers),\n            \"body\": resp_body,\n            \"elapsed_ms\": round(elapsed, 2),\n        }\n\n    return _do_request()\n```\n\n---\n\n### `graphql_client`\n\n**Description:** Sends GraphQL queries and mutations to any endpoint, with variable substitution, error extraction, and the same retry semantics as `rest_client`.\n\n| Field | Detail |\n|---|---|\n| **Input** | `endpoint: str`, `query: str`, `variables: dict = {}`, `headers: dict = {}`, `retries: int = 3` |\n| **Output** | `dict` with `data: Any`, `errors: list | None`, `status: int` |\n| **Key Dependencies** | `httpx`, `tenacity` |\n| **MCP Alternative** | None standard; wrap as custom MCP tool |\n\n```python\nimport httpx\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\ndef main(endpoint: str, query: str, variables: dict | None = None,\n         headers: dict | None = None, retries: int = 3) -> dict:\n    \"\"\"Execute a GraphQL query or mutation with retry logic.\"\"\"\n\n    @retry(stop=stop_after_attempt(retries),\n           wait=wait_exponential(multiplier=1, min=1, max=15))\n    def _execute():\n        payload = {\"query\": query}\n        if variables:\n            payload[\"variables\"] = variables\n        resp = httpx.post(\n            endpoint,\n            json=payload,\n            headers={\"Content-Type\": \"application/json\", **(headers or {})},\n            timeout=30,\n        )\n        resp.raise_for_status()\n        result = resp.json()\n        return {\n            \"data\": result.get(\"data\"),\n            \"errors\": result.get(\"errors\"),\n            \"status\": resp.status_code,\n        }\n\n    return _execute()\n```\n\n---\n\n### `oauth_token_refresh`\n\n**Description:** Manages OAuth 2.0 token lifecycle -- obtains tokens via client_credentials or authorization_code grants, caches them, and transparently refreshes before expiry.\n\n| Field | Detail |\n|---|---|\n| **Input** | `token_url: str`, `client_id: str`, `client_secret: str`, `grant_type: str = \"client_credentials\"`, `refresh_token: str = \"\"`, `scopes: list[str] = []` |\n| **Output** | `dict` with `access_token: str`, `expires_in: int`, `token_type: str` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | None; typically embedded within other tools |\n\n```python\nimport httpx, time\n\n_token_cache: dict = {}\n\ndef main(token_url: str, client_id: str, client_secret: str,\n         grant_type: str = \"client_credentials\",\n         refresh_token: str = \"\", scopes: list[str] | None = None) -> dict:\n    \"\"\"Obtain or refresh an OAuth 2.0 access token.\"\"\"\n    cache_key = f\"{token_url}:{client_id}\"\n\n    # Return cached token if still valid\n    cached = _token_cache.get(cache_key)\n    if cached and cached[\"expires_at\"] > time.time() + 60:\n        return {\n            \"access_token\": cached[\"access_token\"],\n            \"expires_in\": int(cached[\"expires_at\"] - time.time()),\n            \"token_type\": cached[\"token_type\"],\n        }\n\n    data = {\"grant_type\": grant_type, \"client_id\": client_id,\n            \"client_secret\": client_secret}\n    if refresh_token:\n        data[\"refresh_token\"] = refresh_token\n    if scopes:\n        data[\"scope\"] = \" \".join(scopes)\n\n    resp = httpx.post(token_url, data=data, timeout=15)\n    resp.raise_for_status()\n    token_data = resp.json()\n\n    _token_cache[cache_key] = {\n        \"access_token\": token_data[\"access_token\"],\n        \"token_type\": token_data.get(\"token_type\", \"Bearer\"),\n        \"expires_at\": time.time() + token_data.get(\"expires_in\", 3600),\n    }\n    return {\n        \"access_token\": token_data[\"access_token\"],\n        \"expires_in\": token_data.get(\"expires_in\", 3600),\n        \"token_type\": token_data.get(\"token_type\", \"Bearer\"),\n    }\n```\n\n---\n\n## Data Processing\n\n### `filter_sort`\n\n**Description:** Applies filter predicates and multi-key sorting to a list of dictionaries, supporting comparison operators, regex matching, and null handling.\n\n| Field | Detail |\n|---|---|\n| **Input** | `data: list[dict]`, `filters: list[dict]` (each with `field`, `op`, `value`), `sort_by: list[dict]` (each with `field`, `order`) |\n| **Output** | `list[dict]` -- filtered and sorted records |\n| **Key Dependencies** | `re` (stdlib) |\n| **MCP Alternative** | None; pure logic tool |\n\n```python\nimport re, operator\n\nOPS = {\n    \"eq\": operator.eq, \"ne\": operator.ne,\n    \"gt\": operator.gt, \"ge\": operator.ge,\n    \"lt\": operator.lt, \"le\": operator.le,\n    \"contains\": lambda a, b: b in str(a),\n    \"regex\": lambda a, b: bool(re.search(b, str(a))),\n    \"in\": lambda a, b: a in b,\n}\n\ndef main(data: list[dict], filters: list[dict] | None = None,\n         sort_by: list[dict] | None = None) -> list[dict]:\n    \"\"\"Filter and sort a list of dictionaries.\"\"\"\n    result = data\n\n    # Apply filters\n    for f in (filters or []):\n        op_fn = OPS.get(f[\"op\"], operator.eq)\n        result = [row for row in result if op_fn(row.get(f[\"field\"]), f[\"value\"])]\n\n    # Apply sorting\n    if sort_by:\n        for s in reversed(sort_by):\n            reverse = s.get(\"order\", \"asc\") == \"desc\"\n            result.sort(key=lambda row: (row.get(s[\"field\"]) is None,\n                                          row.get(s[\"field\"], \"\")),\n                        reverse=reverse)\n\n    return result\n```\n\n---\n\n### `dedup`\n\n**Description:** Removes duplicate records from a dataset based on one or more key fields, with configurable strategies for which duplicate to keep (first, last, or merge).\n\n| Field | Detail |\n|---|---|\n| **Input** | `data: list[dict]`, `keys: list[str]`, `strategy: Literal[\"first\", \"last\", \"merge\"] = \"first\"` |\n| **Output** | `dict` with `unique: list[dict]`, `duplicates_removed: int` |\n| **Key Dependencies** | None (stdlib only) |\n| **MCP Alternative** | None; pure logic tool |\n\n```python\ndef main(data: list[dict], keys: list[str],\n         strategy: str = \"first\") -> dict:\n    \"\"\"Deduplicate records by key fields.\"\"\"\n    seen: dict[tuple, dict] = {}\n    order: list[tuple] = []\n\n    for row in data:\n        key = tuple(row.get(k) for k in keys)\n        if key not in seen:\n            seen[key] = row\n            order.append(key)\n        elif strategy == \"last\":\n            seen[key] = row\n        elif strategy == \"merge\":\n            # Merge: later values fill in blanks only\n            for k, v in row.items():\n                if seen[key].get(k) in (None, \"\"):\n                    seen[key][k] = v\n\n    unique = [seen[k] for k in order]\n    return {\n        \"unique\": unique,\n        \"duplicates_removed\": len(data) - len(unique),\n    }\n```\n\n---\n\n### `transform_map`\n\n**Description:** Applies field-level transformations to every record in a dataset -- renaming columns, computing derived fields, type-casting, and applying template expressions.\n\n| Field | Detail |\n|---|---|\n| **Input** | `data: list[dict]`, `mappings: list[dict]` (each with `source`, `target`, `transform`), `drop_unmapped: bool = False` |\n| **Output** | `list[dict]` -- transformed records |\n| **Key Dependencies** | None (stdlib only) |\n| **MCP Alternative** | None; pure logic tool |\n\n```python\nfrom datetime import datetime\n\nTRANSFORMS = {\n    \"upper\": lambda v: str(v).upper(),\n    \"lower\": lambda v: str(v).lower(),\n    \"strip\": lambda v: str(v).strip(),\n    \"int\": lambda v: int(v) if v not in (None, \"\") else None,\n    \"float\": lambda v: float(v) if v not in (None, \"\") else None,\n    \"isodate\": lambda v: datetime.fromisoformat(str(v)).isoformat(),\n    \"bool\": lambda v: str(v).lower() in (\"true\", \"1\", \"yes\"),\n    \"default\": lambda v: v,\n}\n\ndef main(data: list[dict], mappings: list[dict],\n         drop_unmapped: bool = False) -> list[dict]:\n    \"\"\"Apply field-level transformations to each record.\"\"\"\n    results = []\n    for row in data:\n        new_row = {} if drop_unmapped else dict(row)\n        for m in mappings:\n            source_val = row.get(m[\"source\"])\n            transform_fn = TRANSFORMS.get(m.get(\"transform\", \"default\"),\n                                           TRANSFORMS[\"default\"])\n            new_row[m[\"target\"]] = transform_fn(source_val)\n        results.append(new_row)\n    return results\n```\n\n---\n\n## AI Processing\n\n### `llm_prompt`\n\n**Description:** Sends a prompt to an LLM provider (OpenAI, Anthropic, or compatible endpoint) with system instructions, returning the raw text completion and token usage metadata.\n\n| Field | Detail |\n|---|---|\n| **Input** | `prompt: str`, `system: str = \"\"`, `provider: Literal[\"openai\", \"anthropic\"] = \"anthropic\"`, `model: str = \"\"`, `max_tokens: int = 4096`, `temperature: float = 0.3` |\n| **Output** | `dict` with `text: str`, `model: str`, `usage: dict` |\n| **Key Dependencies** | `anthropic`, `openai` |\n| **MCP Alternative** | None; this IS the core LLM call |\n\n```python\nimport os\n\ndef main(prompt: str, system: str = \"\", provider: str = \"anthropic\",\n         model: str = \"\", max_tokens: int = 4096,\n         temperature: float = 0.3) -> dict:\n    \"\"\"Send a prompt to an LLM and return the completion.\"\"\"\n\n    if provider == \"anthropic\":\n        import anthropic\n        client = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n        resolved_model = model or \"claude-sonnet-4-20250514\"\n        msg = client.messages.create(\n            model=resolved_model,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            system=system or \"You are a helpful assistant.\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n        )\n        return {\n            \"text\": msg.content[0].text,\n            \"model\": resolved_model,\n            \"usage\": {\"input_tokens\": msg.usage.input_tokens,\n                      \"output_tokens\": msg.usage.output_tokens},\n        }\n\n    # OpenAI / compatible\n    import openai\n    client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n    resolved_model = model or \"gpt-4o\"\n    messages = []\n    if system:\n        messages.append({\"role\": \"system\", \"content\": system})\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n    resp = client.chat.completions.create(\n        model=resolved_model, messages=messages,\n        max_tokens=max_tokens, temperature=temperature,\n    )\n    choice = resp.choices[0]\n    return {\n        \"text\": choice.message.content,\n        \"model\": resolved_model,\n        \"usage\": {\"input_tokens\": resp.usage.prompt_tokens,\n                  \"output_tokens\": resp.usage.completion_tokens},\n    }\n```\n\n---\n\n### `structured_extract`\n\n**Description:** Sends content to an LLM with a JSON schema instruction, parses the response into a validated Python object, and retries with corrective prompts if the response fails schema validation.\n\n| Field | Detail |\n|---|---|\n| **Input** | `content: str`, `schema: dict` (JSON Schema), `instructions: str = \"\"`, `retries: int = 2` |\n| **Output** | `dict` with `data: Any` (validated), `raw: str`, `attempts: int` |\n| **Key Dependencies** | `anthropic` or `openai`, `jsonschema` |\n| **MCP Alternative** | None; core LLM extraction pattern |\n\n```python\nimport json, os\nfrom jsonschema import validate, ValidationError\n\ndef main(content: str, schema: dict, instructions: str = \"\",\n         retries: int = 2) -> dict:\n    \"\"\"Extract structured data from content using an LLM with schema validation.\"\"\"\n    import anthropic\n    client = anthropic.Anthropic(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n\n    system_prompt = (\n        \"Extract information from the provided content and return ONLY valid JSON \"\n        \"matching the schema below. No markdown fences, no commentary.\\n\\n\"\n        f\"Schema:\\n```json\\n{json.dumps(schema, indent=2)}\\n```\"\n    )\n    if instructions:\n        system_prompt += f\"\\n\\nAdditional instructions: {instructions}\"\n\n    user_content = content\n    for attempt in range(1, retries + 2):\n        msg = client.messages.create(\n            model=\"claude-sonnet-4-20250514\",\n            max_tokens=4096,\n            temperature=0.0,\n            system=system_prompt,\n            messages=[{\"role\": \"user\", \"content\": user_content}],\n        )\n        raw = msg.content[0].text.strip()\n\n        # Strip markdown fences if present\n        if raw.startswith(\"```\"):\n            raw = raw.split(\"\\n\", 1)[1].rsplit(\"```\", 1)[0].strip()\n\n        try:\n            parsed = json.loads(raw)\n            validate(instance=parsed, schema=schema)\n            return {\"data\": parsed, \"raw\": raw, \"attempts\": attempt}\n        except (json.JSONDecodeError, ValidationError) as exc:\n            if attempt > retries:\n                raise ValueError(f\"Failed after {attempt} attempts: {exc}\") from exc\n            user_content = (\n                f\"Your previous response was invalid: {exc}\\n\\n\"\n                f\"Original content:\\n{content}\\n\\n\"\n                \"Please try again, returning ONLY valid JSON.\"\n            )\n\n    raise RuntimeError(\"Unreachable\")\n```\n\n---\n\n### `embedding_generate`\n\n**Description:** Generates vector embeddings for one or more text inputs using OpenAI, Voyage, or a local model, returning arrays ready for storage in a vector database.\n\n| Field | Detail |\n|---|---|\n| **Input** | `texts: list[str]`, `provider: Literal[\"openai\", \"voyage\"] = \"openai\"`, `model: str = \"\"` |\n| **Output** | `dict` with `embeddings: list[list[float]]`, `model: str`, `dimensions: int` |\n| **Key Dependencies** | `openai` or `voyageai` |\n| **MCP Alternative** | None standard |\n\n```python\nimport os\n\ndef main(texts: list[str], provider: str = \"openai\",\n         model: str = \"\") -> dict:\n    \"\"\"Generate vector embeddings for text inputs.\"\"\"\n\n    if provider == \"openai\":\n        import openai\n        client = openai.OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n        resolved = model or \"text-embedding-3-small\"\n        resp = client.embeddings.create(input=texts, model=resolved)\n        vecs = [item.embedding for item in resp.data]\n        return {\n            \"embeddings\": vecs,\n            \"model\": resolved,\n            \"dimensions\": len(vecs[0]) if vecs else 0,\n        }\n\n    if provider == \"voyage\":\n        import voyageai\n        client = voyageai.Client(api_key=os.environ[\"VOYAGE_API_KEY\"])\n        resolved = model or \"voyage-2\"\n        resp = client.embed(texts, model=resolved)\n        return {\n            \"embeddings\": resp.embeddings,\n            \"model\": resolved,\n            \"dimensions\": len(resp.embeddings[0]) if resp.embeddings else 0,\n        }\n\n    raise ValueError(f\"Unsupported provider: {provider}\")\n```\n\n---\n\n## Notifications\n\n### `slack_notify`\n\n**Description:** Sends a message to a Slack channel or thread via the Slack Web API or an incoming webhook, supporting Block Kit for rich formatting.\n\n| Field | Detail |\n|---|---|\n| **Input** | `channel: str`, `text: str`, `blocks: list[dict] = []`, `thread_ts: str = \"\"`, `use_webhook: bool = False` |\n| **Output** | `dict` with `ok: bool`, `ts: str`, `channel: str` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | `@anthropic/slack-mcp` server -- exposes `slack_post_message`, `slack_list_channels`, etc. |\n\n```python\nimport os, httpx\n\ndef main(channel: str, text: str, blocks: list[dict] | None = None,\n         thread_ts: str = \"\", use_webhook: bool = False) -> dict:\n    \"\"\"Send a Slack message to a channel or thread.\"\"\"\n\n    if use_webhook:\n        webhook_url = os.environ[\"SLACK_WEBHOOK_URL\"]\n        payload = {\"text\": text}\n        if blocks:\n            payload[\"blocks\"] = blocks\n        resp = httpx.post(webhook_url, json=payload, timeout=10)\n        resp.raise_for_status()\n        return {\"ok\": True, \"ts\": \"\", \"channel\": channel}\n\n    token = os.environ[\"SLACK_BOT_TOKEN\"]\n    payload = {\"channel\": channel, \"text\": text}\n    if blocks:\n        payload[\"blocks\"] = blocks\n    if thread_ts:\n        payload[\"thread_ts\"] = thread_ts\n\n    resp = httpx.post(\n        \"https://slack.com/api/chat.postMessage\",\n        json=payload,\n        headers={\"Authorization\": f\"Bearer {token}\",\n                 \"Content-Type\": \"application/json\"},\n        timeout=10,\n    )\n    data = resp.json()\n    return {\"ok\": data.get(\"ok\", False), \"ts\": data.get(\"ts\", \"\"),\n            \"channel\": data.get(\"channel\", channel)}\n```\n\n---\n\n### `discord_webhook`\n\n**Description:** Posts a message or embed to a Discord channel via a webhook URL, with support for rich embeds, username overrides, and file attachments.\n\n| Field | Detail |\n|---|---|\n| **Input** | `webhook_url: str`, `content: str = \"\"`, `embeds: list[dict] = []`, `username: str = \"\"` |\n| **Output** | `dict` with `status: int`, `success: bool` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | None standard; wrap as custom MCP tool |\n\n```python\nimport httpx\n\ndef main(webhook_url: str, content: str = \"\", embeds: list[dict] | None = None,\n         username: str = \"\") -> dict:\n    \"\"\"Send a message or embed to Discord via webhook.\"\"\"\n    payload: dict = {}\n    if content:\n        payload[\"content\"] = content\n    if embeds:\n        payload[\"embeds\"] = embeds\n    if username:\n        payload[\"username\"] = username\n\n    if not payload:\n        raise ValueError(\"Must provide content or embeds\")\n\n    resp = httpx.post(webhook_url, json=payload, timeout=10)\n    return {\n        \"status\": resp.status_code,\n        \"success\": resp.status_code in (200, 204),\n    }\n```\n\n---\n\n### `email_send`\n\n**Description:** Sends an email via SMTP or a transactional email API (SendGrid, Resend), supporting HTML body, plain-text fallback, and file attachments.\n\n| Field | Detail |\n|---|---|\n| **Input** | `to: list[str]`, `subject: str`, `body_html: str = \"\"`, `body_text: str = \"\"`, `provider: Literal[\"smtp\", \"sendgrid\", \"resend\"] = \"resend\"`, `from_addr: str = \"\"` |\n| **Output** | `dict` with `success: bool`, `message_id: str` |\n| **Key Dependencies** | `httpx`, `smtplib` (stdlib) |\n| **MCP Alternative** | None standard |\n\n```python\nimport os, httpx\n\ndef main(to: list[str], subject: str, body_html: str = \"\",\n         body_text: str = \"\", provider: str = \"resend\",\n         from_addr: str = \"\") -> dict:\n    \"\"\"Send an email via a transactional provider or SMTP.\"\"\"\n    sender = from_addr or os.environ.get(\"EMAIL_FROM\", \"noreply@example.com\")\n\n    if provider == \"resend\":\n        resp = httpx.post(\n            \"https://api.resend.com/emails\",\n            headers={\"Authorization\": f\"Bearer {os.environ['RESEND_API_KEY']}\",\n                     \"Content-Type\": \"application/json\"},\n            json={\n                \"from\": sender, \"to\": to, \"subject\": subject,\n                \"html\": body_html or f\"<pre>{body_text}</pre>\",\n                \"text\": body_text or \"\",\n            },\n            timeout=15,\n        )\n        resp.raise_for_status()\n        data = resp.json()\n        return {\"success\": True, \"message_id\": data.get(\"id\", \"\")}\n\n    if provider == \"sendgrid\":\n        resp = httpx.post(\n            \"https://api.sendgrid.com/v3/mail/send\",\n            headers={\"Authorization\": f\"Bearer {os.environ['SENDGRID_API_KEY']}\",\n                     \"Content-Type\": \"application/json\"},\n            json={\n                \"personalizations\": [{\"to\": [{\"email\": e} for e in to]}],\n                \"from\": {\"email\": sender},\n                \"subject\": subject,\n                \"content\": [\n                    {\"type\": \"text/plain\", \"value\": body_text or \"\"},\n                    {\"type\": \"text/html\", \"value\": body_html or body_text},\n                ],\n            },\n            timeout=15,\n        )\n        resp.raise_for_status()\n        return {\"success\": True, \"message_id\": resp.headers.get(\"X-Message-Id\", \"\")}\n\n    # SMTP fallback\n    import smtplib\n    from email.mime.text import MIMEText\n    msg = MIMEText(body_html or body_text, \"html\" if body_html else \"plain\")\n    msg[\"Subject\"] = subject\n    msg[\"From\"] = sender\n    msg[\"To\"] = \", \".join(to)\n    with smtplib.SMTP(os.environ[\"SMTP_HOST\"], int(os.environ.get(\"SMTP_PORT\", 587))) as s:\n        s.starttls()\n        s.login(os.environ[\"SMTP_USER\"], os.environ[\"SMTP_PASS\"])\n        s.send_message(msg)\n    return {\"success\": True, \"message_id\": \"\"}\n```\n\n---\n\n## Git Operations\n\n### `git_commit_push`\n\n**Description:** Stages specified files (or all changes), creates a commit with a conventional-commit message, and optionally pushes to the remote -- the bread-and-butter of automated code updates.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo_path: str`, `message: str`, `files: list[str] = []`, `push: bool = True`, `branch: str = \"\"` |\n| **Output** | `dict` with `commit_sha: str`, `branch: str`, `pushed: bool` |\n| **Key Dependencies** | `subprocess` (stdlib), `git` CLI |\n| **MCP Alternative** | `@anthropic/git-mcp` server -- exposes `git_status`, `git_commit`, `git_push`, etc. |\n\n```python\nimport subprocess\n\ndef _git(repo: str, *args: str) -> str:\n    result = subprocess.run(\n        [\"git\", *args], cwd=repo,\n        capture_output=True, text=True, check=True,\n    )\n    return result.stdout.strip()\n\ndef main(repo_path: str, message: str, files: list[str] | None = None,\n         push: bool = True, branch: str = \"\") -> dict:\n    \"\"\"Stage, commit, and optionally push changes in a git repo.\"\"\"\n    if branch:\n        try:\n            _git(repo_path, \"checkout\", branch)\n        except subprocess.CalledProcessError:\n            _git(repo_path, \"checkout\", \"-b\", branch)\n\n    # Stage\n    if files:\n        for f in files:\n            _git(repo_path, \"add\", f)\n    else:\n        _git(repo_path, \"add\", \"-A\")\n\n    # Commit\n    _git(repo_path, \"commit\", \"-m\", message)\n    sha = _git(repo_path, \"rev-parse\", \"HEAD\")\n\n    current_branch = _git(repo_path, \"rev-parse\", \"--abbrev-ref\", \"HEAD\")\n\n    pushed = False\n    if push:\n        _git(repo_path, \"push\", \"-u\", \"origin\", current_branch)\n        pushed = True\n\n    return {\"commit_sha\": sha, \"branch\": current_branch, \"pushed\": pushed}\n```\n\n---\n\n### `git_branch_manage`\n\n**Description:** Creates, lists, switches, or deletes branches in a local repository, with optional remote tracking setup.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo_path: str`, `action: Literal[\"create\", \"switch\", \"delete\", \"list\"]`, `branch: str = \"\"`, `from_ref: str = \"main\"` |\n| **Output** | `dict` with `branch: str`, `action: str`, `branches: list[str]` (for list action) |\n| **Key Dependencies** | `subprocess` (stdlib), `git` CLI |\n| **MCP Alternative** | `@anthropic/git-mcp` |\n\n```python\nimport subprocess\n\ndef _git(repo: str, *args: str) -> str:\n    result = subprocess.run(\n        [\"git\", *args], cwd=repo,\n        capture_output=True, text=True, check=True,\n    )\n    return result.stdout.strip()\n\ndef main(repo_path: str, action: str = \"list\", branch: str = \"\",\n         from_ref: str = \"main\") -> dict:\n    \"\"\"Create, switch, delete, or list branches.\"\"\"\n\n    if action == \"list\":\n        raw = _git(repo_path, \"branch\", \"-a\")\n        branches = [b.strip().lstrip(\"* \") for b in raw.splitlines()]\n        return {\"action\": \"list\", \"branches\": branches, \"branch\": \"\"}\n\n    if not branch:\n        raise ValueError(\"branch name required for create/switch/delete\")\n\n    if action == \"create\":\n        _git(repo_path, \"checkout\", \"-b\", branch, from_ref)\n        return {\"action\": \"create\", \"branch\": branch, \"branches\": []}\n\n    if action == \"switch\":\n        _git(repo_path, \"checkout\", branch)\n        return {\"action\": \"switch\", \"branch\": branch, \"branches\": []}\n\n    if action == \"delete\":\n        _git(repo_path, \"branch\", \"-d\", branch)\n        return {\"action\": \"delete\", \"branch\": branch, \"branches\": []}\n\n    raise ValueError(f\"Unknown action: {action}\")\n```\n\n---\n\n### `git_read_repo`\n\n**Description:** Reads repository structure and file contents without requiring a local clone, using either a local path or the GitHub API for remote repos.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo: str` (local path or `owner/repo`), `path: str = \"\"`, `ref: str = \"main\"`, `depth: int = 2` |\n| **Output** | `dict` with `tree: list[dict]`, `file_content: str` (if path points to a file) |\n| **Key Dependencies** | `httpx`, `subprocess` (stdlib) |\n| **MCP Alternative** | `@anthropic/github-mcp` for remote repos |\n\n```python\nimport os, httpx, subprocess, pathlib\n\ndef main(repo: str, path: str = \"\", ref: str = \"main\",\n         depth: int = 2) -> dict:\n    \"\"\"Read repository tree or file contents.\"\"\"\n\n    # Local repo\n    if os.path.isdir(repo):\n        target = pathlib.Path(repo) / path\n        if target.is_file():\n            return {\"tree\": [], \"file_content\": target.read_text(encoding=\"utf-8\")}\n        tree = []\n        for item in sorted(target.rglob(\"*\")):\n            rel = item.relative_to(pathlib.Path(repo))\n            if len(rel.parts) <= depth and not any(p.startswith(\".\") for p in rel.parts):\n                tree.append({\n                    \"path\": str(rel), \"type\": \"dir\" if item.is_dir() else \"file\",\n                    \"size\": item.stat().st_size if item.is_file() else 0,\n                })\n        return {\"tree\": tree, \"file_content\": \"\"}\n\n    # Remote GitHub repo\n    headers = {\"Authorization\": f\"token {os.environ['GITHUB_TOKEN']}\",\n               \"Accept\": \"application/vnd.github.v3+json\"}\n    base = f\"https://api.github.com/repos/{repo}/contents/{path}\"\n    resp = httpx.get(base, headers=headers, params={\"ref\": ref}, timeout=15)\n    resp.raise_for_status()\n    data = resp.json()\n\n    if isinstance(data, dict) and data.get(\"type\") == \"file\":\n        import base64\n        content = base64.b64decode(data[\"content\"]).decode(\"utf-8\")\n        return {\"tree\": [], \"file_content\": content}\n\n    tree = [{\"path\": item[\"path\"], \"type\": item[\"type\"], \"size\": item.get(\"size\", 0)}\n            for item in data]\n    return {\"tree\": tree, \"file_content\": \"\"}\n```\n\n---\n\n## GitHub API\n\n### `github_create_issue`\n\n**Description:** Creates a new issue on a GitHub repository with title, body, labels, assignees, and milestone, returning the full issue URL and number.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo: str` (`owner/repo`), `title: str`, `body: str = \"\"`, `labels: list[str] = []`, `assignees: list[str] = []`, `milestone: int = 0` |\n| **Output** | `dict` with `number: int`, `url: str`, `html_url: str` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | `@anthropic/github-mcp` -- exposes `create_issue` |\n\n```python\nimport os, httpx\n\ndef main(repo: str, title: str, body: str = \"\",\n         labels: list[str] | None = None,\n         assignees: list[str] | None = None,\n         milestone: int = 0) -> dict:\n    \"\"\"Create a GitHub issue.\"\"\"\n    headers = {\n        \"Authorization\": f\"token {os.environ['GITHUB_TOKEN']}\",\n        \"Accept\": \"application/vnd.github.v3+json\",\n    }\n    payload: dict = {\"title\": title, \"body\": body}\n    if labels:\n        payload[\"labels\"] = labels\n    if assignees:\n        payload[\"assignees\"] = assignees\n    if milestone:\n        payload[\"milestone\"] = milestone\n\n    resp = httpx.post(\n        f\"https://api.github.com/repos/{repo}/issues\",\n        headers=headers, json=payload, timeout=15,\n    )\n    resp.raise_for_status()\n    data = resp.json()\n    return {\n        \"number\": data[\"number\"],\n        \"url\": data[\"url\"],\n        \"html_url\": data[\"html_url\"],\n    }\n```\n\n---\n\n### `github_pull_request`\n\n**Description:** Creates or updates a pull request, with support for draft PRs, auto-merge labels, and reviewer assignment.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo: str`, `title: str`, `body: str`, `head: str`, `base: str = \"main\"`, `draft: bool = False`, `reviewers: list[str] = []` |\n| **Output** | `dict` with `number: int`, `html_url: str`, `state: str` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | `@anthropic/github-mcp` -- exposes `create_pull_request` |\n\n```python\nimport os, httpx\n\ndef main(repo: str, title: str, body: str, head: str,\n         base: str = \"main\", draft: bool = False,\n         reviewers: list[str] | None = None) -> dict:\n    \"\"\"Create a GitHub pull request.\"\"\"\n    headers = {\n        \"Authorization\": f\"token {os.environ['GITHUB_TOKEN']}\",\n        \"Accept\": \"application/vnd.github.v3+json\",\n    }\n    payload = {\n        \"title\": title, \"body\": body,\n        \"head\": head, \"base\": base, \"draft\": draft,\n    }\n\n    resp = httpx.post(\n        f\"https://api.github.com/repos/{repo}/pulls\",\n        headers=headers, json=payload, timeout=15,\n    )\n    resp.raise_for_status()\n    pr = resp.json()\n\n    # Request reviewers if specified\n    if reviewers:\n        httpx.post(\n            f\"https://api.github.com/repos/{repo}/pulls/{pr['number']}/requested_reviewers\",\n            headers=headers,\n            json={\"reviewers\": reviewers},\n            timeout=10,\n        )\n\n    return {\n        \"number\": pr[\"number\"],\n        \"html_url\": pr[\"html_url\"],\n        \"state\": pr[\"state\"],\n    }\n```\n\n---\n\n### `github_workflow_dispatch`\n\n**Description:** Triggers a GitHub Actions workflow via the repository dispatch or workflow_dispatch API, passing custom inputs and waiting optionally for completion status.\n\n| Field | Detail |\n|---|---|\n| **Input** | `repo: str`, `workflow_id: str` (filename or ID), `ref: str = \"main\"`, `inputs: dict = {}`, `wait: bool = False`, `timeout: int = 300` |\n| **Output** | `dict` with `triggered: bool`, `run_id: int`, `status: str`, `conclusion: str` |\n| **Key Dependencies** | `httpx`, `time` (stdlib) |\n| **MCP Alternative** | `@anthropic/github-mcp` |\n\n```python\nimport os, httpx, time\n\ndef main(repo: str, workflow_id: str, ref: str = \"main\",\n         inputs: dict | None = None, wait: bool = False,\n         timeout: int = 300) -> dict:\n    \"\"\"Trigger a GitHub Actions workflow and optionally wait for completion.\"\"\"\n    headers = {\n        \"Authorization\": f\"token {os.environ['GITHUB_TOKEN']}\",\n        \"Accept\": \"application/vnd.github.v3+json\",\n    }\n    # Trigger\n    resp = httpx.post(\n        f\"https://api.github.com/repos/{repo}/actions/workflows/{workflow_id}/dispatches\",\n        headers=headers,\n        json={\"ref\": ref, \"inputs\": inputs or {}},\n        timeout=15,\n    )\n    resp.raise_for_status()\n\n    if not wait:\n        return {\"triggered\": True, \"run_id\": 0, \"status\": \"queued\", \"conclusion\": \"\"}\n\n    # Poll for the new run\n    time.sleep(3)\n    deadline = time.time() + timeout\n    while time.time() < deadline:\n        runs_resp = httpx.get(\n            f\"https://api.github.com/repos/{repo}/actions/workflows/{workflow_id}/runs\",\n            headers=headers, params={\"branch\": ref, \"per_page\": 1},\n            timeout=15,\n        )\n        runs = runs_resp.json().get(\"workflow_runs\", [])\n        if runs:\n            run = runs[0]\n            if run[\"status\"] == \"completed\":\n                return {\n                    \"triggered\": True, \"run_id\": run[\"id\"],\n                    \"status\": run[\"status\"], \"conclusion\": run[\"conclusion\"],\n                }\n        time.sleep(10)\n\n    return {\"triggered\": True, \"run_id\": 0, \"status\": \"timeout\", \"conclusion\": \"\"}\n```\n\n---\n\n## Database Operations\n\n### `supabase_query`\n\n**Description:** Executes queries against a Supabase project using the PostgREST API -- supports select, insert, update, upsert, and delete with filter chaining.\n\n| Field | Detail |\n|---|---|\n| **Input** | `table: str`, `action: Literal[\"select\", \"insert\", \"update\", \"upsert\", \"delete\"]`, `data: dict | list[dict] = {}`, `filters: list[dict] = []`, `select_columns: str = \"*\"`, `limit: int = 100` |\n| **Output** | `dict` with `data: list[dict]`, `count: int`, `status: int` |\n| **Key Dependencies** | `supabase` (supabase-py) |\n| **MCP Alternative** | `@anthropic/supabase-mcp` server |\n\n```python\nimport os\nfrom supabase import create_client\n\ndef main(table: str, action: str = \"select\", data=None,\n         filters: list[dict] | None = None,\n         select_columns: str = \"*\", limit: int = 100) -> dict:\n    \"\"\"Execute a Supabase database operation.\"\"\"\n    client = create_client(\n        os.environ[\"SUPABASE_URL\"],\n        os.environ[\"SUPABASE_SERVICE_KEY\"],\n    )\n\n    query = client.table(table)\n\n    if action == \"select\":\n        query = query.select(select_columns)\n        for f in (filters or []):\n            query = getattr(query, f.get(\"op\", \"eq\"))(f[\"field\"], f[\"value\"])\n        result = query.limit(limit).execute()\n        return {\"data\": result.data, \"count\": len(result.data), \"status\": 200}\n\n    if action == \"insert\":\n        result = query.insert(data).execute()\n        return {\"data\": result.data, \"count\": len(result.data), \"status\": 201}\n\n    if action == \"upsert\":\n        result = query.upsert(data).execute()\n        return {\"data\": result.data, \"count\": len(result.data), \"status\": 200}\n\n    if action == \"update\":\n        q = query.update(data)\n        for f in (filters or []):\n            q = getattr(q, f.get(\"op\", \"eq\"))(f[\"field\"], f[\"value\"])\n        result = q.execute()\n        return {\"data\": result.data, \"count\": len(result.data), \"status\": 200}\n\n    if action == \"delete\":\n        q = query.delete()\n        for f in (filters or []):\n            q = getattr(q, f.get(\"op\", \"eq\"))(f[\"field\"], f[\"value\"])\n        result = q.execute()\n        return {\"data\": result.data, \"count\": len(result.data), \"status\": 200}\n\n    raise ValueError(f\"Unknown action: {action}\")\n```\n\n---\n\n### `sqlite_query`\n\n**Description:** Opens (or creates) a local SQLite database and executes arbitrary SQL, returning rows as dictionaries -- ideal for lightweight local state, caching, and intermediate pipeline storage.\n\n| Field | Detail |\n|---|---|\n| **Input** | `db_path: str`, `sql: str`, `params: list = []`, `mode: Literal[\"query\", \"execute\", \"script\"] = \"query\"` |\n| **Output** | `dict` with `rows: list[dict]`, `rowcount: int`, `lastrowid: int` |\n| **Key Dependencies** | `sqlite3` (stdlib) |\n| **MCP Alternative** | `@anthropic/sqlite-mcp` server |\n\n```python\nimport sqlite3\n\ndef main(db_path: str, sql: str, params: list | None = None,\n         mode: str = \"query\") -> dict:\n    \"\"\"Execute SQL against a SQLite database.\"\"\"\n    conn = sqlite3.connect(db_path)\n    conn.row_factory = sqlite3.Row\n    cur = conn.cursor()\n\n    try:\n        if mode == \"script\":\n            cur.executescript(sql)\n            conn.commit()\n            return {\"rows\": [], \"rowcount\": cur.rowcount, \"lastrowid\": cur.lastrowid}\n\n        if mode == \"execute\":\n            cur.execute(sql, params or [])\n            conn.commit()\n            return {\"rows\": [], \"rowcount\": cur.rowcount, \"lastrowid\": cur.lastrowid}\n\n        # query mode\n        cur.execute(sql, params or [])\n        rows = [dict(row) for row in cur.fetchall()]\n        return {\"rows\": rows, \"rowcount\": len(rows), \"lastrowid\": 0}\n\n    finally:\n        conn.close()\n```\n\n---\n\n### `airtable_ops`\n\n**Description:** Reads, creates, updates, or deletes records in an Airtable base, handling pagination and field-type coercion automatically.\n\n| Field | Detail |\n|---|---|\n| **Input** | `base_id: str`, `table_name: str`, `action: Literal[\"list\", \"create\", \"update\", \"delete\"]`, `records: list[dict] = []`, `filter_formula: str = \"\"`, `max_records: int = 100` |\n| **Output** | `dict` with `records: list[dict]`, `count: int` |\n| **Key Dependencies** | `httpx` |\n| **MCP Alternative** | None standard; wrap as custom MCP tool |\n\n```python\nimport os, httpx\n\nBASE_URL = \"https://api.airtable.com/v0\"\n\ndef _headers():\n    return {\n        \"Authorization\": f\"Bearer {os.environ['AIRTABLE_API_KEY']}\",\n        \"Content-Type\": \"application/json\",\n    }\n\ndef main(base_id: str, table_name: str, action: str = \"list\",\n         records: list[dict] | None = None,\n         filter_formula: str = \"\", max_records: int = 100) -> dict:\n    \"\"\"Perform CRUD operations on Airtable records.\"\"\"\n    url = f\"{BASE_URL}/{base_id}/{table_name}\"\n\n    if action == \"list\":\n        params: dict = {\"maxRecords\": max_records}\n        if filter_formula:\n            params[\"filterByFormula\"] = filter_formula\n        all_records, offset = [], None\n        while True:\n            if offset:\n                params[\"offset\"] = offset\n            resp = httpx.get(url, headers=_headers(), params=params, timeout=15)\n            resp.raise_for_status()\n            data = resp.json()\n            all_records.extend(data.get(\"records\", []))\n            offset = data.get(\"offset\")\n            if not offset or len(all_records) >= max_records:\n                break\n        return {\"records\": all_records, \"count\": len(all_records)}\n\n    if action == \"create\":\n        payload = {\"records\": [{\"fields\": r} for r in (records or [])]}\n        resp = httpx.post(url, headers=_headers(), json=payload, timeout=15)\n        resp.raise_for_status()\n        created = resp.json().get(\"records\", [])\n        return {\"records\": created, \"count\": len(created)}\n\n    if action == \"update\":\n        payload = {\"records\": records or []}  # expects [{\"id\": ..., \"fields\": {...}}]\n        resp = httpx.patch(url, headers=_headers(), json=payload, timeout=15)\n        resp.raise_for_status()\n        updated = resp.json().get(\"records\", [])\n        return {\"records\": updated, \"count\": len(updated)}\n\n    if action == \"delete\":\n        ids = [r if isinstance(r, str) else r.get(\"id\") for r in (records or [])]\n        resp = httpx.delete(url, headers=_headers(),\n                            params={\"records[]\": ids}, timeout=15)\n        resp.raise_for_status()\n        return {\"records\": resp.json().get(\"records\", []),","truncated":true,"truncatedBy":"bytes","totalLines":2869,"totalBytes":105510,"outputLines":1444,"outputBytes":51200,"lastLinePartial":false,"firstLineExceedsLimit":false,"maxLines":2000,"maxBytes":51200}},"isError":false,"timestamp":1770815899757}}
{"type":"message","id":"98759386","parentId":"ce9d3267","timestamp":"2026-02-11T13:18:19.759Z","message":{"role":"toolResult","toolCallId":"toolu_012DUCPo8zoejFH1syCv5hm7","toolName":"read","content":[{"type":"text","text":"# MCP Registry\n\nThe factory's awareness of all available MCPs and integrations. This is the single source of truth for what tools are available when building WAT systems.\n\n**Usage**: Before generating tools, always check this registry. Prefer MCPs over raw API calls when available — less code to maintain, fewer bugs. But always provide a fallback.\n\n**Adding new MCPs**: Add a new entry following the format below. The factory will automatically discover it during the Research step.\n\n---\n\n## Web & Data Access\n\n### Firecrawl\n- **Description**: Web scraping and crawling service. Extracts clean content from websites, handles JavaScript rendering, and returns structured data.\n- **Key Capabilities**: Scrape single pages, crawl entire sites, extract structured data, handle JS-heavy sites, return markdown or JSON\n- **Common Use Cases**: Company research, content extraction, competitive analysis, price monitoring\n- **Tool Pattern**: `firecrawl_scrape(url) → markdown/JSON content`\n- **Secret**: `FIRECRAWL_API_KEY`\n- **Fallback**: Direct HTTP with `requests` + `beautifulsoup4` (won't handle JS rendering)\n\n### Brave Search\n- **Description**: Web search API. Returns search results with snippets and metadata.\n- **Key Capabilities**: Web search, news search, image search, summarized results\n- **Common Use Cases**: Research gathering, fact-checking, finding URLs, news monitoring\n- **Tool Pattern**: `brave_search(query) → list of search results with URLs and snippets`\n- **Secret**: `BRAVE_SEARCH_API_KEY`\n- **Fallback**: Google Custom Search API or SerpAPI\n\n### Puppeteer\n- **Description**: Browser automation MCP. Controls a headless Chrome instance for web interaction.\n- **Key Capabilities**: Navigate pages, fill forms, click buttons, take screenshots, extract DOM content\n- **Common Use Cases**: Scraping dynamic sites, form submission, visual testing, PDF generation\n- **Tool Pattern**: `puppeteer_navigate(url) → page content; puppeteer_screenshot() → image`\n- **Secret**: None (local browser)\n- **Fallback**: Playwright via Python `playwright` package\n\n### Fetch\n- **Description**: Simple HTTP fetch MCP. Makes HTTP requests and returns responses.\n- **Key Capabilities**: GET, POST, PUT, DELETE requests with headers and body\n- **Common Use Cases**: API calls, webhooks, simple page fetches\n- **Tool Pattern**: `fetch(url, method, headers, body) → response`\n- **Secret**: None\n- **Fallback**: Python `requests` library\n\n---\n\n## Code & DevOps\n\n### GitHub\n- **Description**: GitHub API integration. Full access to repos, issues, PRs, Actions, and more.\n- **Key Capabilities**: Create/read/update repos, issues, PRs, manage Actions, read file contents, manage releases\n- **Common Use Cases**: Automated PR creation, issue management, repo setup, CI/CD integration\n- **Tool Pattern**: `github_create_issue(repo, title, body); github_create_pr(repo, ...)`\n- **Secret**: `GITHUB_TOKEN` or `GITHUB_PAT`\n- **Fallback**: `gh` CLI or direct GitHub REST API via `requests`\n\n### Docker\n- **Description**: Docker container management MCP.\n- **Key Capabilities**: Build, run, stop, and manage containers\n- **Common Use Cases**: Isolated tool execution, testing environments\n- **Tool Pattern**: `docker_run(image, command) → output`\n- **Secret**: None (local Docker daemon)\n- **Fallback**: `subprocess.run([\"docker\", ...])`\n\n---\n\n## Communication\n\n### Slack\n- **Description**: Slack messaging MCP. Send messages, read channels, manage threads.\n- **Key Capabilities**: Send messages, post to channels, read channel history, manage threads, upload files\n- **Common Use Cases**: Notifications, alerts, status updates, team communication\n- **Tool Pattern**: `slack_send(channel, message); slack_read(channel) → messages`\n- **Secret**: `SLACK_BOT_TOKEN` or `SLACK_WEBHOOK_URL`\n- **Fallback**: Slack Incoming Webhook via HTTP POST\n\n### Telegram\n- **Description**: Telegram Bot API MCP. Send and receive messages via Telegram bots.\n- **Key Capabilities**: Send messages, photos, documents; read updates; inline keyboards\n- **Common Use Cases**: Alerts, notifications, command-driven bots, status reports\n- **Tool Pattern**: `telegram_send(chat_id, message); telegram_get_updates() → messages`\n- **Secret**: `TELEGRAM_BOT_TOKEN`\n- **Fallback**: Telegram Bot API via HTTP\n\n### Gmail\n- **Description**: Gmail API MCP. Send and read emails.\n- **Key Capabilities**: Send emails, read inbox, search emails, manage labels\n- **Common Use Cases**: Email notifications, inbox monitoring, automated responses\n- **Tool Pattern**: `gmail_send(to, subject, body); gmail_search(query) → emails`\n- **Secret**: `GMAIL_CREDENTIALS`\n- **Fallback**: SMTP via `smtplib`\n\n### Discord\n- **Description**: Discord messaging via webhooks or bot.\n- **Key Capabilities**: Send messages, embeds, files to channels\n- **Common Use Cases**: Team notifications, alerts, community updates\n- **Tool Pattern**: `discord_send(webhook_url, message)`\n- **Secret**: `DISCORD_WEBHOOK_URL`\n- **Fallback**: Discord webhook via HTTP POST\n\n---\n\n## Storage & Databases\n\n### Supabase\n- **Description**: Supabase (Postgres + Auth + Storage) MCP. Full database and storage access.\n- **Key Capabilities**: CRUD operations, real-time subscriptions, file storage, auth management\n- **Common Use Cases**: Persistent data storage, user management, file hosting\n- **Tool Pattern**: `supabase_insert(table, data); supabase_query(table, filters) → rows`\n- **Secret**: `SUPABASE_URL`, `SUPABASE_KEY`\n- **Fallback**: Direct PostgreSQL via `psycopg2` or Supabase REST API\n\n### Google Drive\n- **Description**: Google Drive file management MCP.\n- **Key Capabilities**: Upload, download, list, search, share files and folders\n- **Common Use Cases**: File storage, document sharing, backup\n- **Tool Pattern**: `gdrive_upload(file, folder_id); gdrive_list(folder_id) → files`\n- **Secret**: `GOOGLE_CREDENTIALS`\n- **Fallback**: Google Drive API via `google-api-python-client`\n\n### Airtable\n- **Description**: Airtable database MCP. CRUD operations on Airtable bases and tables.\n- **Key Capabilities**: Create, read, update, delete records; list tables; manage views\n- **Common Use Cases**: Lightweight database, CRM data, project tracking, content calendars\n- **Tool Pattern**: `airtable_create(base, table, record); airtable_list(base, table) → records`\n- **Secret**: `AIRTABLE_API_KEY`\n- **Fallback**: Airtable REST API via `requests`\n\n### Notion\n- **Description**: Notion API MCP. Manage pages, databases, and blocks.\n- **Key Capabilities**: Create/update pages, query databases, manage blocks\n- **Common Use Cases**: Knowledge base, documentation, project management, content publishing\n- **Tool Pattern**: `notion_create_page(parent, title, content); notion_query_db(db_id, filter) → pages`\n- **Secret**: `NOTION_API_KEY`\n- **Fallback**: Notion API via `requests`\n\n### SQLite\n- **Description**: Local SQLite database MCP. Embedded SQL database.\n- **Key Capabilities**: Create tables, CRUD operations, run SQL queries\n- **Common Use Cases**: Local data caching, lightweight storage, offline-first data\n- **Tool Pattern**: `sqlite_query(db_path, sql) → rows`\n- **Secret**: None (local file)\n- **Fallback**: Python `sqlite3` standard library module\n\n---\n\n## AI & Processing\n\n### OpenAI\n- **Description**: OpenAI API MCP. Access GPT models, DALL-E, Whisper, and embeddings.\n- **Key Capabilities**: Chat completions, image generation, speech-to-text, embeddings, structured output\n- **Common Use Cases**: Text generation, summarization, classification, image creation, transcription\n- **Tool Pattern**: `openai_chat(messages) → response; openai_embed(text) → vector`\n- **Secret**: `OPENAI_API_KEY`\n- **Fallback**: OpenAI Python SDK directly\n\n### Anthropic\n- **Description**: Anthropic API for Claude models.\n- **Key Capabilities**: Chat completions, tool use, structured output, long context\n- **Common Use Cases**: Complex reasoning, analysis, code generation, document processing\n- **Tool Pattern**: `anthropic_message(messages) → response`\n- **Secret**: `ANTHROPIC_API_KEY`\n- **Fallback**: Anthropic Python SDK directly\n\n### ElevenLabs\n- **Description**: AI voice synthesis and cloning.\n- **Key Capabilities**: Text-to-speech, voice cloning, audio generation\n- **Common Use Cases**: Audio content creation, podcasts, voice notifications\n- **Tool Pattern**: `elevenlabs_tts(text, voice_id) → audio_file`\n- **Secret**: `ELEVENLABS_API_KEY`\n- **Fallback**: ElevenLabs REST API\n\n### Replicate\n- **Description**: Run ML models via API. Access thousands of open-source models.\n- **Key Capabilities**: Image generation, video generation, audio processing, custom models\n- **Common Use Cases**: Image manipulation, model inference, content generation\n- **Tool Pattern**: `replicate_run(model, input) → output`\n- **Secret**: `REPLICATE_API_TOKEN`\n- **Fallback**: Replicate Python SDK or REST API\n\n---\n\n## Business & Productivity\n\n### Stripe\n- **Description**: Stripe payment processing API.\n- **Key Capabilities**: Create charges, manage subscriptions, list transactions, handle webhooks\n- **Common Use Cases**: Payment tracking, subscription management, revenue reporting\n- **Tool Pattern**: `stripe_list_charges(filters) → charges`\n- **Secret**: `STRIPE_API_KEY`\n- **Fallback**: Stripe Python SDK\n\n### Google Sheets\n- **Description**: Google Sheets API MCP.\n- **Key Capabilities**: Read/write cells, create sheets, format, formulas\n- **Common Use Cases**: Data input/output, reporting, dashboards, data collection\n- **Tool Pattern**: `sheets_read(spreadsheet_id, range) → data; sheets_write(spreadsheet_id, range, data)`\n- **Secret**: `GOOGLE_CREDENTIALS`\n- **Fallback**: `gspread` Python library\n\n### Google Calendar\n- **Description**: Google Calendar API MCP.\n- **Key Capabilities**: Create/read/update events, list calendars, manage attendees\n- **Common Use Cases**: Scheduling, event tracking, availability checks\n- **Tool Pattern**: `calendar_create_event(calendar_id, event); calendar_list(calendar_id) → events`\n- **Secret**: `GOOGLE_CREDENTIALS`\n- **Fallback**: Google Calendar API via `google-api-python-client`\n\n### Linear\n- **Description**: Linear project management API.\n- **Key Capabilities**: Create/update issues, manage projects, track cycles\n- **Common Use Cases**: Task creation, sprint management, automated issue tracking\n- **Tool Pattern**: `linear_create_issue(team, title, description)`\n- **Secret**: `LINEAR_API_KEY`\n- **Fallback**: Linear GraphQL API via `requests`\n\n---\n\n## Local & System\n\n### Filesystem\n- **Description**: Local filesystem access MCP.\n- **Key Capabilities**: Read, write, list, delete files and directories\n- **Common Use Cases**: File processing, data storage, configuration management\n- **Tool Pattern**: `fs_read(path) → content; fs_write(path, content)`\n- **Secret**: None\n- **Fallback**: Python `pathlib` / built-in `open()`\n\n### Shell / Desktop Commander\n- **Description**: Execute shell commands on the local system.\n- **Key Capabilities**: Run commands, manage processes, system information\n- **Common Use Cases**: Tool execution, system automation, build processes\n- **Tool Pattern**: `shell_exec(command) → output`\n- **Secret**: None\n- **Fallback**: Python `subprocess`\n\n---\n\n## Social & Content\n\n### Twitter/X\n- **Description**: Twitter/X API integration.\n- **Key Capabilities**: Post tweets, read timelines, search tweets, manage lists\n- **Common Use Cases**: Social media posting, monitoring mentions, content distribution\n- **Tool Pattern**: `twitter_post(text); twitter_search(query) → tweets`\n- **Secret**: `TWITTER_BEARER_TOKEN`\n- **Fallback**: Twitter API v2 via `requests`\n\n### LinkedIn\n- **Description**: LinkedIn API integration.\n- **Key Capabilities**: Post updates, read profile, company pages\n- **Common Use Cases**: Professional content posting, company updates\n- **Tool Pattern**: `linkedin_post(text, image_url)`\n- **Secret**: `LINKEDIN_ACCESS_TOKEN`\n- **Fallback**: LinkedIn API via `requests`\n\n### YouTube\n- **Description**: YouTube Data API.\n- **Key Capabilities**: Search videos, get video details, manage playlists, read comments\n- **Common Use Cases**: Video research, content analysis, playlist management\n- **Tool Pattern**: `youtube_search(query) → videos; youtube_video_details(video_id) → metadata`\n- **Secret**: `YOUTUBE_API_KEY`\n- **Fallback**: YouTube Data API via `google-api-python-client`\n\n---\n\n## Custom / Any API\n\nAny REST API can be integrated into a WAT system without an MCP:\n\n1. Write a Python tool using `requests` with proper auth headers\n2. Store API credentials in GitHub Secrets\n3. Reference via `os.environ[\"SECRET_NAME\"]` in the tool\n4. Add retry logic with `tenacity` or manual retry loops\n5. Document the API in the system's CLAUDE.md\n\nThe factory is never limited to the MCPs listed here. This registry helps with tool selection, but any API accessible via HTTP can be wrapped as a Python tool.\n"}],"isError":false,"timestamp":1770815899758}}
{"type":"message","id":"d0951087","parentId":"98759386","timestamp":"2026-02-11T13:18:28.100Z","message":{"role":"assistant","content":[{"type":"thinking","thinking":"Great, I have all the key reference material. Now let me read the PRP template to understand the structure.","thinkingSignature":"EpMCCkYICxgCKkBGBoCgo+ATR12OgUvlIhGKmmt1QL8RUukYhv+qb5qOgBHR/NhIf6dkyYUDtHEX5RUE7qIs7zVVYs1W0Ggo+aELEgyP+MyoqRj2K876U2waDHBc/+v7xX6ocdbj1CIwBP/6DMJiILGvEa4t1yCszbIZwyQTiYW+3FWgoi8wgzuJuTkQkYk65xX2F9nakFJLKntuJ6WuCn/Q4hbCh3gHHi/IPh/DQ/WlxLQIGuBdF0bP5UMRwgTtFmTaVyDN+6nxM/yD+O61jvfKFMKAtQayFrM3IuxrFZlQdHPP/TQQ/EiBDK0FWsYuwtQ6PwEA6NwXlj1VMpZiyAMrgvO/GzA8SPihZW2YWom4zQ48eOsYAQ=="},{"type":"toolCall","id":"toolu_01FjTdafuTeZdtNaTdyTptoD","name":"read","arguments":{"path":"PRPs/templates/prp_base.md"}}],"api":"anthropic-messages","provider":"anthropic","model":"claude-sonnet-4-5-20250929","usage":{"input":14,"output":92,"cacheRead":8662,"cacheWrite":40273,"totalTokens":49041,"cost":{"input":0.000042000000000000004,"output":0.00138,"cacheRead":0.0025986,"cacheWrite":0.15102375,"total":0.15504435000000003}},"stopReason":"toolUse","timestamp":1770815899759}}
{"type":"message","id":"f2936a0b","parentId":"d0951087","timestamp":"2026-02-11T13:18:28.102Z","message":{"role":"toolResult","toolCallId":"toolu_01FjTdafuTeZdtNaTdyTptoD","toolName":"read","content":[{"type":"text","text":"name: \"{System Name}\"\ndescription: |\n\n## Purpose\nWAT System PRP (Product Requirements Prompt) — a structured blueprint that gives the factory enough context to build a complete, working system in one pass.\n\n## Core Principles\n1. **Context is King**: Include ALL necessary documentation, references, and caveats\n2. **Validation Loops**: Provide executable checks the factory can run at each gate\n3. **Information Dense**: Use keywords and patterns from the WAT framework\n4. **Progressive Success**: Start simple, validate, then enhance\n5. **Global rules**: Follow all rules in CLAUDE.md\n\n---\n\n## Goal\n[What system needs to be built. Be specific about the end state: what it does, who uses it, and what success looks like when deployed.]\n\n## Why\n- [Business value and user impact]\n- [What manual process this automates or what gap it fills]\n- [Who benefits and how often they benefit]\n\n## What\n[User-visible behavior and technical requirements. Describe what the system does from the perspective of someone triggering it.]\n\n### Success Criteria\n- [ ] [Specific measurable outcome 1]\n- [ ] [Specific measurable outcome 2]\n- [ ] [Specific measurable outcome 3]\n- [ ] System runs autonomously via GitHub Actions on schedule\n- [ ] Results are committed back to repo or delivered via webhook/notification\n- [ ] All three execution paths work: CLI, GitHub Actions, Agent HQ\n\n---\n\n## Inputs\n[What goes into the system. Be specific about format, source, and any validation requirements.]\n\n```yaml\n- name: \"{input_name}\"\n  type: \"{string | JSON | file | URL | list}\"\n  source: \"{manual input | API | file path | webhook payload}\"\n  required: true\n  description: \"{What this input is and what format it should be in}\"\n  example: \"{A concrete example value}\"\n```\n\n## Outputs\n[What comes out of the system. Where do results go?]\n\n```yaml\n- name: \"{output_name}\"\n  type: \"{JSON | CSV | Markdown | file | webhook}\"\n  destination: \"{repo commit | email | Slack | webhook URL | file path}\"\n  description: \"{What this output contains}\"\n  example: \"{A concrete example or link to sample}\"\n```\n\n---\n\n## All Needed Context\n\n### Documentation & References\n```yaml\n# MUST READ — Include these in context when building\n- url: \"{Official API docs URL}\"\n  why: \"{Specific sections/methods the tools will need}\"\n\n- url: \"{Library documentation URL}\"\n  why: \"{Specific patterns or gotchas to follow}\"\n\n- file: \"{path/to/existing/pattern.py}\"\n  why: \"{Pattern to follow, conventions to match}\"\n\n- doc: \"config/mcp_registry.md\"\n  why: \"Check which MCPs provide the capabilities this system needs\"\n\n- doc: \"library/patterns.md\"\n  why: \"Select the best workflow pattern for this system\"\n\n- doc: \"library/tool_catalog.md\"\n  why: \"Identify reusable tool patterns to adapt\"\n```\n\n### Workflow Pattern Selection\n```yaml\n# Reference library/patterns.md — select the best-fit pattern\npattern: \"{Pattern name from library/patterns.md, e.g., Scrape > Process > Output}\"\nrationale: \"{Why this pattern fits the problem}\"\nmodifications: \"{Any adjustments needed to the standard pattern}\"\n```\n\n### MCP & Tool Requirements\n```yaml\n# Reference config/mcp_registry.md — list capabilities needed\ncapabilities:\n  - name: \"{capability, e.g., web scraping}\"\n    primary_mcp: \"{MCP name from registry, e.g., firecrawl}\"\n    alternative_mcp: \"{Alternative MCP}\"\n    fallback: \"{Direct API/HTTP approach if no MCP available}\"\n    secret_name: \"{UPPERCASE_SECRET_NAME}\"\n\n  - name: \"{capability 2}\"\n    primary_mcp: \"{MCP name}\"\n    alternative_mcp: \"{Alternative}\"\n    fallback: \"{Fallback approach}\"\n    secret_name: \"{SECRET_NAME}\"\n```\n\n### Known Gotchas & Constraints\n```\n# CRITICAL: {API/service name} has rate limit of {N} requests per {period}\n# CRITICAL: {Library} requires {specific setup or version}\n# CRITICAL: {Data source} returns {unexpected format} — must handle {edge case}\n# CRITICAL: Secrets are NEVER hardcoded — always use GitHub Secrets or .env\n# CRITICAL: Every tool must have try/except, logging, type hints, and a main() function\n```\n\n---\n\n## System Design\n\n### Subagent Architecture\n[Define the specialist subagents this system needs. One subagent per major capability or workflow phase.]\n\n```yaml\nsubagents:\n  - name: \"{function}-specialist\"\n    description: \"When Claude should delegate to this subagent — be specific\"\n    tools: \"Read, Bash, Grep\"\n    model: \"sonnet\"\n    permissionMode: \"default\"\n    responsibilities:\n      - \"{What this subagent does — step 1}\"\n      - \"{What this subagent does — step 2}\"\n    inputs: \"{What data it receives}\"\n    outputs: \"{What data it produces}\"\n\n  - name: \"{function}-specialist\"\n    description: \"When Claude should delegate to this subagent\"\n    tools: \"Read, Write, Bash\"\n    model: \"sonnet\"\n    permissionMode: \"default\"\n    responsibilities:\n      - \"{Responsibility 1}\"\n      - \"{Responsibility 2}\"\n    inputs: \"{Input data}\"\n    outputs: \"{Output data}\"\n```\n\n### Agent Teams Analysis\n```yaml\n# Apply the 3+ Independent Tasks Rule\nindependent_tasks:\n  - \"{Task A — no dependency on other tasks}\"\n  - \"{Task B — no dependency on other tasks}\"\n  - \"{Task C — no dependency on other tasks}\"\n\nindependent_task_count: \"{N}\"\nrecommendation: \"{Use Agent Teams | Sequential execution}\"\nrationale: \"{Why — e.g., '3 independent API calls each taking 10s = 30s sequential vs 10s parallel'}\"\n\n# If Agent Teams recommended:\nteam_lead_responsibilities:\n  - \"Create shared task list\"\n  - \"Spawn teammates for independent tasks\"\n  - \"Merge results and run quality control\"\n\nteammates:\n  - name: \"{Teammate 1}\"\n    task: \"{Scoped task description}\"\n    inputs: \"{What it receives}\"\n    outputs: \"{What it produces}\"\n\n# If NOT recommended:\nsequential_rationale: \"{Why sequential is sufficient — e.g., 'all tasks depend on previous output'}\"\n```\n\n### GitHub Actions Triggers\n```yaml\ntriggers:\n  - type: \"{workflow_dispatch | schedule | repository_dispatch}\"\n    config: \"{cron expression, input parameters, or event type}\"\n    description: \"{When and why this trigger fires}\"\n```\n\n---\n\n## Implementation Blueprint\n\n### Workflow Steps\n[Ordered list of workflow phases. Each step becomes a section in workflow.md.]\n\n```yaml\nsteps:\n  - name: \"{Step 1 name}\"\n    description: \"{What happens in this step}\"\n    subagent: \"{Which subagent handles this, if any}\"\n    tools: [\"{tool_1.py}\", \"{tool_2.py}\"]\n    inputs: \"{What this step receives}\"\n    outputs: \"{What this step produces}\"\n    failure_mode: \"{What can go wrong}\"\n    fallback: \"{What to do if it fails}\"\n\n  - name: \"{Step 2 name}\"\n    description: \"{Description}\"\n    subagent: \"{subagent name}\"\n    tools: [\"{tool_name.py}\"]\n    inputs: \"{From previous step}\"\n    outputs: \"{For next step}\"\n    failure_mode: \"{Failure scenario}\"\n    fallback: \"{Recovery action}\"\n```\n\n### Tool Specifications\n[For each tool the system needs. Reference library/tool_catalog.md for reusable patterns.]\n\n```yaml\ntools:\n  - name: \"{tool_name}.py\"\n    purpose: \"{One-line description}\"\n    catalog_pattern: \"{Pattern name from tool_catalog.md, or 'new' if custom}\"\n    inputs:\n      - \"{arg1}: {type} — {description}\"\n      - \"{arg2}: {type} — {description}\"\n    outputs: \"{JSON structure or file format}\"\n    dependencies: [\"{pip package 1}\", \"{pip package 2}\"]\n    mcp_used: \"{MCP name, or 'none'}\"\n    error_handling: \"{How errors are handled}\"\n\n  - name: \"{tool_name_2}.py\"\n    purpose: \"{Description}\"\n    catalog_pattern: \"{Pattern or 'new'}\"\n    inputs:\n      - \"{arg}: {type} — {description}\"\n    outputs: \"{Output format}\"\n    dependencies: [\"{package}\"]\n    mcp_used: \"{MCP or 'none'}\"\n    error_handling: \"{Error approach}\"\n```\n\n### Per-Tool Pseudocode\n```python\n# {tool_name}.py\ndef main():\n    # PATTERN: {Which catalog pattern this follows}\n    # Step 1: Parse inputs\n    args = parse_args()  # argparse or env vars\n\n    # Step 2: {Core logic description}\n    # GOTCHA: {Important caveat about this API/library}\n    result = do_work(args.input)\n\n    # Step 3: Output results\n    # PATTERN: Structured JSON to stdout\n    print(json.dumps(result))\n\n# {tool_name_2}.py\ndef main():\n    # PATTERN: {Catalog pattern}\n    # CRITICAL: {Key constraint — rate limit, auth, format}\n    pass\n```\n\n### Integration Points\n```yaml\nSECRETS:\n  - name: \"{SECRET_NAME}\"\n    purpose: \"{What API/service this authenticates}\"\n    required: true\n\nENVIRONMENT:\n  - file: \".env.example\"\n    vars:\n      - \"{VAR_NAME}={placeholder_value}  # {description}\"\n\nDEPENDENCIES:\n  - file: \"requirements.txt\"\n    packages:\n      - \"{package}=={version}  # {why}\"\n\nGITHUB_ACTIONS:\n  - trigger: \"{workflow_dispatch | schedule | repository_dispatch}\"\n    config: \"{Details}\"\n```\n\n---\n\n## Validation Loop\n\n### Level 1: Syntax & Structure\n```bash\n# Run FIRST — every tool must pass before proceeding to Level 2\n# AST parse — verify valid Python syntax\npython -c \"import ast; ast.parse(open('tools/{tool_name}.py').read())\"\n\n# Import check — verify no missing dependencies\npython -c \"import importlib; importlib.import_module('tools.{tool_name}')\"\n\n# Structure check — verify main() exists\npython -c \"from tools.{tool_name} import main; assert callable(main)\"\n\n# Repeat for every tool file\n# Expected: All pass with no errors. If any fail, fix before proceeding.\n```\n\n### Level 2: Unit Tests\n```bash\n# Run SECOND — each tool must produce correct output for sample inputs\n# Test each tool independently with mock/sample data\n\npython tools/{tool_name}.py --input \"{sample_input}\"\n# Expected output: {expected JSON or result}\n\npython tools/{tool_name_2}.py --input \"{sample_input}\"\n# Expected output: {expected result}\n\n# If any tool fails: Read the error, fix the root cause, re-run.\n# NEVER mock to make tests pass — fix the actual code.\n```\n\n### Level 3: Integration Tests\n```bash\n# Run THIRD — verify tools work together as a pipeline\n# Simulate the full workflow with sample data\n\n# Step 1: Run first tool\npython tools/{tool_1}.py --input \"{sample}\" > /tmp/step1_output.json\n\n# Step 2: Feed output to next tool\npython tools/{tool_2}.py --input /tmp/step1_output.json > /tmp/step2_output.json\n\n# Step 3: Verify final output\npython -c \"\nimport json\nresult = json.load(open('/tmp/step2_output.json'))\nassert '{expected_key}' in result, 'Missing expected key in output'\nprint('Integration test passed')\n\"\n\n# Verify workflow.md references match actual tool files\n# Verify CLAUDE.md documents all tools and subagents\n# Verify .github/workflows/ YAML is valid\n```\n\n---\n\n## Final Validation Checklist\n- [ ] All tools pass Level 1 (syntax, imports, structure)\n- [ ] All tools pass Level 2 (unit tests with sample data)\n- [ ] Pipeline passes Level 3 (integration test end-to-end)\n- [ ] workflow.md has failure modes and fallbacks for every step\n- [ ] CLAUDE.md documents all tools, subagents, MCPs, and secrets\n- [ ] .github/workflows/ has timeout-minutes and failure notifications\n- [ ] .env.example lists all required environment variables\n- [ ] .gitignore excludes .env, __pycache__/, credentials\n- [ ] README.md covers all three execution paths (CLI, Actions, Agent HQ)\n- [ ] No hardcoded secrets anywhere in the codebase\n- [ ] Subagent files have valid YAML frontmatter and specific system prompts\n- [ ] requirements.txt lists all Python dependencies\n\n---\n\n## Anti-Patterns to Avoid\n- Do not hardcode API keys, tokens, or credentials — use GitHub Secrets or .env\n- Do not use `git add -A` or `git add .` — stage only specific files\n- Do not skip validation because \"it should work\" — run all three levels\n- Do not catch bare `except:` — always catch specific exception types\n- Do not build tools that require interactive input — all tools must run unattended\n- Do not create MCP-dependent tools without HTTP/API fallbacks\n- Do not design subagents that call other subagents — only the main agent delegates\n- Do not use Agent Teams when fewer than 3 independent tasks exist — the overhead is not justified\n- Do not commit .env files, credentials, or API keys to the repository\n- Do not ignore failing tests — fix the root cause, never mock to pass\n- Do not generate workflow steps without failure modes and fallback actions\n- Do not write tools without try/except, logging, type hints, and a main() function\n\n---\n\n## Confidence Score: {N}/10\n\n**Score rationale:**\n- [{Area}]: {Why confident or uncertain} — Confidence: {high | medium | low}\n- [{Area}]: {Rationale} — Confidence: {high | medium | low}\n- [{Area}]: {Rationale} — Confidence: {high | medium | low}\n\n**Ambiguity flags** (areas requiring clarification before building):\n- [ ] {Ambiguous requirement 1 — what needs clarification and why}\n- [ ] {Ambiguous requirement 2 — what needs clarification and why}\n\n**If any ambiguity flag is checked, DO NOT proceed to build. Ask the user to clarify first.**\n\n---\n\n## Factory Build Prompt\n\nWhen this PRP is ready, execute the build with:\n\n```\n/execute-prp PRPs/{system-name}.md\n```\n\nThis will trigger `factory/workflow.md` with the above requirements as structured input.\n"}],"isError":false,"timestamp":1770815908102}}
{"type":"message","id":"98d97d57","parentId":"f2936a0b","timestamp":"2026-02-11T13:21:18.728Z","message":{"role":"assistant","content":[{"type":"thinking","thinking":"Perfect. Now I have all the information I need:\n1. CLAUDE.md - WAT Factory Bot documentation\n2. factory/workflow.md - The factory's build process\n3. library/patterns.md - Proven workflow patterns\n4. library/tool_catalog.md - Reusable tool patterns\n5. config/mcp_registry.md - Available MCPs\n6. PRPs/templates/prp_base.md - PRP template structure\n\nNow I need to generate a comprehensive PRP for the content-repurposer system. Let me analyze the problem:\n\n**Problem description:**\n- Take a blog post URL as input\n- Extract content from the blog post\n- Generate platform-specific content:\n  - Twitter thread\n  - LinkedIn post\n  - Email newsletter section\n  - Instagram caption\n- Each optimized for platform format and character limits\n- Analyze tone of source content\n- Match style per platform\n- Suggest hashtags and mentions per platform\n- Save all outputs to a single JSON file with one key per platform\n\nLet me map this to WAT patterns:\n1. This is an **Intake > Enrich > Deliver** pattern - taking raw input (URL), enriching it (extracting content, analyzing tone, generating platform-specific versions), and delivering structured output (JSON file)\n2. Could also be seen as **Scrape > Process > Output** since we're scraping the blog post, processing it with AI, and outputting multiple formats\n\nSubagents needed:\n- content-extractor (scraping/extraction specialist)\n- tone-analyzer (analyzes source content tone)\n- platform-writer (generates platform-specific content) - could be 4 separate specialists or one multi-platform specialist\n\nAgent Teams analysis:\n- Once we have the extracted content and tone analysis, we have 4 independent content generation tasks (Twitter, LinkedIn, Email, Instagram)\n- These have no data dependencies on each other\n- This is a perfect 3+ Independent Tasks scenario - 4 parallel writes\n- Each generation is 5-10 seconds with Claude\n\n\n- Running sequentially takes ~40 seconds, but parallelizing cuts that to ~10 seconds—a significant improvement\n\nI'll need Firecrawl or a fetch tool for scraping the blog post (with a BeautifulSoup fallback), the Anthropic API for tone analysis and generation, and filesystem access to write the JSON output. For secrets, I should plan for FIRECRAWL_API_KEY as optional and ANTHROPIC_API_KEY as required. The core tools will be extract_content.py for scraping, analyze_tone.py for style detection, and generate_twitter.py for platform-specific content creation.\n\nRather than building separate generators for each platform, I could consolidate into a single generate_platform_content.py that handles Twitter, LinkedIn, email, and Instagram—either parallelizing internally or being called four times. Then package_output.py merges everything into the final JSON. Alternatively, I could use an agent-based approach where a team lead coordinates four specialized teammates, each responsible for one platform's content generation, which would naturally parallelize the work.","thinkingSignature":"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"},{"type":"text","text":"Now let me generate the comprehensive PRP for the content-repurposer system:"},{"type":"toolCall","id":"toolu_013E9MEf2XseFPqYHKoGGEwR","name":"write","arguments":{"path":"PRPs/content-repurposer.md","content":"name: \"content-repurposer\"\ndescription: |\n  Multi-channel content repurposing system that transforms blog posts into platform-optimized social media and email content with automatic tone matching and hashtag suggestions.\n\n## Purpose\nWAT System PRP (Product Requirements Prompt) — a structured blueprint that gives the factory enough context to build a complete, working system in one pass.\n\n## Core Principles\n1. **Context is King**: Include ALL necessary documentation, references, and caveats\n2. **Validation Loops**: Provide executable checks the factory can run at each gate\n3. **Information Dense**: Use keywords and patterns from the WAT framework\n4. **Progressive Success**: Start simple, validate, then enhance\n5. **Global rules**: Follow all rules in CLAUDE.md\n\n---\n\n## Goal\nBuild an autonomous content repurposing system that takes a single blog post URL and generates optimized content for four platforms (Twitter, LinkedIn, Email, Instagram) in one execution. The system analyzes the source content's tone and style, then generates platform-specific versions that match the original voice while adhering to each platform's format constraints and best practices. All outputs are packaged into a single JSON file committed to the repository.\n\nSuccess means a marketing team can drop a blog URL into the system and receive ready-to-use content for all four channels in under 60 seconds, with consistent brand voice and platform-appropriate formatting.\n\n## Why\n- **Reduces manual work**: Eliminates 2-3 hours of manual content adaptation per blog post\n- **Maintains consistency**: Ensures brand voice carries across all platforms with automated tone matching\n- **Scales content reach**: Every blog post becomes 4+ pieces of social/email content automatically\n- **Marketing teams benefit**: Content marketers, social media managers, and email campaign coordinators use this system 2-5 times per week per blog post published\n\n## What\nThe system accepts a blog post URL, extracts the core content and key points, analyzes the writing tone (professional, casual, technical, conversational, etc.), and generates four platform-optimized outputs:\n\n1. **Twitter Thread**: 3-7 tweets optimized for engagement, with thread structure and relevant hashtags\n2. **LinkedIn Post**: Professional-length post (1300 char limit) with paragraph breaks and platform-appropriate hashtags\n3. **Email Newsletter Section**: Formatted section ready to paste into a newsletter with subject line suggestion\n4. **Instagram Caption**: Visual-first caption (2200 char limit) with emoji usage and Instagram hashtags\n\nEach output includes:\n- Platform-optimized formatting\n- Tone-matched content (mirrors source blog's style)\n- Suggested hashtags (3-5 relevant, trending-aware)\n- Mention suggestions where appropriate (@-mentions for Twitter/LinkedIn)\n- Character/length validation (no platform limit violations)\n\n### Success Criteria\n- [ ] System extracts blog content from any standard blog platform URL (WordPress, Medium, Ghost, Substack, custom sites)\n- [ ] Tone analysis correctly identifies and matches writing style (validated against 10 sample blogs)\n- [ ] All four platform outputs are generated with correct formatting and character limits\n- [ ] JSON output file contains all four platforms with consistent schema\n- [ ] Suggested hashtags are relevant to content (not generic marketing hashtags)\n- [ ] System runs autonomously via GitHub Actions on schedule or manual trigger\n- [ ] Results are committed back to repo in `output/` directory\n- [ ] All three execution paths work: CLI, GitHub Actions, Agent HQ\n- [ ] Parallel generation reduces total time to under 60 seconds for all four platforms\n\n---\n\n## Inputs\nWhat goes into the system. Be specific about format, source, and any validation requirements.\n\n```yaml\n- name: \"blog_url\"\n  type: \"string (URL)\"\n  source: \"manual input via workflow_dispatch or Agent HQ issue\"\n  required: true\n  description: \"Full URL to a blog post. Must be publicly accessible (no auth walls).\"\n  example: \"https://example.com/blog/how-to-scale-content-marketing\"\n\n- name: \"output_filename\"\n  type: \"string\"\n  source: \"manual input (optional, defaults to sanitized blog title)\"\n  required: false\n  description: \"Filename for the output JSON (without .json extension). Defaults to blog post slug.\"\n  example: \"how-to-scale-content-marketing\"\n\n- name: \"brand_voice_guidelines\"\n  type: \"string (file path or text)\"\n  source: \"optional config file at config/brand_voice.md\"\n  required: false\n  description: \"Optional brand voice guidelines to influence tone matching. If not provided, mirrors source blog exactly.\"\n  example: \"config/brand_voice.md\"\n```\n\n## Outputs\nWhat comes out of the system. Where do results go?\n\n```yaml\n- name: \"repurposed_content.json\"\n  type: \"JSON\"\n  destination: \"repo commit at output/{sanitized-title}-repurposed.json\"\n  description: \"Single JSON file containing all four platform outputs with metadata\"\n  example: |\n    {\n      \"metadata\": {\n        \"source_url\": \"https://example.com/blog/post\",\n        \"generated_at\": \"2026-02-11T13:00:00Z\",\n        \"tone_analysis\": {\n          \"primary_tone\": \"professional\",\n          \"secondary_tone\": \"conversational\",\n          \"confidence\": 0.89\n        }\n      },\n      \"twitter\": {\n        \"thread\": [\n          \"Tweet 1 text here...\",\n          \"Tweet 2 text here...\",\n          \"Tweet 3 text here...\"\n        ],\n        \"hashtags\": [\"#ContentMarketing\", \"#MarketingTips\", \"#B2B\"],\n        \"mentions\": [\"@relevantaccount\"],\n        \"character_count\": 850\n      },\n      \"linkedin\": {\n        \"post\": \"LinkedIn post text here with\\n\\nparagraph breaks...\",\n        \"hashtags\": [\"#ContentStrategy\", \"#MarketingAutomation\"],\n        \"character_count\": 1150\n      },\n      \"email\": {\n        \"subject\": \"Suggested subject line\",\n        \"section\": \"Email-formatted content here...\",\n        \"word_count\": 250\n      },\n      \"instagram\": {\n        \"caption\": \"Instagram caption with emojis 🚀...\",\n        \"hashtags\": [\"#Marketing\", \"#ContentCreation\", \"#DigitalMarketing\"],\n        \"character_count\": 1800\n      }\n    }\n\n- name: \"generation_log.json\"\n  type: \"JSON\"\n  destination: \"repo commit at logs/{timestamp}-generation.json\"\n  description: \"Machine-readable log of generation process with timing and token usage\"\n  example: |\n    {\n      \"source_url\": \"https://example.com/blog/post\",\n      \"extraction_time_ms\": 2341,\n      \"tone_analysis_time_ms\": 1205,\n      \"generation_time_ms\": 8742,\n      \"total_time_ms\": 12288,\n      \"token_usage\": {\n        \"input_tokens\": 4521,\n        \"output_tokens\": 2103\n      },\n      \"parallel_execution\": true\n    }\n```\n\n---\n\n## All Needed Context\n\n### Documentation & References\n```yaml\n# MUST READ — Include these in context when building\n- url: \"https://developer.twitter.com/en/docs/twitter-api/v2/tweets\"\n  why: \"Twitter character limits, thread structure, hashtag best practices\"\n\n- url: \"https://www.linkedin.com/help/linkedin/answer/a521928\"\n  why: \"LinkedIn post character limits (1300 for main text), formatting guidelines\"\n\n- url: \"https://help.instagram.com/1631821640426723\"\n  why: \"Instagram caption limits (2200 chars), hashtag recommendations (3-30 tags)\"\n\n- doc: \"config/mcp_registry.md\"\n  why: \"Check Firecrawl for web scraping, Anthropic for content generation\"\n\n- doc: \"library/patterns.md\"\n  why: \"Use Intake > Enrich > Deliver + Fan-Out > Process > Merge for parallel generation\"\n\n- doc: \"library/tool_catalog.md\"\n  why: \"Reuse firecrawl_scrape, llm_prompt, structured_extract patterns\"\n\n- url: \"https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/tone-matching\"\n  why: \"Best practices for tone analysis and style matching with Claude\"\n```\n\n### Workflow Pattern Selection\n```yaml\npattern: \"Intake > Enrich > Deliver + Fan-Out > Process > Merge\"\nrationale: |\n  Primary pattern is Intake > Enrich > Deliver:\n  - Intake: Receive blog URL and extract content\n  - Enrich: Analyze tone, generate platform-specific versions\n  - Deliver: Package into JSON and commit\n\n  Fan-Out > Process > Merge applies to the content generation phase:\n  - Once we have extracted content + tone analysis, we have 4 independent generation tasks\n  - Each platform can be generated in parallel (no data dependencies)\n  - Team lead merges all outputs into single JSON\n\nmodifications: |\n  Standard Intake > Enrich > Deliver assumes sequential enrichment.\n  We modify the \"Enrich\" phase to use Agent Teams for parallel generation of the 4 platform outputs.\n  This reduces wall time from ~40s (sequential) to ~10s (parallel).\n```\n\n### MCP & Tool Requirements\n```yaml\ncapabilities:\n  - name: \"web content extraction\"\n    primary_mcp: \"firecrawl\"\n    alternative_mcp: \"fetch\"\n    fallback: \"direct HTTP with requests + BeautifulSoup4 for HTML parsing\"\n    secret_name: \"FIRECRAWL_API_KEY\"\n\n  - name: \"tone analysis\"\n    primary_mcp: \"anthropic\"\n    alternative_mcp: \"none\"\n    fallback: \"none - Anthropic API is required\"\n    secret_name: \"ANTHROPIC_API_KEY\"\n\n  - name: \"content generation\"\n    primary_mcp: \"anthropic\"\n    alternative_mcp: \"none\"\n    fallback: \"none - Anthropic API is required\"\n    secret_name: \"ANTHROPIC_API_KEY\"\n\n  - name: \"file output\"\n    primary_mcp: \"filesystem\"\n    alternative_mcp: \"none\"\n    fallback: \"Python pathlib + json.dump()\"\n    secret_name: \"none\"\n```\n\n### Known Gotchas & Constraints\n```\n# CRITICAL: Twitter thread limit is 280 chars per tweet, but threads can have 25 tweets max\n# CRITICAL: LinkedIn post limit is 3000 chars total but 1300 chars before \"see more\" fold — optimize for 1300\n# CRITICAL: Instagram supports 2200 chars but captions are better at 1000-1500 for engagement\n# CRITICAL: Email section should be 150-300 words — scannable length for newsletter inclusion\n# CRITICAL: Firecrawl API may fail on JavaScript-heavy sites — fallback to direct fetch + BeautifulSoup\n# CRITICAL: Some blogs have paywalls or anti-scraping — tool must detect and fail gracefully with clear error\n# CRITICAL: Tone analysis confidence below 0.7 should trigger a warning but not block generation\n# CRITICAL: Hashtag suggestions must avoid banned/spam hashtags on each platform\n# CRITICAL: Secrets are NEVER hardcoded — always use GitHub Secrets or .env\n# CRITICAL: Every tool must have try/except, logging, type hints, and a main() function\n# CRITICAL: Agent Teams requires CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=true in environment\n# CRITICAL: Character counts must be validated BEFORE committing output — no platform limit violations\n# CRITICAL: Brand voice guidelines are optional — system must work without them (mirrors source tone only)\n```\n\n---\n\n## System Design\n\n### Subagent Architecture\nDefine the specialist subagents this system needs. One subagent per major capability or workflow phase.\n\n```yaml\nsubagents:\n  - name: \"content-extractor\"\n    description: \"Extract and clean blog post content from any URL. Delegate when you need to scrape a blog post, extract markdown, or handle content from various blog platforms.\"\n    tools: \"Read, Bash, Grep\"\n    model: \"sonnet\"\n    permissionMode: \"default\"\n    responsibilities:\n      - \"Scrape blog post URL using Firecrawl or fallback methods\"\n      - \"Extract clean markdown content (title, author, date, body)\"\n      - \"Handle extraction errors gracefully (paywalls, 404s, anti-scraping)\"\n      - \"Return structured content object with metadata\"\n    inputs: \"blog_url (string)\"\n    outputs: \"dict with keys: title, author, publish_date, content_markdown, word_count, extract_method\"\n\n  - name: \"tone-analyzer\"\n    description: \"Analyze the tone, style, and voice of written content. Delegate when you need to understand the writing style of source content before adapting it.\"\n    tools: \"Read, Bash\"\n    model: \"sonnet\"\n    permissionMode: \"default\"\n    responsibilities:\n      - \"Analyze source content for tone dimensions (professional, casual, technical, emotional, etc.)\"\n      - \"Identify key stylistic elements (sentence length, vocabulary level, rhetorical devices)\"\n      - \"Generate a tone profile with confidence score\"\n      - \"Flag any brand voice guideline conflicts if provided\"\n    inputs: \"content_markdown (string), brand_voice_guidelines (optional string)\"\n    outputs: \"dict with keys: primary_tone, secondary_tone, style_elements (list), confidence (float 0-1), voice_notes\"\n\n  - name: \"platform-content-generator\"\n    description: \"Generate platform-specific content matching a given tone profile. Delegate when you need to write content optimized for a specific social media or email platform.\"\n    tools: \"Read, Bash\"\n    model: \"sonnet\"\n    permissionMode: \"default\"\n    responsibilities:\n      - \"Generate Twitter thread (3-7 tweets) with hashtags and mentions\"\n      - \"Generate LinkedIn post (1300 char optimized) with professional formatting\"\n      - \"Generate email newsletter section with subject line\"\n      - \"Generate Instagram caption with emoji usage and hashtags\"\n      - \"Match tone profile from analyzer in all outputs\"\n      - \"Validate character limits for each platform\"\n      - \"Suggest relevant, non-generic hashtags\"\n    inputs: \"content_markdown (string), tone_profile (dict), platform (string: twitter|linkedin|email|instagram)\"\n    outputs: \"dict with platform-specific keys (thread/post/section/caption, hashtags, mentions, character_count)\"\n\n  - name: \"output-packager\"\n    description: \"Package multiple platform outputs into a single structured JSON file. Delegate when you need to merge results and write final output.\"\n    tools: \"Read, Write, Bash\"\n    model: \"haiku\"\n    permissionMode: \"default\"\n    responsibilities:\n      - \"Collect outputs from all platform generators\"\n      - \"Validate schema consistency\"\n      - \"Add metadata (source URL, timestamp, tone analysis)\"\n      - \"Write to JSON file with proper formatting\"\n      - \"Generate machine-readable log file\"\n    inputs: \"source_url (string), tone_profile (dict), platform_outputs (dict of dicts), output_path (string)\"\n    outputs: \"dict with keys: output_file_path, log_file_path, success (bool)\"\n```\n\n### Agent Teams Analysis\n```yaml\n# Apply the 3+ Independent Tasks Rule\nindependent_tasks:\n  - \"Generate Twitter thread from source content + tone profile\"\n  - \"Generate LinkedIn post from source content + tone profile\"\n  - \"Generate email newsletter section from source content + tone profile\"\n  - \"Generate Instagram caption from source content + tone profile\"\n\nindependent_task_count: 4\nrecommendation: \"Use Agent Teams\"\nrationale: |\n  We have 4 truly independent content generation tasks with no data dependencies on each other.\n  Each task receives the same inputs (content_markdown + tone_profile) and produces independent outputs.\n  Each generation takes ~8-12 seconds with Claude Sonnet.\n  Sequential execution: 40-48 seconds. Parallel execution: ~12 seconds (longest single task).\n  Speedup: 3-4x reduction in wall time. This is a meaningful user experience improvement (under 60s total vs 90s+).\n  Merge complexity is trivial — just collect 4 JSON objects into a dict.\n\n# Agent Teams configuration:\nteam_lead_responsibilities:\n  - \"Create shared task list with 4 platform generation tasks\"\n  - \"Spawn 4 teammates (one per platform) with identical content_markdown + tone_profile inputs\"\n  - \"Collect completed outputs from all teammates\"\n  - \"Validate character counts and schema for each platform\"\n  - \"Merge into single JSON structure\"\n  - \"Delegate to output-packager subagent for final file write\"\n\nteammates:\n  - name: \"twitter-generator\"\n    task: \"Generate Twitter thread from source content matching tone profile. Return JSON with thread array, hashtags, mentions, character_count.\"\n    inputs: \"content_markdown (string), tone_profile (dict)\"\n    outputs: \"dict with keys: thread (list[str]), hashtags (list[str]), mentions (list[str]), character_count (int)\"\n\n  - name: \"linkedin-generator\"\n    task: \"Generate LinkedIn post from source content matching tone profile. Return JSON with post text, hashtags, character_count. Optimize for 1300 char limit.\"\n    inputs: \"content_markdown (string), tone_profile (dict)\"\n    outputs: \"dict with keys: post (str), hashtags (list[str]), character_count (int)\"\n\n  - name: \"email-generator\"\n    task: \"Generate email newsletter section from source content matching tone profile. Return JSON with subject, section text, word_count.\"\n    inputs: \"content_markdown (string), tone_profile (dict)\"\n    outputs: \"dict with keys: subject (str), section (str), word_count (int)\"\n\n  - name: \"instagram-generator\"\n    task: \"Generate Instagram caption from source content matching tone profile. Return JSON with caption, hashtags, character_count. Include emoji usage.\"\n    inputs: \"content_markdown (string), tone_profile (dict)\"\n    outputs: \"dict with keys: caption (str), hashtags (list[str]), character_count (int)\"\n\n# Sequential fallback:\nsequential_rationale: \"If Agent Teams is not enabled (CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS != true), execute platform generation sequentially by delegating to platform-content-generator subagent 4 times. Total time increases to ~40-50s but output is identical.\"\n```\n\n### GitHub Actions Triggers\n```yaml\ntriggers:\n  - type: \"workflow_dispatch\"\n    config: |\n      inputs:\n        blog_url:\n          description: 'Blog post URL to repurpose'\n          required: true\n          type: string\n        output_filename:\n          description: 'Output filename (optional, defaults to blog slug)'\n          required: false\n          type: string\n    description: \"Manual trigger for on-demand content repurposing. Marketing team member pastes blog URL and clicks Run Workflow.\"\n\n  - type: \"schedule\"\n    config: \"cron: '0 10 * * 1'  # Every Monday at 10 AM UTC\"\n    description: \"Optional scheduled run to process a queue of blog URLs (requires separate queue management tool). Disabled by default — enable if batch processing is needed.\"\n\n  - type: \"repository_dispatch\"\n    config: \"event_type: repurpose_content\"\n    description: \"Webhook trigger for integration with CMS publishing workflows. When a new blog post is published, CMS sends webhook to trigger repurposing.\"\n```\n\n---\n\n## Implementation Blueprint\n\n### Workflow Steps\nOrdered list of workflow phases. Each step becomes a section in workflow.md.\n\n```yaml\nsteps:\n  - name: \"Extract Blog Content\"\n    description: \"Scrape the blog post URL and extract clean markdown content with metadata.\"\n    subagent: \"content-extractor\"\n    tools: [\"extract_content.py\"]\n    inputs: \"blog_url (string from workflow input)\"\n    outputs: \"content_object (dict): title, author, publish_date, content_markdown, word_count, extract_method\"\n    failure_mode: \"URL is unreachable, paywall detected, anti-scraping blocks, invalid URL format\"\n    fallback: \"Try Firecrawl first. If fails, fall back to direct HTTP + BeautifulSoup. If both fail, return error with clear message indicating paywall or scraping block.\"\n\n  - name: \"Analyze Content Tone\"\n    description: \"Analyze the extracted content to identify tone, style, and voice characteristics.\"\n    subagent: \"tone-analyzer\"\n    tools: [\"analyze_tone.py\"]\n    inputs: \"content_markdown (from previous step), brand_voice_guidelines (optional from config/brand_voice.md)\"\n    outputs: \"tone_profile (dict): primary_tone, secondary_tone, style_elements, confidence, voice_notes\"\n    failure_mode: \"Content is too short (<100 words) for accurate tone analysis\"\n    fallback: \"If content is very short, use a simplified tone profile with 'conversational' as default and confidence=0.5. Log warning but continue.\"\n\n  - name: \"Generate Platform Content (Parallel)\"\n    description: \"Generate optimized content for all four platforms in parallel using Agent Teams.\"\n    subagent: \"none (Team Lead orchestrates 4 teammates)\"\n    tools: [\"generate_twitter.py\", \"generate_linkedin.py\", \"generate_email.py\", \"generate_instagram.py\"]\n    inputs: \"content_markdown (string), tone_profile (dict)\"\n    outputs: \"platform_outputs (dict): twitter (dict), linkedin (dict), email (dict), instagram (dict)\"\n    failure_mode: \"Claude API rate limit, one or more generations fail, character limit violations\"\n    fallback: \"Retry failed generations once with 5s delay. If any platform still fails after retry, include error message in that platform's output but continue with others. Never fail the entire pipeline because one platform failed.\"\n\n  - name: \"Package and Write Output\"\n    description: \"Merge all platform outputs into a single JSON file and write to repository.\"\n    subagent: \"output-packager\"\n    tools: [\"package_output.py\"]\n    inputs: \"source_url (string), tone_profile (dict), platform_outputs (dict), output_filename (string)\"\n    outputs: \"output/{{sanitized-filename}}-repurposed.json, logs/{{timestamp}}-generation.json\"\n    failure_mode: \"File write fails, invalid JSON structure, disk space issues\"\n    fallback: \"Validate JSON structure before write. If write fails, log to stdout and exit with error code 1. GitHub Actions will capture stdout for debugging.\"\n\n  - name: \"Commit Results\"\n    description: \"Commit the output JSON and generation log to the repository.\"\n    subagent: \"none (handled by main agent)\"\n    tools: [\"git CLI\"]\n    inputs: \"output/{{filename}}-repurposed.json, logs/{{timestamp}}-generation.json\"\n    outputs: \"Git commit with message 'feat: repurposed content for {{blog-title}}'\"\n    failure_mode: \"Git conflicts, authentication failure, branch protection\"\n    fallback: \"Use git pull --rebase before commit to handle concurrent writes. If commit fails, output files are still written to disk for manual recovery.\"\n```\n\n### Tool Specifications\nFor each tool the system needs. Reference library/tool_catalog.md for reusable patterns.\n\n```yaml\ntools:\n  - name: \"extract_content.py\"\n    purpose: \"Scrape blog post URL and return clean markdown content\"\n    catalog_pattern: \"firecrawl_scrape with HTTP + BeautifulSoup fallback\"\n    inputs:\n      - \"url: str — Blog post URL to scrape\"\n    outputs: \"JSON dict: {title, author, publish_date, content_markdown, word_count, extract_method}\"\n    dependencies: [\"firecrawl-py\", \"httpx\", \"beautifulsoup4\", \"markdownify\"]\n    mcp_used: \"firecrawl (optional)\"\n    error_handling: \"try Firecrawl first, catch HTTPError and fall back to requests + BeautifulSoup, raise ValueError if both fail with clear error message\"\n\n  - name: \"analyze_tone.py\"\n    purpose: \"Analyze content tone and style using Claude\"\n    catalog_pattern: \"structured_extract with JSON schema for tone profile\"\n    inputs:\n      - \"content_markdown: str — Markdown text to analyze\"\n      - \"brand_voice_guidelines: str (optional) — Brand voice reference text\"\n    outputs: \"JSON dict: {primary_tone, secondary_tone, style_elements, confidence, voice_notes}\"\n    dependencies: [\"anthropic\"]\n    mcp_used: \"anthropic\"\n    error_handling: \"retry once with 5s delay on API errors, if content < 100 words return simplified profile with warning\"\n\n  - name: \"generate_twitter.py\"\n    purpose: \"Generate Twitter thread with hashtags and mentions\"\n    catalog_pattern: \"llm_prompt with structured output validation\"\n    inputs:\n      - \"content_markdown: str — Source content\"\n      - \"tone_profile: dict — Tone analysis from analyzer\"\n    outputs: \"JSON dict: {thread: list[str], hashtags: list[str], mentions: list[str], character_count: int}\"\n    dependencies: [\"anthropic\"]\n    mcp_used: \"anthropic\"\n    error_handling: \"validate each tweet is <=280 chars, retry generation if any tweet exceeds limit, include error key in output if retries exhausted\"\n\n  - name: \"generate_linkedin.py\"\n    purpose: \"Generate LinkedIn post optimized for 1300 char limit\"\n    catalog_pattern: \"llm_prompt with structured output validation\"\n    inputs:\n      - \"content_markdown: str — Source content\"\n      - \"tone_profile: dict — Tone analysis\"\n    outputs: \"JSON dict: {post: str, hashtags: list[str], character_count: int}\"\n    dependencies: [\"anthropic\"]\n    mcp_used: \"anthropic\"\n    error_handling: \"validate post is <=1300 chars before 'see more' fold, retry if exceeded, include error key if retries exhausted\"\n\n  - name: \"generate_email.py\"\n    purpose: \"Generate email newsletter section with subject line\"\n    catalog_pattern: \"llm_prompt with structured output validation\"\n    inputs:\n      - \"content_markdown: str — Source content\"\n      - \"tone_profile: dict — Tone analysis\"\n    outputs: \"JSON dict: {subject: str, section: str, word_count: int}\"\n    dependencies: [\"anthropic\"]\n    mcp_used: \"anthropic\"\n    error_handling: \"target 150-300 words, validate word count, retry if far outside range, include error key if retries exhausted\"\n\n  - name: \"generate_instagram.py\"\n    purpose: \"Generate Instagram caption with emojis and hashtags\"\n    catalog_pattern: \"llm_prompt with structured output validation\"\n    inputs:\n      - \"content_markdown: str — Source content\"\n      - \"tone_profile: dict — Tone analysis\"\n    outputs: \"JSON dict: {caption: str, hashtags: list[str], character_count: int}\"\n    dependencies: [\"anthropic\"]\n    mcp_used: \"anthropic\"\n    error_handling: \"validate caption is <=2200 chars, validate 3-30 hashtags, retry if limits exceeded, include error key if retries exhausted\"\n\n  - name: \"package_output.py\"\n    purpose: \"Merge all platform outputs into single JSON file\"\n    catalog_pattern: \"json_read_write with schema validation\"\n    inputs:\n      - \"source_url: str — Original blog URL\"\n      - \"tone_profile: dict — Tone analysis result\"\n      - \"platform_outputs: dict — All four platform dicts\"\n      - \"output_path: str — Where to write JSON file\"\n    outputs: \"JSON dict: {output_file_path: str, log_file_path: str, success: bool}\"\n    dependencies: [\"none (stdlib json + pathlib)\"]\n    mcp_used: \"none\"\n    error_handling: \"validate JSON structure matches schema before write, log validation errors, exit 1 if write fails\"\n```\n\n### Per-Tool Pseudocode\n```python\n# extract_content.py\ndef main():\n    # PATTERN: firecrawl_scrape with HTTP fallback\n    # Step 1: Parse input URL\n    args = parse_args()  # argparse: --url\n    url = args.url\n\n    # Step 2: Try Firecrawl first\n    try:\n        result = firecrawl_scrape(url)\n        return {\n            \"title\": result.metadata.get(\"title\"),\n            \"author\": result.metadata.get(\"author\"),\n            \"publish_date\": result.metadata.get(\"date\"),\n            \"content_markdown\": result.markdown,\n            \"word_count\": len(result.markdown.split()),\n            \"extract_method\": \"firecrawl\"\n        }\n    except Exception as e:\n        logging.warning(f\"Firecrawl failed: {e}, trying HTTP fallback\")\n\n    # Step 3: Fallback to HTTP + BeautifulSoup\n    # CRITICAL: Handle paywalls, 404s, anti-scraping\n    try:\n        resp = httpx.get(url, timeout=15, headers={\"User-Agent\": \"...\"})\n        resp.raise_for_status()\n        soup = BeautifulSoup(resp.text, \"html.parser\")\n        # Extract article content (heuristic: <article>, <main>, or largest <div>)\n        content = extract_article_content(soup)\n        markdown = markdownify.markdownify(content)\n        return {\n            \"title\": soup.find(\"h1\").text,\n            \"content_markdown\": markdown,\n            \"word_count\": len(markdown.split()),\n            \"extract_method\": \"http+beautifulsoup\"\n        }\n    except Exception as e:\n        raise ValueError(f\"Content extraction failed: {e}\")\n\n# analyze_tone.py\ndef main():\n    # PATTERN: structured_extract with tone analysis schema\n    # CRITICAL: Return confidence score, handle short content\n    args = parse_args()  # --content, --brand-voice (optional)\n    content = args.content\n\n    if len(content.split()) < 100:\n        logging.warning(\"Content too short for accurate tone analysis\")\n        return {\n            \"primary_tone\": \"conversational\",\n            \"secondary_tone\": \"neutral\",\n            \"style_elements\": [\"brief\"],\n            \"confidence\": 0.5,\n            \"voice_notes\": \"Content too short for detailed analysis\"\n        }\n\n    # Use Claude to analyze tone\n    schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"primary_tone\": {\"type\": \"string\"},\n            \"secondary_tone\": {\"type\": \"string\"},\n            \"style_elements\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n            \"confidence\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1}\n        }\n    }\n    result = structured_extract(\n        content=content,\n        schema=schema,\n        instructions=\"Analyze the tone and style of this content...\"\n    )\n    return result.data\n\n# generate_twitter.py\ndef main():\n    # PATTERN: llm_prompt with Twitter thread generation\n    # CRITICAL: Validate 280 char per tweet, 3-7 tweets total\n    args = parse_args()  # --content, --tone-profile\n    content = args.content\n    tone = json.loads(args.tone_profile)\n\n    prompt = f\"\"\"\n    Convert this blog content into a Twitter thread matching the tone: {tone['primary_tone']}.\n    Rules:\n    - 3-7 tweets maximum\n    - Each tweet <=280 characters\n    - Include 3-5 relevant hashtags\n    - Suggest @mentions if appropriate\n    - Match the tone: {tone['voice_notes']}\n\n    Content:\n    {content}\n    \"\"\"\n\n    result = llm_prompt(\n        prompt=prompt,\n        system=\"You are a social media content strategist...\",\n        temperature=0.7\n    )\n\n    # Validate and structure\n    thread = parse_thread(result.text)\n    validate_twitter_limits(thread)  # raises if any tweet > 280 chars\n\n    return {\n        \"thread\": thread,\n        \"hashtags\": extract_hashtags(result.text),\n        \"mentions\": extract_mentions(result.text),\n        \"character_count\": sum(len(t) for t in thread)\n    }\n\n# generate_linkedin.py (similar pattern to Twitter)\n# generate_email.py (similar pattern)\n# generate_instagram.py (similar pattern)\n\n# package_output.py\ndef main():\n    # PATTERN: json_read_write with schema validation\n    args = parse_args()  # --source-url, --tone-profile, --twitter, --linkedin, --email, --instagram, --output-path\n    \n    output = {\n        \"metadata\": {\n            \"source_url\": args.source_url,\n            \"generated_at\": datetime.utcnow().isoformat(),\n            \"tone_analysis\": json.loads(args.tone_profile)\n        },\n        \"twitter\": json.loads(args.twitter),\n        \"linkedin\": json.loads(args.linkedin),\n        \"email\": json.loads(args.email),\n        \"instagram\": json.loads(args.instagram)\n    }\n\n    # Validate schema\n    validate_output_schema(output)\n\n    # Write to file\n    output_path = Path(args.output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    output_path.write_text(json.dumps(output, indent=2))\n\n    # Generate log file\n    log_path = Path(\"logs\") / f\"{datetime.now().isoformat()}-generation.json\"\n    # ... write log ...\n\n    return {\"output_file_path\": str(output_path), \"success\": True}\n```\n\n### Integration Points\n```yaml\nSECRETS:\n  - name: \"FIRECRAWL_API_KEY\"\n    purpose: \"Firecrawl API for web scraping (optional — has fallback)\"\n    required: false\n\n  - name: \"ANTHROPIC_API_KEY\"\n    purpose: \"Claude API for tone analysis and content generation\"\n    required: true\n\nENVIRONMENT:\n  - file: \".env.example\"\n    vars:\n      - \"FIRECRAWL_API_KEY=your_key_here  # Optional, for web scraping\"\n      - \"ANTHROPIC_API_KEY=your_key_here  # Required, for AI generation\"\n      - \"CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=true  # Required for parallel generation\"\n\nDEPENDENCIES:\n  - file: \"requirements.txt\"\n    packages:\n      - \"anthropic>=0.18.0  # Claude API\"\n      - \"firecrawl-py>=0.0.5  # Optional web scraping\"\n      - \"httpx>=0.26.0  # HTTP fallback\"\n      - \"beautifulsoup4>=4.12.0  # HTML parsing fallback\"\n      - \"markdownify>=0.11.6  # HTML to markdown conversion\"\n      - \"python-dateutil>=2.8.2  # Date parsing\"\n\nGITHUB_ACTIONS:\n  - trigger: \"workflow_dispatch\"\n    config: |\n      inputs:\n        blog_url: {required: true, type: string}\n        output_filename: {required: false, type: string}\n```\n\n---\n\n## Validation Loop\n\n### Level 1: Syntax & Structure\n```bash\n# Run FIRST — every tool must pass before proceeding to Level 2\n# AST parse — verify valid Python syntax\npython -c \"import ast; ast.parse(open('tools/extract_content.py').read())\"\npython -c \"import ast; ast.parse(open('tools/analyze_tone.py').read())\"\npython -c \"import ast; ast.parse(open('tools/generate_twitter.py').read())\"\npython -c \"import ast; ast.parse(open('tools/generate_linkedin.py').read())\"\npython -c \"import ast; ast.parse(open('tools/generate_email.py').read())\"\npython -c \"import ast; ast.parse(open('tools/generate_instagram.py').read())\"\npython -c \"import ast; ast.parse(open('tools/package_output.py').read())\"\n\n# Import check — verify no missing dependencies\npython -c \"import importlib; importlib.import_module('tools.extract_content')\"\npython -c \"import importlib; importlib.import_module('tools.analyze_tone')\"\npython -c \"import importlib; importlib.import_module('tools.generate_twitter')\"\npython -c \"import importlib; importlib.import_module('tools.generate_linkedin')\"\npython -c \"import importlib; importlib.import_module('tools.generate_email')\"\npython -c \"import importlib; importlib.import_module('tools.generate_instagram')\"\npython -c \"import importlib; importlib.import_module('tools.package_output')\"\n\n# Structure check — verify main() exists\npython -c \"from tools.extract_content import main; assert callable"},"partialJson":"{\"path\": \"PRPs/content-repurposer.md\", \"content\": \"name: \\\"content-repurposer\\\"\\ndescription: |\\n  Multi-channel content repurposing system that transforms blog posts into platform-optimized social media and email content with automatic tone matching and hashtag suggestions.\\n\\n## Purpose\\nWAT System PRP (Product Requirements Prompt) — a structured blueprint that gives the factory enough context to build a complete, working system in one pass.\\n\\n## Core Principles\\n1. **Context is King**: Include ALL necessary documentation, references, and caveats\\n2. **Validation Loops**: Provide executable checks the factory can run at each gate\\n3. **Information Dense**: Use keywords and patterns from the WAT framework\\n4. **Progressive Success**: Start simple, validate, then enhance\\n5. **Global rules**: Follow all rules in CLAUDE.md\\n\\n---\\n\\n## Goal\\nBuild an autonomous content repurposing system that takes a single blog post URL and generates optimized content for four platforms (Twitter, LinkedIn, Email, Instagram) in one execution. The system analyzes the source content's tone and style, then generates platform-specific versions that match the original voice while adhering to each platform's format constraints and best practices. All outputs are packaged into a single JSON file committed to the repository.\\n\\nSuccess means a marketing team can drop a blog URL into the system and receive ready-to-use content for all four channels in under 60 seconds, with consistent brand voice and platform-appropriate formatting.\\n\\n## Why\\n- **Reduces manual work**: Eliminates 2-3 hours of manual content adaptation per blog post\\n- **Maintains consistency**: Ensures brand voice carries across all platforms with automated tone matching\\n- **Scales content reach**: Every blog post becomes 4+ pieces of social/email content automatically\\n- **Marketing teams benefit**: Content marketers, social media managers, and email campaign coordinators use this system 2-5 times per week per blog post published\\n\\n## What\\nThe system accepts a blog post URL, extracts the core content and key points, analyzes the writing tone (professional, casual, technical, conversational, etc.), and generates four platform-optimized outputs:\\n\\n1. **Twitter Thread**: 3-7 tweets optimized for engagement, with thread structure and relevant hashtags\\n2. **LinkedIn Post**: Professional-length post (1300 char limit) with paragraph breaks and platform-appropriate hashtags\\n3. **Email Newsletter Section**: Formatted section ready to paste into a newsletter with subject line suggestion\\n4. **Instagram Caption**: Visual-first caption (2200 char limit) with emoji usage and Instagram hashtags\\n\\nEach output includes:\\n- Platform-optimized formatting\\n- Tone-matched content (mirrors source blog's style)\\n- Suggested hashtags (3-5 relevant, trending-aware)\\n- Mention suggestions where appropriate (@-mentions for Twitter/LinkedIn)\\n- Character/length validation (no platform limit violations)\\n\\n### Success Criteria\\n- [ ] System extracts blog content from any standard blog platform URL (WordPress, Medium, Ghost, Substack, custom sites)\\n- [ ] Tone analysis correctly identifies and matches writing style (validated against 10 sample blogs)\\n- [ ] All four platform outputs are generated with correct formatting and character limits\\n- [ ] JSON output file contains all four platforms with consistent schema\\n- [ ] Suggested hashtags are relevant to content (not generic marketing hashtags)\\n- [ ] System runs autonomously via GitHub Actions on schedule or manual trigger\\n- [ ] Results are committed back to repo in `output/` directory\\n- [ ] All three execution paths work: CLI, GitHub Actions, Agent HQ\\n- [ ] Parallel generation reduces total time to under 60 seconds for all four platforms\\n\\n---\\n\\n## Inputs\\nWhat goes into the system. Be specific about format, source, and any validation requirements.\\n\\n```yaml\\n- name: \\\"blog_url\\\"\\n  type: \\\"string (URL)\\\"\\n  source: \\\"manual input via workflow_dispatch or Agent HQ issue\\\"\\n  required: true\\n  description: \\\"Full URL to a blog post. Must be publicly accessible (no auth walls).\\\"\\n  example: \\\"https://example.com/blog/how-to-scale-content-marketing\\\"\\n\\n- name: \\\"output_filename\\\"\\n  type: \\\"string\\\"\\n  source: \\\"manual input (optional, defaults to sanitized blog title)\\\"\\n  required: false\\n  description: \\\"Filename for the output JSON (without .json extension). Defaults to blog post slug.\\\"\\n  example: \\\"how-to-scale-content-marketing\\\"\\n\\n- name: \\\"brand_voice_guidelines\\\"\\n  type: \\\"string (file path or text)\\\"\\n  source: \\\"optional config file at config/brand_voice.md\\\"\\n  required: false\\n  description: \\\"Optional brand voice guidelines to influence tone matching. If not provided, mirrors source blog exactly.\\\"\\n  example: \\\"config/brand_voice.md\\\"\\n```\\n\\n## Outputs\\nWhat comes out of the system. Where do results go?\\n\\n```yaml\\n- name: \\\"repurposed_content.json\\\"\\n  type: \\\"JSON\\\"\\n  destination: \\\"repo commit at output/{sanitized-title}-repurposed.json\\\"\\n  description: \\\"Single JSON file containing all four platform outputs with metadata\\\"\\n  example: |\\n    {\\n      \\\"metadata\\\": {\\n        \\\"source_url\\\": \\\"https://example.com/blog/post\\\",\\n        \\\"generated_at\\\": \\\"2026-02-11T13:00:00Z\\\",\\n        \\\"tone_analysis\\\": {\\n          \\\"primary_tone\\\": \\\"professional\\\",\\n          \\\"secondary_tone\\\": \\\"conversational\\\",\\n          \\\"confidence\\\": 0.89\\n        }\\n      },\\n      \\\"twitter\\\": {\\n        \\\"thread\\\": [\\n          \\\"Tweet 1 text here...\\\",\\n          \\\"Tweet 2 text here...\\\",\\n          \\\"Tweet 3 text here...\\\"\\n        ],\\n        \\\"hashtags\\\": [\\\"#ContentMarketing\\\", \\\"#MarketingTips\\\", \\\"#B2B\\\"],\\n        \\\"mentions\\\": [\\\"@relevantaccount\\\"],\\n        \\\"character_count\\\": 850\\n      },\\n      \\\"linkedin\\\": {\\n        \\\"post\\\": \\\"LinkedIn post text here with\\\\n\\\\nparagraph breaks...\\\",\\n        \\\"hashtags\\\": [\\\"#ContentStrategy\\\", \\\"#MarketingAutomation\\\"],\\n        \\\"character_count\\\": 1150\\n      },\\n      \\\"email\\\": {\\n        \\\"subject\\\": \\\"Suggested subject line\\\",\\n        \\\"section\\\": \\\"Email-formatted content here...\\\",\\n        \\\"word_count\\\": 250\\n      },\\n      \\\"instagram\\\": {\\n        \\\"caption\\\": \\\"Instagram caption with emojis 🚀...\\\",\\n        \\\"hashtags\\\": [\\\"#Marketing\\\", \\\"#ContentCreation\\\", \\\"#DigitalMarketing\\\"],\\n        \\\"character_count\\\": 1800\\n      }\\n    }\\n\\n- name: \\\"generation_log.json\\\"\\n  type: \\\"JSON\\\"\\n  destination: \\\"repo commit at logs/{timestamp}-generation.json\\\"\\n  description: \\\"Machine-readable log of generation process with timing and token usage\\\"\\n  example: |\\n    {\\n      \\\"source_url\\\": \\\"https://example.com/blog/post\\\",\\n      \\\"extraction_time_ms\\\": 2341,\\n      \\\"tone_analysis_time_ms\\\": 1205,\\n      \\\"generation_time_ms\\\": 8742,\\n      \\\"total_time_ms\\\": 12288,\\n      \\\"token_usage\\\": {\\n        \\\"input_tokens\\\": 4521,\\n        \\\"output_tokens\\\": 2103\\n      },\\n      \\\"parallel_execution\\\": true\\n    }\\n```\\n\\n---\\n\\n## All Needed Context\\n\\n### Documentation & References\\n```yaml\\n# MUST READ — Include these in context when building\\n- url: \\\"https://developer.twitter.com/en/docs/twitter-api/v2/tweets\\\"\\n  why: \\\"Twitter character limits, thread structure, hashtag best practices\\\"\\n\\n- url: \\\"https://www.linkedin.com/help/linkedin/answer/a521928\\\"\\n  why: \\\"LinkedIn post character limits (1300 for main text), formatting guidelines\\\"\\n\\n- url: \\\"https://help.instagram.com/1631821640426723\\\"\\n  why: \\\"Instagram caption limits (2200 chars), hashtag recommendations (3-30 tags)\\\"\\n\\n- doc: \\\"config/mcp_registry.md\\\"\\n  why: \\\"Check Firecrawl for web scraping, Anthropic for content generation\\\"\\n\\n- doc: \\\"library/patterns.md\\\"\\n  why: \\\"Use Intake > Enrich > Deliver + Fan-Out > Process > Merge for parallel generation\\\"\\n\\n- doc: \\\"library/tool_catalog.md\\\"\\n  why: \\\"Reuse firecrawl_scrape, llm_prompt, structured_extract patterns\\\"\\n\\n- url: \\\"https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/tone-matching\\\"\\n  why: \\\"Best practices for tone analysis and style matching with Claude\\\"\\n```\\n\\n### Workflow Pattern Selection\\n```yaml\\npattern: \\\"Intake > Enrich > Deliver + Fan-Out > Process > Merge\\\"\\nrationale: |\\n  Primary pattern is Intake > Enrich > Deliver:\\n  - Intake: Receive blog URL and extract content\\n  - Enrich: Analyze tone, generate platform-specific versions\\n  - Deliver: Package into JSON and commit\\n\\n  Fan-Out > Process > Merge applies to the content generation phase:\\n  - Once we have extracted content + tone analysis, we have 4 independent generation tasks\\n  - Each platform can be generated in parallel (no data dependencies)\\n  - Team lead merges all outputs into single JSON\\n\\nmodifications: |\\n  Standard Intake > Enrich > Deliver assumes sequential enrichment.\\n  We modify the \\\"Enrich\\\" phase to use Agent Teams for parallel generation of the 4 platform outputs.\\n  This reduces wall time from ~40s (sequential) to ~10s (parallel).\\n```\\n\\n### MCP & Tool Requirements\\n```yaml\\ncapabilities:\\n  - name: \\\"web content extraction\\\"\\n    primary_mcp: \\\"firecrawl\\\"\\n    alternative_mcp: \\\"fetch\\\"\\n    fallback: \\\"direct HTTP with requests + BeautifulSoup4 for HTML parsing\\\"\\n    secret_name: \\\"FIRECRAWL_API_KEY\\\"\\n\\n  - name: \\\"tone analysis\\\"\\n    primary_mcp: \\\"anthropic\\\"\\n    alternative_mcp: \\\"none\\\"\\n    fallback: \\\"none - Anthropic API is required\\\"\\n    secret_name: \\\"ANTHROPIC_API_KEY\\\"\\n\\n  - name: \\\"content generation\\\"\\n    primary_mcp: \\\"anthropic\\\"\\n    alternative_mcp: \\\"none\\\"\\n    fallback: \\\"none - Anthropic API is required\\\"\\n    secret_name: \\\"ANTHROPIC_API_KEY\\\"\\n\\n  - name: \\\"file output\\\"\\n    primary_mcp: \\\"filesystem\\\"\\n    alternative_mcp: \\\"none\\\"\\n    fallback: \\\"Python pathlib + json.dump()\\\"\\n    secret_name: \\\"none\\\"\\n```\\n\\n### Known Gotchas & Constraints\\n```\\n# CRITICAL: Twitter thread limit is 280 chars per tweet, but threads can have 25 tweets max\\n# CRITICAL: LinkedIn post limit is 3000 chars total but 1300 chars before \\\"see more\\\" fold — optimize for 1300\\n# CRITICAL: Instagram supports 2200 chars but captions are better at 1000-1500 for engagement\\n# CRITICAL: Email section should be 150-300 words — scannable length for newsletter inclusion\\n# CRITICAL: Firecrawl API may fail on JavaScript-heavy sites — fallback to direct fetch + BeautifulSoup\\n# CRITICAL: Some blogs have paywalls or anti-scraping — tool must detect and fail gracefully with clear error\\n# CRITICAL: Tone analysis confidence below 0.7 should trigger a warning but not block generation\\n# CRITICAL: Hashtag suggestions must avoid banned/spam hashtags on each platform\\n# CRITICAL: Secrets are NEVER hardcoded — always use GitHub Secrets or .env\\n# CRITICAL: Every tool must have try/except, logging, type hints, and a main() function\\n# CRITICAL: Agent Teams requires CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=true in environment\\n# CRITICAL: Character counts must be validated BEFORE committing output — no platform limit violations\\n# CRITICAL: Brand voice guidelines are optional — system must work without them (mirrors source tone only)\\n```\\n\\n---\\n\\n## System Design\\n\\n### Subagent Architecture\\nDefine the specialist subagents this system needs. One subagent per major capability or workflow phase.\\n\\n```yaml\\nsubagents:\\n  - name: \\\"content-extractor\\\"\\n    description: \\\"Extract and clean blog post content from any URL. Delegate when you need to scrape a blog post, extract markdown, or handle content from various blog platforms.\\\"\\n    tools: \\\"Read, Bash, Grep\\\"\\n    model: \\\"sonnet\\\"\\n    permissionMode: \\\"default\\\"\\n    responsibilities:\\n      - \\\"Scrape blog post URL using Firecrawl or fallback methods\\\"\\n      - \\\"Extract clean markdown content (title, author, date, body)\\\"\\n      - \\\"Handle extraction errors gracefully (paywalls, 404s, anti-scraping)\\\"\\n      - \\\"Return structured content object with metadata\\\"\\n    inputs: \\\"blog_url (string)\\\"\\n    outputs: \\\"dict with keys: title, author, publish_date, content_markdown, word_count, extract_method\\\"\\n\\n  - name: \\\"tone-analyzer\\\"\\n    description: \\\"Analyze the tone, style, and voice of written content. Delegate when you need to understand the writing style of source content before adapting it.\\\"\\n    tools: \\\"Read, Bash\\\"\\n    model: \\\"sonnet\\\"\\n    permissionMode: \\\"default\\\"\\n    responsibilities:\\n      - \\\"Analyze source content for tone dimensions (professional, casual, technical, emotional, etc.)\\\"\\n      - \\\"Identify key stylistic elements (sentence length, vocabulary level, rhetorical devices)\\\"\\n      - \\\"Generate a tone profile with confidence score\\\"\\n      - \\\"Flag any brand voice guideline conflicts if provided\\\"\\n    inputs: \\\"content_markdown (string), brand_voice_guidelines (optional string)\\\"\\n    outputs: \\\"dict with keys: primary_tone, secondary_tone, style_elements (list), confidence (float 0-1), voice_notes\\\"\\n\\n  - name: \\\"platform-content-generator\\\"\\n    description: \\\"Generate platform-specific content matching a given tone profile. Delegate when you need to write content optimized for a specific social media or email platform.\\\"\\n    tools: \\\"Read, Bash\\\"\\n    model: \\\"sonnet\\\"\\n    permissionMode: \\\"default\\\"\\n    responsibilities:\\n      - \\\"Generate Twitter thread (3-7 tweets) with hashtags and mentions\\\"\\n      - \\\"Generate LinkedIn post (1300 char optimized) with professional formatting\\\"\\n      - \\\"Generate email newsletter section with subject line\\\"\\n      - \\\"Generate Instagram caption with emoji usage and hashtags\\\"\\n      - \\\"Match tone profile from analyzer in all outputs\\\"\\n      - \\\"Validate character limits for each platform\\\"\\n      - \\\"Suggest relevant, non-generic hashtags\\\"\\n    inputs: \\\"content_markdown (string), tone_profile (dict), platform (string: twitter|linkedin|email|instagram)\\\"\\n    outputs: \\\"dict with platform-specific keys (thread/post/section/caption, hashtags, mentions, character_count)\\\"\\n\\n  - name: \\\"output-packager\\\"\\n    description: \\\"Package multiple platform outputs into a single structured JSON file. Delegate when you need to merge results and write final output.\\\"\\n    tools: \\\"Read, Write, Bash\\\"\\n    model: \\\"haiku\\\"\\n    permissionMode: \\\"default\\\"\\n    responsibilities:\\n      - \\\"Collect outputs from all platform generators\\\"\\n      - \\\"Validate schema consistency\\\"\\n      - \\\"Add metadata (source URL, timestamp, tone analysis)\\\"\\n      - \\\"Write to JSON file with proper formatting\\\"\\n      - \\\"Generate machine-readable log file\\\"\\n    inputs: \\\"source_url (string), tone_profile (dict), platform_outputs (dict of dicts), output_path (string)\\\"\\n    outputs: \\\"dict with keys: output_file_path, log_file_path, success (bool)\\\"\\n```\\n\\n### Agent Teams Analysis\\n```yaml\\n# Apply the 3+ Independent Tasks Rule\\nindependent_tasks:\\n  - \\\"Generate Twitter thread from source content + tone profile\\\"\\n  - \\\"Generate LinkedIn post from source content + tone profile\\\"\\n  - \\\"Generate email newsletter section from source content + tone profile\\\"\\n  - \\\"Generate Instagram caption from source content + tone profile\\\"\\n\\nindependent_task_count: 4\\nrecommendation: \\\"Use Agent Teams\\\"\\nrationale: |\\n  We have 4 truly independent content generation tasks with no data dependencies on each other.\\n  Each task receives the same inputs (content_markdown + tone_profile) and produces independent outputs.\\n  Each generation takes ~8-12 seconds with Claude Sonnet.\\n  Sequential execution: 40-48 seconds. Parallel execution: ~12 seconds (longest single task).\\n  Speedup: 3-4x reduction in wall time. This is a meaningful user experience improvement (under 60s total vs 90s+).\\n  Merge complexity is trivial — just collect 4 JSON objects into a dict.\\n\\n# Agent Teams configuration:\\nteam_lead_responsibilities:\\n  - \\\"Create shared task list with 4 platform generation tasks\\\"\\n  - \\\"Spawn 4 teammates (one per platform) with identical content_markdown + tone_profile inputs\\\"\\n  - \\\"Collect completed outputs from all teammates\\\"\\n  - \\\"Validate character counts and schema for each platform\\\"\\n  - \\\"Merge into single JSON structure\\\"\\n  - \\\"Delegate to output-packager subagent for final file write\\\"\\n\\nteammates:\\n  - name: \\\"twitter-generator\\\"\\n    task: \\\"Generate Twitter thread from source content matching tone profile. Return JSON with thread array, hashtags, mentions, character_count.\\\"\\n    inputs: \\\"content_markdown (string), tone_profile (dict)\\\"\\n    outputs: \\\"dict with keys: thread (list[str]), hashtags (list[str]), mentions (list[str]), character_count (int)\\\"\\n\\n  - name: \\\"linkedin-generator\\\"\\n    task: \\\"Generate LinkedIn post from source content matching tone profile. Return JSON with post text, hashtags, character_count. Optimize for 1300 char limit.\\\"\\n    inputs: \\\"content_markdown (string), tone_profile (dict)\\\"\\n    outputs: \\\"dict with keys: post (str), hashtags (list[str]), character_count (int)\\\"\\n\\n  - name: \\\"email-generator\\\"\\n    task: \\\"Generate email newsletter section from source content matching tone profile. Return JSON with subject, section text, word_count.\\\"\\n    inputs: \\\"content_markdown (string), tone_profile (dict)\\\"\\n    outputs: \\\"dict with keys: subject (str), section (str), word_count (int)\\\"\\n\\n  - name: \\\"instagram-generator\\\"\\n    task: \\\"Generate Instagram caption from source content matching tone profile. Return JSON with caption, hashtags, character_count. Include emoji usage.\\\"\\n    inputs: \\\"content_markdown (string), tone_profile (dict)\\\"\\n    outputs: \\\"dict with keys: caption (str), hashtags (list[str]), character_count (int)\\\"\\n\\n# Sequential fallback:\\nsequential_rationale: \\\"If Agent Teams is not enabled (CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS != true), execute platform generation sequentially by delegating to platform-content-generator subagent 4 times. Total time increases to ~40-50s but output is identical.\\\"\\n```\\n\\n### GitHub Actions Triggers\\n```yaml\\ntriggers:\\n  - type: \\\"workflow_dispatch\\\"\\n    config: |\\n      inputs:\\n        blog_url:\\n          description: 'Blog post URL to repurpose'\\n          required: true\\n          type: string\\n        output_filename:\\n          description: 'Output filename (optional, defaults to blog slug)'\\n          required: false\\n          type: string\\n    description: \\\"Manual trigger for on-demand content repurposing. Marketing team member pastes blog URL and clicks Run Workflow.\\\"\\n\\n  - type: \\\"schedule\\\"\\n    config: \\\"cron: '0 10 * * 1'  # Every Monday at 10 AM UTC\\\"\\n    description: \\\"Optional scheduled run to process a queue of blog URLs (requires separate queue management tool). Disabled by default — enable if batch processing is needed.\\\"\\n\\n  - type: \\\"repository_dispatch\\\"\\n    config: \\\"event_type: repurpose_content\\\"\\n    description: \\\"Webhook trigger for integration with CMS publishing workflows. When a new blog post is published, CMS sends webhook to trigger repurposing.\\\"\\n```\\n\\n---\\n\\n## Implementation Blueprint\\n\\n### Workflow Steps\\nOrdered list of workflow phases. Each step becomes a section in workflow.md.\\n\\n```yaml\\nsteps:\\n  - name: \\\"Extract Blog Content\\\"\\n    description: \\\"Scrape the blog post URL and extract clean markdown content with metadata.\\\"\\n    subagent: \\\"content-extractor\\\"\\n    tools: [\\\"extract_content.py\\\"]\\n    inputs: \\\"blog_url (string from workflow input)\\\"\\n    outputs: \\\"content_object (dict): title, author, publish_date, content_markdown, word_count, extract_method\\\"\\n    failure_mode: \\\"URL is unreachable, paywall detected, anti-scraping blocks, invalid URL format\\\"\\n    fallback: \\\"Try Firecrawl first. If fails, fall back to direct HTTP + BeautifulSoup. If both fail, return error with clear message indicating paywall or scraping block.\\\"\\n\\n  - name: \\\"Analyze Content Tone\\\"\\n    description: \\\"Analyze the extracted content to identify tone, style, and voice characteristics.\\\"\\n    subagent: \\\"tone-analyzer\\\"\\n    tools: [\\\"analyze_tone.py\\\"]\\n    inputs: \\\"content_markdown (from previous step), brand_voice_guidelines (optional from config/brand_voice.md)\\\"\\n    outputs: \\\"tone_profile (dict): primary_tone, secondary_tone, style_elements, confidence, voice_notes\\\"\\n    failure_mode: \\\"Content is too short (<100 words) for accurate tone analysis\\\"\\n    fallback: \\\"If content is very short, use a simplified tone profile with 'conversational' as default and confidence=0.5. Log warning but continue.\\\"\\n\\n  - name: \\\"Generate Platform Content (Parallel)\\\"\\n    description: \\\"Generate optimized content for all four platforms in parallel using Agent Teams.\\\"\\n    subagent: \\\"none (Team Lead orchestrates 4 teammates)\\\"\\n    tools: [\\\"generate_twitter.py\\\", \\\"generate_linkedin.py\\\", \\\"generate_email.py\\\", \\\"generate_instagram.py\\\"]\\n    inputs: \\\"content_markdown (string), tone_profile (dict)\\\"\\n    outputs: \\\"platform_outputs (dict): twitter (dict), linkedin (dict), email (dict), instagram (dict)\\\"\\n    failure_mode: \\\"Claude API rate limit, one or more generations fail, character limit violations\\\"\\n    fallback: \\\"Retry failed generations once with 5s delay. If any platform still fails after retry, include error message in that platform's output but continue with others. Never fail the entire pipeline because one platform failed.\\\"\\n\\n  - name: \\\"Package and Write Output\\\"\\n    description: \\\"Merge all platform outputs into a single JSON file and write to repository.\\\"\\n    subagent: \\\"output-packager\\\"\\n    tools: [\\\"package_output.py\\\"]\\n    inputs: \\\"source_url (string), tone_profile (dict), platform_outputs (dict), output_filename (string)\\\"\\n    outputs: \\\"output/{{sanitized-filename}}-repurposed.json, logs/{{timestamp}}-generation.json\\\"\\n    failure_mode: \\\"File write fails, invalid JSON structure, disk space issues\\\"\\n    fallback: \\\"Validate JSON structure before write. If write fails, log to stdout and exit with error code 1. GitHub Actions will capture stdout for debugging.\\\"\\n\\n  - name: \\\"Commit Results\\\"\\n    description: \\\"Commit the output JSON and generation log to the repository.\\\"\\n    subagent: \\\"none (handled by main agent)\\\"\\n    tools: [\\\"git CLI\\\"]\\n    inputs: \\\"output/{{filename}}-repurposed.json, logs/{{timestamp}}-generation.json\\\"\\n    outputs: \\\"Git commit with message 'feat: repurposed content for {{blog-title}}'\\\"\\n    failure_mode: \\\"Git conflicts, authentication failure, branch protection\\\"\\n    fallback: \\\"Use git pull --rebase before commit to handle concurrent writes. If commit fails, output files are still written to disk for manual recovery.\\\"\\n```\\n\\n### Tool Specifications\\nFor each tool the system needs. Reference library/tool_catalog.md for reusable patterns.\\n\\n```yaml\\ntools:\\n  - name: \\\"extract_content.py\\\"\\n    purpose: \\\"Scrape blog post URL and return clean markdown content\\\"\\n    catalog_pattern: \\\"firecrawl_scrape with HTTP + BeautifulSoup fallback\\\"\\n    inputs:\\n      - \\\"url: str — Blog post URL to scrape\\\"\\n    outputs: \\\"JSON dict: {title, author, publish_date, content_markdown, word_count, extract_method}\\\"\\n    dependencies: [\\\"firecrawl-py\\\", \\\"httpx\\\", \\\"beautifulsoup4\\\", \\\"markdownify\\\"]\\n    mcp_used: \\\"firecrawl (optional)\\\"\\n    error_handling: \\\"try Firecrawl first, catch HTTPError and fall back to requests + BeautifulSoup, raise ValueError if both fail with clear error message\\\"\\n\\n  - name: \\\"analyze_tone.py\\\"\\n    purpose: \\\"Analyze content tone and style using Claude\\\"\\n    catalog_pattern: \\\"structured_extract with JSON schema for tone profile\\\"\\n    inputs:\\n      - \\\"content_markdown: str — Markdown text to analyze\\\"\\n      - \\\"brand_voice_guidelines: str (optional) — Brand voice reference text\\\"\\n    outputs: \\\"JSON dict: {primary_tone, secondary_tone, style_elements, confidence, voice_notes}\\\"\\n    dependencies: [\\\"anthropic\\\"]\\n    mcp_used: \\\"anthropic\\\"\\n    error_handling: \\\"retry once with 5s delay on API errors, if content < 100 words return simplified profile with warning\\\"\\n\\n  - name: \\\"generate_twitter.py\\\"\\n    purpose: \\\"Generate Twitter thread with hashtags and mentions\\\"\\n    catalog_pattern: \\\"llm_prompt with structured output validation\\\"\\n    inputs:\\n      - \\\"content_markdown: str — Source content\\\"\\n      - \\\"tone_profile: dict — Tone analysis from analyzer\\\"\\n    outputs: \\\"JSON dict: {thread: list[str], hashtags: list[str], mentions: list[str], character_count: int}\\\"\\n    dependencies: [\\\"anthropic\\\"]\\n    mcp_used: \\\"anthropic\\\"\\n    error_handling: \\\"validate each tweet is <=280 chars, retry generation if any tweet exceeds limit, include error key in output if retries exhausted\\\"\\n\\n  - name: \\\"generate_linkedin.py\\\"\\n    purpose: \\\"Generate LinkedIn post optimized for 1300 char limit\\\"\\n    catalog_pattern: \\\"llm_prompt with structured output validation\\\"\\n    inputs:\\n      - \\\"content_markdown: str — Source content\\\"\\n      - \\\"tone_profile: dict — Tone analysis\\\"\\n    outputs: \\\"JSON dict: {post: str, hashtags: list[str], character_count: int}\\\"\\n    dependencies: [\\\"anthropic\\\"]\\n    mcp_used: \\\"anthropic\\\"\\n    error_handling: \\\"validate post is <=1300 chars before 'see more' fold, retry if exceeded, include error key if retries exhausted\\\"\\n\\n  - name: \\\"generate_email.py\\\"\\n    purpose: \\\"Generate email newsletter section with subject line\\\"\\n    catalog_pattern: \\\"llm_prompt with structured output validation\\\"\\n    inputs:\\n      - \\\"content_markdown: str — Source content\\\"\\n      - \\\"tone_profile: dict — Tone analysis\\\"\\n    outputs: \\\"JSON dict: {subject: str, section: str, word_count: int}\\\"\\n    dependencies: [\\\"anthropic\\\"]\\n    mcp_used: \\\"anthropic\\\"\\n    error_handling: \\\"target 150-300 words, validate word count, retry if far outside range, include error key if retries exhausted\\\"\\n\\n  - name: \\\"generate_instagram.py\\\"\\n    purpose: \\\"Generate Instagram caption with emojis and hashtags\\\"\\n    catalog_pattern: \\\"llm_prompt with structured output validation\\\"\\n    inputs:\\n      - \\\"content_markdown: str — Source content\\\"\\n      - \\\"tone_profile: dict — Tone analysis\\\"\\n    outputs: \\\"JSON dict: {caption: str, hashtags: list[str], character_count: int}\\\"\\n    dependencies: [\\\"anthropic\\\"]\\n    mcp_used: \\\"anthropic\\\"\\n    error_handling: \\\"validate caption is <=2200 chars, validate 3-30 hashtags, retry if limits exceeded, include error key if retries exhausted\\\"\\n\\n  - name: \\\"package_output.py\\\"\\n    purpose: \\\"Merge all platform outputs into single JSON file\\\"\\n    catalog_pattern: \\\"json_read_write with schema validation\\\"\\n    inputs:\\n      - \\\"source_url: str — Original blog URL\\\"\\n      - \\\"tone_profile: dict — Tone analysis result\\\"\\n      - \\\"platform_outputs: dict — All four platform dicts\\\"\\n      - \\\"output_path: str — Where to write JSON file\\\"\\n    outputs: \\\"JSON dict: {output_file_path: str, log_file_path: str, success: bool}\\\"\\n    dependencies: [\\\"none (stdlib json + pathlib)\\\"]\\n    mcp_used: \\\"none\\\"\\n    error_handling: \\\"validate JSON structure matches schema before write, log validation errors, exit 1 if write fails\\\"\\n```\\n\\n### Per-Tool Pseudocode\\n```python\\n# extract_content.py\\ndef main():\\n    # PATTERN: firecrawl_scrape with HTTP fallback\\n    # Step 1: Parse input URL\\n    args = parse_args()  # argparse: --url\\n    url = args.url\\n\\n    # Step 2: Try Firecrawl first\\n    try:\\n        result = firecrawl_scrape(url)\\n        return {\\n            \\\"title\\\": result.metadata.get(\\\"title\\\"),\\n            \\\"author\\\": result.metadata.get(\\\"author\\\"),\\n            \\\"publish_date\\\": result.metadata.get(\\\"date\\\"),\\n            \\\"content_markdown\\\": result.markdown,\\n            \\\"word_count\\\": len(result.markdown.split()),\\n            \\\"extract_method\\\": \\\"firecrawl\\\"\\n        }\\n    except Exception as e:\\n        logging.warning(f\\\"Firecrawl failed: {e}, trying HTTP fallback\\\")\\n\\n    # Step 3: Fallback to HTTP + BeautifulSoup\\n    # CRITICAL: Handle paywalls, 404s, anti-scraping\\n    try:\\n        resp = httpx.get(url, timeout=15, headers={\\\"User-Agent\\\": \\\"...\\\"})\\n        resp.raise_for_status()\\n        soup = BeautifulSoup(resp.text, \\\"html.parser\\\")\\n        # Extract article content (heuristic: <article>, <main>, or largest <div>)\\n        content = extract_article_content(soup)\\n        markdown = markdownify.markdownify(content)\\n        return {\\n            \\\"title\\\": soup.find(\\\"h1\\\").text,\\n            \\\"content_markdown\\\": markdown,\\n            \\\"word_count\\\": len(markdown.split()),\\n            \\\"extract_method\\\": \\\"http+beautifulsoup\\\"\\n        }\\n    except Exception as e:\\n        raise ValueError(f\\\"Content extraction failed: {e}\\\")\\n\\n# analyze_tone.py\\ndef main():\\n    # PATTERN: structured_extract with tone analysis schema\\n    # CRITICAL: Return confidence score, handle short content\\n    args = parse_args()  # --content, --brand-voice (optional)\\n    content = args.content\\n\\n    if len(content.split()) < 100:\\n        logging.warning(\\\"Content too short for accurate tone analysis\\\")\\n        return {\\n            \\\"primary_tone\\\": \\\"conversational\\\",\\n            \\\"secondary_tone\\\": \\\"neutral\\\",\\n            \\\"style_elements\\\": [\\\"brief\\\"],\\n            \\\"confidence\\\": 0.5,\\n            \\\"voice_notes\\\": \\\"Content too short for detailed analysis\\\"\\n        }\\n\\n    # Use Claude to analyze tone\\n    schema = {\\n        \\\"type\\\": \\\"object\\\",\\n        \\\"properties\\\": {\\n            \\\"primary_tone\\\": {\\\"type\\\": \\\"string\\\"},\\n            \\\"secondary_tone\\\": {\\\"type\\\": \\\"string\\\"},\\n            \\\"style_elements\\\": {\\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}},\\n            \\\"confidence\\\": {\\\"type\\\": \\\"number\\\", \\\"minimum\\\": 0, \\\"maximum\\\": 1}\\n        }\\n    }\\n    result = structured_extract(\\n        content=content,\\n        schema=schema,\\n        instructions=\\\"Analyze the tone and style of this content...\\\"\\n    )\\n    return result.data\\n\\n# generate_twitter.py\\ndef main():\\n    # PATTERN: llm_prompt with Twitter thread generation\\n    # CRITICAL: Validate 280 char per tweet, 3-7 tweets total\\n    args = parse_args()  # --content, --tone-profile\\n    content = args.content\\n    tone = json.loads(args.tone_profile)\\n\\n    prompt = f\\\"\\\"\\\"\\n    Convert this blog content into a Twitter thread matching the tone: {tone['primary_tone']}.\\n    Rules:\\n    - 3-7 tweets maximum\\n    - Each tweet <=280 characters\\n    - Include 3-5 relevant hashtags\\n    - Suggest @mentions if appropriate\\n    - Match the tone: {tone['voice_notes']}\\n\\n    Content:\\n    {content}\\n    \\\"\\\"\\\"\\n\\n    result = llm_prompt(\\n        prompt=prompt,\\n        system=\\\"You are a social media content strategist...\\\",\\n        temperature=0.7\\n    )\\n\\n    # Validate and structure\\n    thread = parse_thread(result.text)\\n    validate_twitter_limits(thread)  # raises if any tweet > 280 chars\\n\\n    return {\\n        \\\"thread\\\": thread,\\n        \\\"hashtags\\\": extract_hashtags(result.text),\\n        \\\"mentions\\\": extract_mentions(result.text),\\n        \\\"character_count\\\": sum(len(t) for t in thread)\\n    }\\n\\n# generate_linkedin.py (similar pattern to Twitter)\\n# generate_email.py (similar pattern)\\n# generate_instagram.py (similar pattern)\\n\\n# package_output.py\\ndef main():\\n    # PATTERN: json_read_write with schema validation\\n    args = parse_args()  # --source-url, --tone-profile, --twitter, --linkedin, --email, --instagram, --output-path\\n    \\n    output = {\\n        \\\"metadata\\\": {\\n            \\\"source_url\\\": args.source_url,\\n            \\\"generated_at\\\": datetime.utcnow().isoformat(),\\n            \\\"tone_analysis\\\": json.loads(args.tone_profile)\\n        },\\n        \\\"twitter\\\": json.loads(args.twitter),\\n        \\\"linkedin\\\": json.loads(args.linkedin),\\n        \\\"email\\\": json.loads(args.email),\\n        \\\"instagram\\\": json.loads(args.instagram)\\n    }\\n\\n    # Validate schema\\n    validate_output_schema(output)\\n\\n    # Write to file\\n    output_path = Path(args.output_path)\\n    output_path.parent.mkdir(parents=True, exist_ok=True)\\n    output_path.write_text(json.dumps(output, indent=2))\\n\\n    # Generate log file\\n    log_path = Path(\\\"logs\\\") / f\\\"{datetime.now().isoformat()}-generation.json\\\"\\n    # ... write log ...\\n\\n    return {\\\"output_file_path\\\": str(output_path), \\\"success\\\": True}\\n```\\n\\n### Integration Points\\n```yaml\\nSECRETS:\\n  - name: \\\"FIRECRAWL_API_KEY\\\"\\n    purpose: \\\"Firecrawl API for web scraping (optional — has fallback)\\\"\\n    required: false\\n\\n  - name: \\\"ANTHROPIC_API_KEY\\\"\\n    purpose: \\\"Claude API for tone analysis and content generation\\\"\\n    required: true\\n\\nENVIRONMENT:\\n  - file: \\\".env.example\\\"\\n    vars:\\n      - \\\"FIRECRAWL_API_KEY=your_key_here  # Optional, for web scraping\\\"\\n      - \\\"ANTHROPIC_API_KEY=your_key_here  # Required, for AI generation\\\"\\n      - \\\"CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=true  # Required for parallel generation\\\"\\n\\nDEPENDENCIES:\\n  - file: \\\"requirements.txt\\\"\\n    packages:\\n      - \\\"anthropic>=0.18.0  # Claude API\\\"\\n      - \\\"firecrawl-py>=0.0.5  # Optional web scraping\\\"\\n      - \\\"httpx>=0.26.0  # HTTP fallback\\\"\\n      - \\\"beautifulsoup4>=4.12.0  # HTML parsing fallback\\\"\\n      - \\\"markdownify>=0.11.6  # HTML to markdown conversion\\\"\\n      - \\\"python-dateutil>=2.8.2  # Date parsing\\\"\\n\\nGITHUB_ACTIONS:\\n  - trigger: \\\"workflow_dispatch\\\"\\n    config: |\\n      inputs:\\n        blog_url: {required: true, type: string}\\n        output_filename: {required: false, type: string}\\n```\\n\\n---\\n\\n## Validation Loop\\n\\n### Level 1: Syntax & Structure\\n```bash\\n# Run FIRST — every tool must pass before proceeding to Level 2\\n# AST parse — verify valid Python syntax\\npython -c \\\"import ast; ast.parse(open('tools/extract_content.py').read())\\\"\\npython -c \\\"import ast; ast.parse(open('tools/analyze_tone.py').read())\\\"\\npython -c \\\"import ast; ast.parse(open('tools/generate_twitter.py').read())\\\"\\npython -c \\\"import ast; ast.parse(open('tools/generate_linkedin.py').read())\\\"\\npython -c \\\"import ast; ast.parse(open('tools/generate_email.py').read())\\\"\\npython -c \\\"import ast; ast.parse(open('tools/generate_instagram.py').read())\\\"\\npython -c \\\"import ast; ast.parse(open('tools/package_output.py').read())\\\"\\n\\n# Import check — verify no missing dependencies\\npython -c \\\"import importlib; importlib.import_module('tools.extract_content')\\\"\\npython -c \\\"import importlib; importlib.import_module('tools.analyze_tone')\\\"\\npython -c \\\"import importlib; importlib.import_module('tools.generate_twitter')\\\"\\npython -c \\\"import importlib; importlib.import_module('tools.generate_linkedin')\\\"\\npython -c \\\"import importlib; importlib.import_module('tools.generate_email')\\\"\\npython -c \\\"import importlib; importlib.import_module('tools.generate_instagram')\\\"\\npython -c \\\"import importlib; importlib.import_module('tools.package_output')\\\"\\n\\n# Structure check — verify main() exists\\npython -c \\\"from tools.extract_content import main; assert callable"}],"api":"anthropic-messages","provider":"anthropic","model":"claude-sonnet-4-5-20250929","usage":{"input":13,"output":1,"cacheRead":48935,"cacheWrite":3728,"totalTokens":52677,"cost":{"input":0.000039,"output":0.000015,"cacheRead":0.014680499999999999,"cacheWrite":0.013980000000000001,"total":0.0287145}},"stopReason":"error","timestamp":1770815908102,"errorMessage":"terminated"}}
{"type":"message","id":"3607def9","parentId":"98d97d57","timestamp":"2026-02-11T13:25:07.488Z","message":{"role":"assistant","content":[{"type":"thinking","thinking":"Perfect. Now I have all the context I need to generate a comprehensive PRP for the content-repurposer system. Let me analyze the problem and map it to patterns:\n\nProblem: Multi-channel content repurposing from blog post URL -> Twitter thread, LinkedIn post, email newsletter section, Instagram caption\n- Each optimized for platform format and character limits\n- Include tone analysis and style matching\n- Generate content only (no auto-posting)\n- Suggest hashtags and mentions\n- Save to JSON file\n\nThis maps to the **Scrape > Process > Output** pattern:\n1. Scrape: Extract blog post content from URL (Firecrawl MCP)\n2. Process: \n   - Analyze tone of source content (Claude)\n   - Generate platform-specific content (Claude with structured output)\n   - Apply character limits and format constraints per platform\n3. Output: Save all outputs to JSON file\n\nI'll also need:\n- **Subagents**: content-scraper-specialist, tone-analyzer-specialist, platform-writer-specialist\n- **Agent Teams analysis**: 4 independent tasks (Twitter, LinkedIn, Email, Instagram) -> Agent Teams recommended\n- **Tools**: firecrawl_scrape, tone_analyzer, content_generator, json_writer\n- **MCP**: Firecrawl (primary), Fetch (fallback), Anthropic (for Claude API)\n\nLet me structure a comprehensive PRP now.","thinkingSignature":"EpMLCkYICxgCKkCj4akPDkVQRPa4PDMmbJp5V2HSwxOhxHjEob6RUkirmgh3EtmzBkL7Z6+X07E7rsIoOL/oNJD7++ATNfLPH07zEgwFaeUztMNn0QWVNp4aDHjNpE8sNUsXCDXUjSIwx5pjXpd83qJ/VVv7M/cdIvNyc2r936FPn1i47gvv9wGXxN404NtiMLKasqXr8DBNKvoJdhABjOgeZpfBKpCkw5IT8bYihhVfVfWDy6RffIszAkhafJKSyoHOltfziPRILsiNxUSPxz7+ald5AgT70DwZWI+AJ9FFS1TsA/WwKAtBb9V7THxR6V5k+YsUqco9XAfsNWLRCh+kGW9i2cM+Uvm0kYiy8ql8cyJ57gEw0Vd5K82UcLstObVNQw3bxPX4WzGnTYPhwHS8gffjJJkN/OY7vsuzs0GWW92IXHFKEPk5795nwgfnH+3a9CmUoDgA77WnsJdigayAbcidBIN7dRfKBY+rzawBQOVmD8wx4nTNDFFUp/qG+wmcXrr7vWoD1y7zqaxOrUUsBxKHC00Z34l+8N8+lmcT77ehk5a+ZRN1SqEHAguQTbIyubC4Q0XLxmlj2SAfFtNYFDFpbLFiOUsqD2nanGLAg0wB5bc9VuJMzvdYqI8wOn5Z67+TKns3T6rIU6a1qq0ydjwS9V50y8FpHpNtnaKYEpZRrK5eNjt0wEUWarKV6C4V6sTuxRFUS53N4cb0JPv4AzaX66bUQDvoBOTp2XO/bND/6u6z+/ssKomJQ9WipBu9fC7lSBeAFty6PTxg2qDND+kbql5ibjmXQDy4rLM7PhKRpgBZeCBadThJBHUK04xBpXmzxA1JgZBHQpyBSpTusnc431rl/LDnUtfjjIWcrsQ3x28hSVdPbhCYCJOru+6MHkMr0R2ehOgGkJki7PeoXeDYFNz+X27ygZSfmBs/Y7J4vNWdJCKW+VtU4Df5ey8m2HpYw3mOs9orqjXzZhY6wvpr7tAc2zmoJCL8NUjKJB58OXI43omhBZBUaQizA3C/no+M3U2mYh7CiRS6i85aVkK8Iwc6FPaR+O8cSaqIlQ+x/Rw/JDs8pdc9HYVc0RCpGbw1RzwCDvf2ho4BrxJt3kLT7VlhbbmTI1FY/lC8IkZYJz1j7R3M/WD7sTIDqzZ509/U+ILUQ6c99VhYmAErgg5bAmA4kLJ7yD/eni/r/BZI+FeL5XcYXU07J8G3MgRvcDJ33U3v5nUFS4Nj30LLi+5hoqprDK8fjpknWCs6dFWnqBQcjhSctyzxPMeZ2hb9Jl4Qgspo3HVFwJOr1+w7BlGSwXkqDB5meV+OsWd9sfTCkI0skkDkCL5kAfMtpn74lnnBDgTukTPKVC90JE1lrR+4gLTWZiDdzaqGvUOaQyTkt2rv5s+g0vq92ZmYZG2oG7Mp82Lb4KdCdPkxRIEPBLdRh2wbEuYNUtJZtNFNnV2ocyK0j1y0x4HtWlRlRixIvx8kdErvUG0xz1N/R+yc12SwW35s1bJp8rCPS5Hp8QwdKmQcjkzpNC/inDQaurqqG+VEw5sZ//SlYOEV/xxB93PgZreASc0IHWhDD0ADqzh5yZrXVI+W8hdXdsXz59Qdq44eMdwJeHsJIQFovC3SL9pkt8pIZfxQXsw/u2GBgcbyG1hmqAQ1GLryUivz2qTDV9Ms3+gWCIUlLgDUOICEtoGZw0ad5JUpqXKA/FhzkEI7d4IjYvEaywL4suYeW3DInCkKv4tnTg+5jVGF7WRMbt2uO1Kk3KCKurFG6rnp0FIVav9ODWHrtzrHHpVetfU0WAM6w9E8o9nQoUwCs/RWMMvwUiZsso66ntYPcxvsQOibcmzYnfhn8iQeZAuXvFnx/YCVxVgy8TlHRXk1QySyPx5mReNrPw6mbRa03y0JXgBHQAQYAQ=="},{"type":"text","text":"Now I'll generate a comprehensive PRP for the content-repurposer system:"},{"type":"toolCall","id":"toolu_0142i76SSzkf2Zps18V9Kah4","name":"write","arguments":{"path":"PRPs/content-repurposer.md","content":"name: \"content-repurposer\"\ndescription: |\n  Multi-channel content repurposing system that extracts blog post content and generates platform-optimized social media and newsletter content with tone matching and engagement optimization.\n\n## Purpose\nWAT System PRP (Product Requirements Prompt) — a structured blueprint that gives the factory enough context to build a complete, working system in one pass.\n\n## Core Principles\n1. **Context is King**: Include ALL necessary documentation, references, and caveats\n2. **Validation Loops**: Provide executable checks the factory can run at each gate\n3. **Information Dense**: Use keywords and patterns from the WAT framework\n4. **Progressive Success**: Start simple, validate, then enhance\n5. **Global rules**: Follow all rules in CLAUDE.md\n\n---\n\n## Goal\nBuild a content repurposing pipeline that takes a single blog post URL, extracts and analyzes the content, and generates platform-specific versions for Twitter (thread), LinkedIn (post), email newsletter (section), and Instagram (caption). Each output is optimized for the platform's format, character limits, and engagement patterns. The system includes automatic tone analysis from the source content and applies style matching per platform. Outputs are saved to a single JSON file with one key per platform for easy integration with downstream distribution tools.\n\n## Why\n- **Business value**: Content teams spend 3-5 hours manually adapting each blog post for multiple channels. This system reduces that to 2 minutes of setup and review.\n- **User impact**: Marketing teams, solo creators, and content agencies can 3x their content distribution velocity without hiring additional writers.\n- **Manual process automated**: Eliminates the manual copy-paste-rewrite loop across platforms. Preserves brand voice through automatic tone matching.\n- **Who benefits**: Content marketers, social media managers, newsletter editors, and content agencies. Benefits occur every time a new blog post is published (typically 1-4x per week).\n\n## What\nA GitHub Actions-powered workflow that accepts a blog post URL as input, scrapes the content using Firecrawl, analyzes the tone and style using Claude, generates four platform-specific content variants (Twitter thread, LinkedIn post, email section, Instagram caption) in parallel using Agent Teams, and saves the results to a timestamped JSON file in the repository. The system runs on-demand via workflow dispatch or automatically via repository dispatch webhook.\n\n### Success Criteria\n- [x] System accepts a blog post URL and returns valid JSON with 4 platform-specific content variants\n- [x] Twitter thread respects 280-char limit per tweet, generates 3-7 tweets, includes thread numbering\n- [x] LinkedIn post is 1,200-3,000 characters with professional tone and 3-5 hashtags\n- [x] Email newsletter section is 300-500 words with clear CTA and personalization tokens\n- [x] Instagram caption is 125-150 words with 5-10 hashtags and emoji usage\n- [x] Tone analysis automatically detects source content style (formal, casual, technical, conversational) and matches it per platform\n- [x] Each platform output includes suggested hashtags and @mention placeholders\n- [x] Output JSON includes metadata: source_url, analysis_timestamp, tone_profile, word_counts\n- [x] System runs autonomously via GitHub Actions on schedule or manual trigger\n- [x] Results are committed back to repo in `outputs/` directory with ISO timestamp filename\n- [x] All three execution paths work: CLI, GitHub Actions, Agent HQ\n\n---\n\n## Inputs\nWhat goes into the system:\n\n```yaml\n- name: \"blog_post_url\"\n  type: \"string (URL)\"\n  source: \"manual input via workflow_dispatch or webhook payload\"\n  required: true\n  description: \"Full URL of the blog post to repurpose. Must be publicly accessible.\"\n  example: \"https://example.com/blog/how-to-use-ai-agents\"\n\n- name: \"brand_voice_notes\"\n  type: \"string (optional)\"\n  source: \"manual input or config file\"\n  required: false\n  description: \"Optional brand voice guidelines to override automatic tone detection (e.g., 'Always use we/our, avoid jargon, inject humor')\"\n  example: \"Friendly but authoritative. Use contractions. Avoid corporate speak.\"\n\n- name: \"target_audience\"\n  type: \"string (optional)\"\n  source: \"manual input or config file\"\n  required: false\n  description: \"Target audience persona to shape content (e.g., 'technical founders', 'marketing managers', 'general audience')\"\n  example: \"B2B SaaS founders and CTOs\"\n```\n\n## Outputs\nWhat comes out of the system:\n\n```yaml\n- name: \"repurposed_content.json\"\n  type: \"JSON\"\n  destination: \"repo commit to outputs/YYYY-MM-DD_HH-MM-SS_repurposed.json\"\n  description: \"Complete set of platform-specific content variants with metadata\"\n  example: |\n    {\n      \"metadata\": {\n        \"source_url\": \"https://example.com/blog/post\",\n        \"timestamp\": \"2026-02-11T13:18:00Z\",\n        \"tone_profile\": {\n          \"primary\": \"conversational\",\n          \"formality\": \"medium\",\n          \"technical_level\": \"intermediate\",\n          \"emotion\": \"optimistic\"\n        },\n        \"word_count_source\": 1847\n      },\n      \"twitter\": {\n        \"thread\": [\n          \"1/5 Just published: How AI agents are transforming B2B workflows. Here's what we learned building autonomous systems for 6 months 🧵\",\n          \"2/5 The key insight: agents aren't about replacing humans, they're about removing repetitive context-switching...\",\n          \"3/5 Three patterns emerged: Monitor-Detect-Alert, Scrape-Process-Output, and Issue-Execute-PR...\",\n          \"4/5 Our biggest mistake: trying to automate too much too soon. Start with one painful manual task...\",\n          \"5/5 Full post with code examples and architecture diagrams: [URL]\"\n        ],\n        \"hashtags\": [\"#AI\", \"#Automation\", \"#B2BSaaS\", \"#ProductivityTools\"],\n        \"mentions\": [\"@mention_placeholder\"],\n        \"character_counts\": [139, 276, 268, 251, 89]\n      },\n      \"linkedin\": {\n        \"post\": \"We just wrapped 6 months of building autonomous AI agent systems...\\n\\n[Full professional post with line breaks and formatting]\\n\\nRead the full breakdown: [URL]\\n\\n#AIAgents #B2BAutomation #ProductDevelopment\",\n        \"hashtags\": [\"#AIAgents\", \"#B2BAutomation\", \"#ProductDevelopment\", \"#EngineeringLeadership\"],\n        \"mentions\": [],\n        \"character_count\": 2847\n      },\n      \"email_newsletter\": {\n        \"section\": \"## This Week: AI Agents in Production\\n\\nHi {{first_name}},\\n\\nWe spent the last 6 months building autonomous AI systems...\\n\\n[Full newsletter section with markdown formatting]\\n\\nWant to dive deeper? Read the full post here: [URL]\\n\\nUntil next time,\\n{{sender_name}}\",\n        \"subject_line_suggestion\": \"What we learned building AI agents for 6 months\",\n        \"preview_text\": \"Three patterns that transformed our workflow automation\",\n        \"word_count\": 487,\n        \"personalization_tokens\": [\"{{first_name}}\", \"{{sender_name}}\"]\n      },\n      \"instagram\": {\n        \"caption\": \"6 months, 12 AI agents, countless lessons learned 🤖✨\\n\\nWe built autonomous systems that monitor, scrape, and execute tasks without human input. Here's what actually worked:\\n\\n✅ Start with one painful task\\n✅ Build fallbacks for everything\\n✅ Make the agent transparent\\n\\nSwipe for the full breakdown or check the link in bio 👆\\n\\n#AIAgents #AutomationTools #ProductivityHacks #TechInnovation #B2BSaaS #NoCode #WorkflowAutomation #StartupLife #EngineeringCulture #DevTools\",\n        \"hashtags\": [\"#AIAgents\", \"#AutomationTools\", \"#ProductivityHacks\", \"#TechInnovation\", \"#B2BSaaS\", \"#NoCode\", \"#WorkflowAutomation\", \"#StartupLife\", \"#EngineeringCulture\", \"#DevTools\"],\n        \"emoji_usage\": [\"🤖\", \"✨\", \"✅\", \"👆\"],\n        \"word_count\": 147\n      }\n    }\n```\n\n---\n\n## All Needed Context\n\n### Documentation & References\n```yaml\n- url: \"https://docs.firecrawl.dev/api-reference/endpoint/scrape\"\n  why: \"Primary content extraction API. Need to understand markdown format output, rate limits (500 req/min on paid plan), and error handling.\"\n\n- url: \"https://developer.twitter.com/en/docs/twitter-api/v2/tweet-caps\"\n  why: \"Twitter character limits and thread formatting rules. 280 chars per tweet, link URLs count as 23 chars regardless of actual length.\"\n\n- url: \"https://learn.microsoft.com/en-us/linkedin/marketing/integrations/community-management/shares/ugc-post-api\"\n  why: \"LinkedIn post best practices. Optimal length 1,200-3,000 chars. Posts with 3-5 hashtags perform best.\"\n\n- url: \"https://developers.facebook.com/docs/instagram-api/reference/ig-user/media\"\n  why: \"Instagram caption limits (2,200 chars) and hashtag best practices (5-10 hashtags optimal, 30 max).\"\n\n- doc: \"config/mcp_registry.md\"\n  why: \"Check which MCPs provide web scraping (Firecrawl, Fetch, Puppeteer) and LLM access (Anthropic)\"\n\n- doc: \"library/patterns.md\"\n  why: \"Select Scrape > Process > Output pattern. Evaluate Agent Teams for parallel content generation.\"\n\n- doc: \"library/tool_catalog.md\"\n  why: \"Reuse firecrawl_scrape, llm_prompt, structured_extract patterns. Adapt for multi-platform generation.\"\n```\n\n### Workflow Pattern Selection\n```yaml\npattern: \"Scrape > Process > Output\"\nrationale: \"Classic extraction-transformation-output pipeline. Fetch content from URL, process through tone analysis and multi-platform generation, output to structured JSON. Maps directly to the three-phase workflow.\"\nmodifications: \"Add parallel processing via Agent Teams for the 4 platform-specific generations (Twitter, LinkedIn, Email, Instagram) since they are independent tasks with no data dependencies.\"\n```\n\n### MCP & Tool Requirements\n```yaml\ncapabilities:\n  - name: \"web scraping\"\n    primary_mcp: \"firecrawl\"\n    alternative_mcp: \"fetch\"\n    fallback: \"Direct HTTP with requests + beautifulsoup4 for HTML parsing\"\n    secret_name: \"FIRECRAWL_API_KEY\"\n\n  - name: \"llm text generation\"\n    primary_mcp: \"none (direct API)\"\n    alternative_mcp: \"none\"\n    fallback: \"Anthropic Python SDK with direct API calls\"\n    secret_name: \"ANTHROPIC_API_KEY\"\n\n  - name: \"structured data extraction\"\n    primary_mcp: \"none (direct API)\"\n    alternative_mcp: \"none\"\n    fallback: \"JSON schema validation with jsonschema library\"\n    secret_name: \"ANTHROPIC_API_KEY\"\n```\n\n### Known Gotchas & Constraints\n```\n# CRITICAL: Firecrawl rate limit is 500 requests/min on paid plan, 10 req/min on free tier\n# CRITICAL: Twitter URLs in tweets count as exactly 23 characters regardless of actual URL length\n# CRITICAL: LinkedIn post length sweet spot is 1,200-3,000 chars. Over 3,000 gets \"see more\" truncation\n# CRITICAL: Instagram caption limit is 2,200 chars but first 125 words are visible without \"more\" click\n# CRITICAL: Anthropic API has 200K token context limit. Blog posts up to ~80K words fit in context\n# CRITICAL: Secrets are NEVER hardcoded — always use GitHub Secrets via os.environ\n# CRITICAL: Every tool must have try/except, logging, type hints, and a main() function\n# CRITICAL: Agent Teams is optimal here — 4 independent content generation tasks with no dependencies\n# CRITICAL: Tone analysis must run BEFORE content generation — outputs feed into generation prompts\n# CRITICAL: JSON output must be valid — use json.dumps() with ensure_ascii=False for emoji support\n# CRITICAL: Firecrawl returns markdown — preserve headings, lists, and formatting for context\n```\n\n---\n\n## System Design\n\n### Subagent Architecture\nDefine the specialist subagents this system needs:\n\n```yaml\nsubagents:\n  - name: \"content-scraper-specialist\"\n    description: \"Delegate when the task is to extract and clean content from a URL. Handles Firecrawl API calls, fallback to direct HTTP, and markdown normalization.\"\n    tools: \"Read, Write, Bash\"\n    model: \"sonnet\"\n    permissionMode: \"default\"\n    responsibilities:\n      - \"Call Firecrawl API to scrape blog post URL\"\n      - \"Handle rate limiting with exponential backoff\"\n      - \"Fall back to direct HTTP + BeautifulSoup if Firecrawl fails\"\n      - \"Normalize markdown output (strip extra whitespace, preserve structure)\"\n      - \"Validate scraped content is not empty or error page\"\n    inputs: \"blog_post_url (string)\"\n    outputs: \"markdown_content (string), metadata (dict with title, author, publish_date if available)\"\n\n  - name: \"tone-analyzer-specialist\"\n    description: \"Delegate when the task is to analyze writing style and tone from source content. Uses Claude to perform deep linguistic analysis across multiple dimensions.\"\n    tools: \"Read\"\n    model: \"sonnet\"\n    permissionMode: \"default\"\n    responsibilities:\n      - \"Analyze markdown content for tone and style characteristics\"\n      - \"Detect formality level (formal, professional, conversational, casual)\"\n      - \"Identify technical depth (beginner, intermediate, advanced, expert)\"\n      - \"Assess emotional tone (neutral, optimistic, cautious, urgent, humorous)\"\n      - \"Extract key themes and messaging priorities\"\n      - \"Generate tone profile JSON with confidence scores\"\n    inputs: \"markdown_content (string), brand_voice_notes (optional string)\"\n    outputs: \"tone_profile (dict with primary_tone, formality, technical_level, emotion, key_themes, confidence_scores)\"\n\n  - name: \"platform-writer-specialist\"\n    description: \"Delegate when the task is to generate platform-specific content. Handles all content generation with platform-specific formatting, character limits, and engagement optimization.\"\n    tools: \"Read\"\n    model: \"sonnet\"\n    permissionMode: \"default\"\n    responsibilities:\n      - \"Generate Twitter thread (3-7 tweets, 280 chars each, numbered, engaging hooks)\"\n      - \"Generate LinkedIn post (1,200-3,000 chars, professional tone, line breaks for readability)\"\n      - \"Generate email newsletter section (300-500 words, markdown formatting, personalization tokens)\"\n      - \"Generate Instagram caption (125-150 words visible, emoji usage, hashtag strategy)\"\n      - \"Apply tone profile to match source content style per platform conventions\"\n      - \"Suggest relevant hashtags (platform-appropriate count and style)\"\n      - \"Include @mention placeholders where audience tagging is appropriate\"\n      - \"Validate character counts and format constraints\"\n    inputs: \"markdown_content (string), tone_profile (dict), target_audience (optional string), platform (string)\"\n    outputs: \"platform_content (dict with post/thread, hashtags, mentions, metadata)\"\n\n  - name: \"output-formatter-specialist\"\n    description: \"Delegate when the task is to combine and format all generated content into the final JSON output structure. Handles validation, metadata assembly, and file writing.\"\n    tools: \"Write, Bash\"\n    model: \"haiku\"\n    permissionMode: \"default\"\n    responsibilities:\n      - \"Combine all platform-specific content into unified JSON structure\"\n      - \"Add metadata (source_url, timestamp, tone_profile, word_counts)\"\n      - \"Validate JSON structure against schema\"\n      - \"Write to timestamped output file in outputs/ directory\"\n      - \"Generate human-readable summary for commit message\"\n    inputs: \"twitter_content (dict), linkedin_content (dict), email_content (dict), instagram_content (dict), tone_profile (dict), source_url (string)\"\n    outputs: \"output_filepath (string), summary (string for commit message)\"\n```\n\n### Agent Teams Analysis\n```yaml\n# Apply the 3+ Independent Tasks Rule\nindependent_tasks:\n  - \"Generate Twitter thread — no dependency on other platform content\"\n  - \"Generate LinkedIn post — no dependency on other platform content\"\n  - \"Generate email newsletter section — no dependency on other platform content\"\n  - \"Generate Instagram caption — no dependency on other platform content\"\n\nindependent_task_count: 4\nrecommendation: \"Use Agent Teams\"\nrationale: \"Four completely independent content generation tasks. Each requires 10-30 seconds of LLM generation time. Sequential execution = 40-120s total. Parallel execution = 10-30s total (4x speedup). All tasks share the same inputs (markdown_content, tone_profile) with no data dependencies between them. Results are merged into a single JSON with no conflict resolution needed. This is the ideal Agent Teams use case.\"\n\n# Agent Teams recommended:\nteam_lead_responsibilities:\n  - \"Create shared task list with 4 platform generation tasks\"\n  - \"Spawn 4 teammates (one per platform)\"\n  - \"Collect all generated content from temp/ directory\"\n  - \"Merge results into unified JSON structure\"\n  - \"Validate final output completeness and format\"\n  - \"Delegate to output-formatter-specialist for final file write\"\n\nteammates:\n  - name: \"twitter-writer\"\n    task: \"Generate Twitter thread from source content and tone profile. Follow Twitter-specific constraints (280 chars, 3-7 tweets, thread numbering, engaging hooks). Output to temp/twitter.json.\"\n    inputs: \"markdown_content, tone_profile, target_audience\"\n    outputs: \"temp/twitter.json (thread array, hashtags, character counts)\"\n\n  - name: \"linkedin-writer\"\n    task: \"Generate LinkedIn post from source content and tone profile. Follow LinkedIn constraints (1,200-3,000 chars, professional tone, line breaks, 3-5 hashtags). Output to temp/linkedin.json.\"\n    inputs: \"markdown_content, tone_profile, target_audience\"\n    outputs: \"temp/linkedin.json (post text, hashtags, character count)\"\n\n  - name: \"email-writer\"\n    task: \"Generate email newsletter section from source content and tone profile. Follow email constraints (300-500 words, markdown, personalization tokens, clear CTA). Output to temp/email.json.\"\n    inputs: \"markdown_content, tone_profile, target_audience\"\n    outputs: \"temp/email.json (section text, subject suggestion, preview text, tokens)\"\n\n  - name: \"instagram-writer\"\n    task: \"Generate Instagram caption from source content and tone profile. Follow Instagram constraints (125-150 words visible, 5-10 hashtags, emoji usage, link in bio CTA). Output to temp/instagram.json.\"\n    inputs: \"markdown_content, tone_profile, target_audience\"\n    outputs: \"temp/instagram.json (caption text, hashtags, emoji list, word count)\"\n\n# Sequential fallback:\nsequential_rationale: \"If Agent Teams is disabled or fails, execute the 4 platform generations sequentially using platform-writer-specialist. Results are identical, just slower (4x wall-clock time).\"\n```\n\n### GitHub Actions Triggers\n```yaml\ntriggers:\n  - type: \"workflow_dispatch\"\n    config: |\n      inputs:\n        blog_post_url:\n          description: 'Blog post URL to repurpose'\n          required: true\n        brand_voice_notes:\n          description: 'Optional brand voice guidelines'\n          required: false\n        target_audience:\n          description: 'Target audience persona'\n          required: false\n    description: \"Manual trigger for on-demand content repurposing. Primary execution path.\"\n\n  - type: \"repository_dispatch\"\n    config: |\n      event_type: repurpose_content\n      payload: { url, brand_voice, audience }\n    description: \"Webhook trigger for integration with CMS publish hooks or Zapier. Fires when new blog post is published.\"\n\n  - type: \"schedule\"\n    config: \"0 10 * * 1  # Weekly on Monday at 10:00 UTC\"\n    description: \"Optional scheduled run to batch-process queued URLs from a config file. Low priority.\"\n```\n\n---\n\n## Implementation Blueprint\n\n### Workflow Steps\nOrdered list of workflow phases:\n\n```yaml\nsteps:\n  - name: \"Scrape Blog Post\"\n    description: \"Extract full blog post content from provided URL using Firecrawl API with fallback to direct HTTP scraping.\"\n    subagent: \"content-scraper-specialist\"\n    tools: [\"scrape_content.py\"]\n    inputs: \"blog_post_url (from workflow dispatch or webhook)\"\n    outputs: \"markdown_content (string), metadata (dict with title, author, date)\"\n    failure_mode: \"URL unreachable, Firecrawl rate limit hit, content behind paywall, invalid URL format\"\n    fallback: \"Retry with exponential backoff (3 attempts). If Firecrawl fails, fall back to direct HTTP + BeautifulSoup. If both fail, log error and exit with non-zero code.\"\n\n  - name: \"Analyze Tone\"\n    description: \"Perform linguistic analysis on source content to extract tone profile (formality, technical level, emotion, key themes).\"\n    subagent: \"tone-analyzer-specialist\"\n    tools: [\"analyze_tone.py\"]\n    inputs: \"markdown_content (from scrape step), brand_voice_notes (optional)\"\n    outputs: \"tone_profile (dict with primary_tone, formality, technical_level, emotion, key_themes)\"\n    failure_mode: \"Content too short for analysis (<100 words), LLM API error, malformed content\"\n    fallback: \"If content too short, use default neutral tone profile. If LLM fails, retry 2x with exponential backoff. If still fails, use fallback tone profile (conversational, medium formality).\"\n\n  - name: \"Generate Platform Content (Parallel)\"\n    description: \"Generate four platform-specific content variants in parallel using Agent Teams. Each teammate operates independently.\"\n    subagent: \"platform-writer-specialist (delegated to 4 teammates)\"\n    tools: [\"generate_twitter.py\", \"generate_linkedin.py\", \"generate_email.py\", \"generate_instagram.py\"]\n    inputs: \"markdown_content, tone_profile, target_audience (shared by all teammates)\"\n    outputs: \"Four JSON files in temp/: twitter.json, linkedin.json, email.json, instagram.json\"\n    failure_mode: \"One or more platform generations fail due to LLM errors, timeout, or formatting issues\"\n    fallback: \"Team lead collects successful results and logs failed platforms. If 2+ platforms fail, abort and report. If 1 platform fails, continue with partial output and flag the missing platform in metadata.\"\n\n  - name: \"Merge and Format Output\"\n    description: \"Combine all platform content into unified JSON structure with metadata, validate schema, and write to timestamped output file.\"\n    subagent: \"output-formatter-specialist\"\n    tools: [\"merge_output.py\"]\n    inputs: \"temp/twitter.json, temp/linkedin.json, temp/email.json, temp/instagram.json, tone_profile, source_url\"\n    outputs: \"outputs/YYYY-MM-DD_HH-MM-SS_repurposed.json, commit_summary (string)\"\n    failure_mode: \"JSON schema validation fails, file write permission error, missing platform data\"\n    fallback: \"If validation fails, log the specific schema errors and write a partial output with a '.partial' extension. If write fails, output to stdout. Team lead creates a GitHub issue with error details.\"\n\n  - name: \"Commit Results\"\n    description: \"Stage the output JSON file and commit to repository with descriptive message.\"\n    subagent: \"none (main agent Git operations)\"\n    tools: [\"git CLI via Bash\"]\n    inputs: \"output_filepath, commit_summary\"\n    outputs: \"Git commit SHA\"\n    failure_mode: \"Git push fails due to network error or permission issue\"\n    fallback: \"Retry push 3 times with exponential backoff. If all retries fail, log the error and leave the file in the working directory for manual commit.\"\n```\n\n### Tool Specifications\nFor each tool the system needs:\n\n```yaml\ntools:\n  - name: \"scrape_content.py\"\n    purpose: \"Extract blog post content from URL via Firecrawl API with fallback to direct HTTP\"\n    catalog_pattern: \"firecrawl_scrape (from tool_catalog.md)\"\n    inputs:\n      - \"url: str — Blog post URL to scrape\"\n      - \"api_key: str — Firecrawl API key from env\"\n    outputs: \"JSON with {markdown: str, metadata: {title, author, date}}\"\n    dependencies: [\"firecrawl-py\", \"httpx\", \"beautifulsoup4\"]\n    mcp_used: \"firecrawl (via API, not MCP server)\"\n    error_handling: \"Retry 3x with exponential backoff on rate limits. Fall back to direct HTTP + BeautifulSoup if Firecrawl fails. Exit 1 if both fail.\"\n\n  - name: \"analyze_tone.py\"\n    purpose: \"Analyze source content tone and style using Claude with structured output\"\n    catalog_pattern: \"structured_extract (from tool_catalog.md)\"\n    inputs:\n      - \"markdown_content: str — Source blog post content\"\n      - \"brand_voice_notes: Optional[str] — Brand voice override\"\n    outputs: \"JSON tone_profile: {primary_tone, formality, technical_level, emotion, key_themes}\"\n    dependencies: [\"anthropic\"]\n    mcp_used: \"none (direct Anthropic API)\"\n    error_handling: \"Retry 2x on API errors. If retries fail, return default neutral tone profile. Never exit non-zero — tone analysis failure should not block content generation.\"\n\n  - name: \"generate_twitter.py\"\n    purpose: \"Generate Twitter thread with character limit enforcement and engagement optimization\"\n    catalog_pattern: \"llm_prompt (adapted for Twitter constraints)\"\n    inputs:\n      - \"markdown_content: str — Source content\"\n      - \"tone_profile: dict — Tone analysis results\"\n      - \"target_audience: Optional[str] — Audience persona\"\n    outputs: \"JSON: {thread: [str], hashtags: [str], mentions: [str], character_counts: [int]}\"\n    dependencies: [\"anthropic\"]\n    mcp_used: \"none (direct Anthropic API)\"\n    error_handling: \"Retry 2x on API errors. Validate each tweet is ≤280 chars. If validation fails, regenerate with stricter prompt. Exit 1 if regeneration fails after 2 attempts.\"\n\n  - name: \"generate_linkedin.py\"\n    purpose: \"Generate LinkedIn post with professional tone and optimal length (1,200-3,000 chars)\"\n    catalog_pattern: \"llm_prompt (adapted for LinkedIn constraints)\"\n    inputs:\n      - \"markdown_content: str — Source content\"\n      - \"tone_profile: dict — Tone analysis results\"\n      - \"target_audience: Optional[str] — Audience persona\"\n    outputs: \"JSON: {post: str, hashtags: [str], mentions: [str], character_count: int}\"\n    dependencies: [\"anthropic\"]\n    mcp_used: \"none (direct Anthropic API)\"\n    error_handling: \"Retry 2x on API errors. Validate post length 1,200-3,000 chars. If out of range, regenerate with adjusted prompt. Exit 1 if regeneration fails.\"\n\n  - name: \"generate_email.py\"\n    purpose: \"Generate email newsletter section with markdown formatting and personalization tokens\"\n    catalog_pattern: \"llm_prompt (adapted for email constraints)\"\n    inputs:\n      - \"markdown_content: str — Source content\"\n      - \"tone_profile: dict — Tone analysis results\"\n      - \"target_audience: Optional[str] — Audience persona\"\n    outputs: \"JSON: {section: str, subject_line_suggestion: str, preview_text: str, word_count: int, personalization_tokens: [str]}\"\n    dependencies: [\"anthropic\"]\n    mcp_used: \"none (direct Anthropic API)\"\n    error_handling: \"Retry 2x on API errors. Validate word count 300-500. Include default subject/preview if generation incomplete. Exit 1 if core section generation fails.\"\n\n  - name: \"generate_instagram.py\"\n    purpose: \"Generate Instagram caption with hashtag strategy and emoji usage\"\n    catalog_pattern: \"llm_prompt (adapted for Instagram constraints)\"\n    inputs:\n      - \"markdown_content: str — Source content\"\n      - \"tone_profile: dict — Tone analysis results\"\n      - \"target_audience: Optional[str] — Audience persona\"\n    outputs: \"JSON: {caption: str, hashtags: [str], emoji_usage: [str], word_count: int}\"\n    dependencies: [\"anthropic\"]\n    mcp_used: \"none (direct Anthropic API)\"\n    error_handling: \"Retry 2x on API errors. Validate 5-10 hashtags, emoji presence, word count 125-150 visible. Exit 1 if validation fails after retry.\"\n\n  - name: \"merge_output.py\"\n    purpose: \"Combine platform-specific content into unified JSON with metadata and validation\"\n    catalog_pattern: \"json_read_write (adapted for multi-source merge)\"\n    inputs:\n      - \"twitter_json: str — Path to temp/twitter.json\"\n      - \"linkedin_json: str — Path to temp/linkedin.json\"\n      - \"email_json: str — Path to temp/email.json\"\n      - \"instagram_json: str — Path to temp/instagram.json\"\n      - \"tone_profile: dict — Tone analysis results\"\n      - \"source_url: str — Original blog post URL\"\n    outputs: \"JSON file at outputs/YYYY-MM-DD_HH-MM-SS_repurposed.json\"\n    dependencies: [\"jsonschema\"]\n    mcp_used: \"none\"\n    error_handling: \"Validate JSON schema. If validation fails, write partial output with .partial extension and log errors. Exit 1 if fewer than 2 platforms succeeded.\"\n```\n\n### Per-Tool Pseudocode\n```python\n# scrape_content.py\ndef main():\n    # PATTERN: firecrawl_scrape with HTTP fallback\n    args = parse_args()  # url, api_key from env\n    \n    # Step 1: Try Firecrawl API\n    # GOTCHA: Rate limit 500/min on paid, 10/min on free\n    try:\n        result = firecrawl_scrape(args.url, args.api_key)\n        return {\"markdown\": result[\"markdown\"], \"metadata\": result.get(\"metadata\", {})}\n    except RateLimitError:\n        logging.warning(\"Firecrawl rate limit hit, retrying with backoff\")\n        retry_with_backoff()\n    except Exception as e:\n        logging.error(f\"Firecrawl failed: {e}\")\n    \n    # Step 2: Fallback to direct HTTP + BeautifulSoup\n    # CRITICAL: Won't handle JS-rendered content\n    response = requests.get(args.url)\n    soup = BeautifulSoup(response.text, \"html.parser\")\n    markdown = html_to_markdown(soup)\n    \n    # Step 3: Output\n    print(json.dumps({\"markdown\": markdown, \"metadata\": extract_metadata(soup)}))\n\n# analyze_tone.py\ndef main():\n    # PATTERN: structured_extract\n    # CRITICAL: Never exit non-zero on tone analysis failure\n    args = parse_args()  # markdown_content, brand_voice_notes\n    \n    schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"primary_tone\": {\"type\": \"string\"},\n            \"formality\": {\"type\": \"string\"},\n            \"technical_level\": {\"type\": \"string\"},\n            \"emotion\": {\"type\": \"string\"},\n            \"key_themes\": {\"type\": \"array\"}\n        }\n    }\n    \n    try:\n        result = structured_extract(args.markdown_content, schema, \n                                     instructions=f\"Analyze tone. {args.brand_voice_notes or ''}\")\n        print(json.dumps(result))\n    except Exception as e:\n        logging.error(f\"Tone analysis failed: {e}\")\n        # Fallback to default neutral profile\n        print(json.dumps(DEFAULT_TONE_PROFILE))\n\n# generate_twitter.py\ndef main():\n    # PATTERN: llm_prompt with Twitter constraints\n    # GOTCHA: URLs count as 23 chars regardless of length\n    args = parse_args()\n    \n    prompt = f\"\"\"Generate a Twitter thread from this content:\n    \n{args.markdown_content}\n\nTone profile: {json.dumps(args.tone_profile)}\nTarget audience: {args.target_audience or 'general'}\n\nRequirements:\n- 3-7 tweets\n- 280 characters max per tweet (URLs count as 23 chars)\n- Number tweets (1/N, 2/N, etc.)\n- First tweet must hook attention\n- Last tweet includes link to source\n- Suggest 3-5 hashtags\n- Include @mention placeholders if relevant\n\nReturn JSON with structure: {{thread: [str], hashtags: [str], mentions: [str]}}\n\"\"\"\n    \n    result = llm_prompt(prompt, provider=\"anthropic\", temperature=0.7)\n    validated = validate_twitter_thread(result)  # Check char limits\n    print(json.dumps(validated))\n\n# generate_linkedin.py, generate_email.py, generate_instagram.py follow similar pattern\n# with platform-specific constraints and validation\n\n# merge_output.py\ndef main():\n    # PATTERN: json_read_write with schema validation\n    # Load all platform JSONs\n    twitter = json.load(open(\"temp/twitter.json\"))\n    linkedin = json.load(open(\"temp/linkedin.json\"))\n    email = json.load(open(\"temp/email.json\"))\n    instagram = json.load(open(\"temp/instagram.json\"))\n    \n    # Combine with metadata\n    output = {\n        \"metadata\": {\n            \"source_url\": args.source_url,\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n            \"tone_profile\": args.tone_profile,\n            \"word_count_source\": count_words(args.markdown_content)\n        },\n        \"twitter\": twitter,\n        \"linkedin\": linkedin,\n        \"email_newsletter\": email,\n        \"instagram\": instagram\n    }\n    \n    # Validate schema\n    validate(output, OUTPUT_SCHEMA)\n    \n    # Write to timestamped file\n    filename = f\"outputs/{datetime.utcnow().strftime('%Y-%m-%d_%H-%M-%S')}_repurposed.json\"\n    with open(filename, \"w\") as f:\n        json.dump(output, f, indent=2, ensure_ascii=False)\n    \n    print(json.dumps({\"output_file\": filename, \"summary\": generate_summary(output)}))\n```\n\n### Integration Points\n```yaml\nSECRETS:\n  - name: \"FIRECRAWL_API_KEY\"\n    purpose: \"Authenticate with Firecrawl web scraping API\"\n    required: true\n\n  - name: \"ANTHROPIC_API_KEY\"\n    purpose: \"Authenticate with Claude API for tone analysis and content generation\"\n    required: true\n\nENVIRONMENT:\n  - file: \".env.example\"\n    vars:\n      - \"FIRECRAWL_API_KEY=fc_your_key_here  # Get from firecrawl.dev\"\n      - \"ANTHROPIC_API_KEY=sk-ant-your_key_here  # Get from console.anthropic.com\"\n\nDEPENDENCIES:\n  - file: \"requirements.txt\"\n    packages:\n      - \"firecrawl-py==1.0.0  # Web scraping SDK\"\n      - \"anthropic==0.39.0  # Claude API client\"\n      - \"httpx==0.28.1  # HTTP client with retry logic\"\n      - \"beautifulsoup4==4.12.3  # HTML parsing fallback\"\n      - \"jsonschema==4.23.0  # Output validation\"\n      - \"python-dateutil==2.9.0  # Timestamp formatting\"\n\nGITHUB_ACTIONS:\n  - trigger: \"workflow_dispatch\"\n    config: |\n      name: Repurpose Content\n      on:\n        workflow_dispatch:\n          inputs:\n            blog_post_url:\n              description: 'Blog post URL'\n              required: true\n            brand_voice_notes:\n              description: 'Brand voice guidelines (optional)'\n              required: false\n            target_audience:\n              description: 'Target audience (optional)'\n              required: false\n      jobs:\n        repurpose:\n          runs-on: ubuntu-latest\n          timeout-minutes: 15\n          steps:\n            - uses: actions/checkout@v4\n            - uses: actions/setup-python@v5\n              with:\n                python-version: '3.12'\n            - run: pip install -r requirements.txt\n            - run: claude -p \"Read workflow.md and execute the content repurposing workflow\"\n              env:\n                FIRECRAWL_API_KEY: ${{ secrets.FIRECRAWL_API_KEY }}\n                ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\n                BLOG_POST_URL: ${{ inputs.blog_post_url }}\n                BRAND_VOICE_NOTES: ${{ inputs.brand_voice_notes }}\n                TARGET_AUDIENCE: ${{ inputs.target_audience }}\n            - run: git config user.name \"github-actions[bot]\" && git config user.email \"github-actions[bot]@users.noreply.github.com\"\n            - run: git add outputs/*.json && git commit -m \"Add repurposed content for ${{ inputs.blog_post_url }}\" && git push\n```\n\n---\n\n## Validation Loop\n\n### Level 1: Syntax & Structure\n```bash\n# Run FIRST — every tool must pass before proceeding to Level 2\n\n# AST parse — verify valid Python syntax\npython -c \"import ast; ast.parse(open('tools/scrape_content.py').read())\"\npython -c \"import ast; ast.parse(open('tools/analyze_tone.py').read())\"\npython -c \"import ast; ast.parse(open('tools/generate_twitter.py').read())\"\npython -c \"import ast; ast.parse(open('tools/generate_linkedin.py').read())\"\npython -c \"import ast; ast.parse(open('tools/generate_email.py').read())\"\npython -c \"import ast; ast.parse(open('tools/generate_instagram.py').read())\"\npython -c \"import ast; ast.parse(open('tools/merge_output.py').read())\"\n\n# Import check — verify no missing dependencies\npython -c \"import importlib; importlib.import_module('tools.scrape_content')\"\npython -c \"import importlib; importlib.import_module('tools.analyze_tone')\"\n# ... repeat for all tools\n\n# Structure check — verify main() exists\npython -c \"from tools.scrape_content import main; assert callable(main)\"\n# ... repeat for all tools\n\n# Subagent validation\npython -c \"import yaml; yaml.safe_load(open('.claude/agents/content-scraper-specialist.md').read().split('---')[1])\"\npython -c \"import yaml; yaml.safe_load(open('.claude/agents/tone-analyzer-specialist.md').read().split('---')[1])\"\npython -c \"import yaml; yaml.safe_load(open('.claude/agents/platform-writer-specialist.md').read().split('---')[1])\"\npython -c \"import yaml; yaml.safe_load(open('.claude/agents/output-formatter-specialist.md').read().split('---')[1])\"\n\n# Expected: All pass with no errors. If any fail, fix before proceeding.\n```\n\n### Level 2: Unit Tests\n```bash\n# Run SECOND — each tool must produce correct output for sample inputs\n\n# Test scraper with known URL\npython tools/scrape_content.py --url \"https://example.com/blog/sample-post\" > /tmp/scraped.json\npython -c \"import json; data = json.load(open('/tmp/scraped.json')); assert 'markdown' in data; assert len(data['markdown']) > 100\"\n\n# Test tone analyzer with sample content\necho '{\"markdown_content\": \"This is a friendly, conversational blog post about AI agents. We believe automation should be accessible to everyone!\"}' | python tools/analyze_tone.py > /tmp/tone.json\npython -c \"import json; tone = json.load(open('/tmp/tone.json')); assert 'primary_tone' in tone; assert tone['primary_tone'] in ['conversational', 'casual', 'friendly']\"\n\n# Test Twitter generator with sample\necho '{\"markdown_content\": \"Sample blog post content here...\", \"tone_profile\": {\"primary_tone\": \"conversational\", \"formality\": \"medium\"}}' | python tools/generate_twitter.py > /tmp/twitter.json\npython -c \"import json; t = json.load(open('/tmp/twitter.json')); assert 'thread' in t; assert len(t['thread']) >= 3; assert all(len(tweet) <= 280 for tweet in t['thread'])\"\n\n# Test LinkedIn generator\necho '{\"markdown_content\": \"Sample content...\", \"tone_profile\": {\"primary_tone\": \"professional\", \"formality\": \"high\"}}' | python tools/generate_linkedin.py > /tmp/linkedin.json\npython -c \"import json; l = json.load(open('/tmp/linkedin.json')); assert 'post' in l; assert 1200 <= len(l['post']) <= 3000\"\n\n# Test email generator\necho '{\"markdown_content\": \"Sample content...\", \"tone_profile\": {\"primary_tone\": \"conversational\"}}' | python tools/generate_email.py > /tmp/email.json\npython -c \"import json; e = json.load(open('/tmp/email.json')); assert 'section' in e; assert '{{first_name}}' in e['section']\"\n\n# Test Instagram generator\necho '{\"markdown_content\": \"Sample content...\", \"tone_profile\": {\"primary_tone\": \"casual\"}}' | python tools/generate_instagram.py > /tmp/instagram.json\npython -c \"import json; i = json.load(open('/tmp/instagram.json')); assert 'caption' in i; assert len(i['hashtags']) >= 5\"\n\n# Test merge tool\npython tools/merge_output.py --twitter /tmp/twitter.json --linkedin /tmp/linkedin.json --email /tmp/email.json --instagram /tmp/instagram.json --source-url \"https://example.com/blog/post\" > /tmp/merged.json\npython -c \"import json; m = json.load(open('/tmp/merged.json')); assert 'metadata' in m; assert 'twitter' in m; assert 'linkedin' in m; assert 'email_newsletter' in m; assert 'instagram' in m\"\n\n# Expected: All tests pass. If any fail, read error, fix code, re-run.\n```\n\n### Level 3: Integration Tests\n```bash\n# Run THIRD — verify tools work together as a pipeline\n\n# Full end-to-end test with real URL (using a known stable blog post)\nexport TEST_URL=\"https://example.com/blog/known-stable-post\"\nexport FIRECRAWL_API_KEY=\"test_key\"\nexport ANTHROPIC_API_KEY=\"test_key\"\n\n# Step 1: Scrape\npython tools/scrape_content.py --url \"$TEST_URL\" > /tmp/int_scraped.json\n\n# Step 2: Analyze tone\npython tools/analyze_tone.py --markdown-content \"$(cat /tmp/int_scraped.json | jq -r '.markdown')\" > /tmp/int_tone.json\n\n# Step 3: Generate all platforms (sequential for testing — parallel happens in production)\nMARKDOWN=$(cat /tmp/int_scraped.json | jq -r '.markdown')\nTONE=$(cat /tmp/int_tone.json)\n\necho \"{\\\"markdown_content\\\": \\\"$MARKDOWN\\\", \\\"tone_profile\\\": $TONE}\" | python tools/generate_twitter.py > /tmp/int_twitter.json\necho \"{\\\"markdown_content\\\": \\\"$MARKDOWN\\\", \\\"tone_profile\\\": $TONE}\" | python tools/generate_linkedin.py > /tmp/int_linkedin.json\necho \"{\\\"markdown_content\\\": \\\"$MARKDOWN\\\", \\\"tone_profile\\\": $TONE}\" | python tools/generate_email.py > /tmp/int_email.json\necho \"{\\\"markdown_content\\\": \\\"$MARKDOWN\\\", \\\"tone_profile\\\": $TONE}\" | python tools/generate_instagram.py > /tmp/int_instagram.json\n\n# Step 4: Merge\npython tools/merge_output.py \\\n  --twitter /tmp/int_twitter.json \\\n  --linkedin /tmp/int_linkedin.json \\\n  --email /tmp/int_email.json \\\n  --instagram /tmp/int_instagram.json \\\n  --tone-profile /tmp/int_tone.json \\\n  --source-url \"$TEST_URL\" > /tmp/final_output.json\n\n# Step 5: Validate final output\npython -c \"\nimport json, sys\nresult = json.load(open('/tmp/final_output.json'))\n\n# Check structure\nassert 'metadata' in result, 'Missing metadata'\nassert 'twitter' in result, 'Missing twitter'\nassert 'linkedin' in result, 'Missing linkedin'\nassert 'email_newsletter' in result, 'Missing email'\nassert 'instagram' in result, 'Missing instagram'\n\n# Check metadata\nassert result['metadata']['source_url'] == '$TEST_URL', 'Wrong source URL'\nassert 'timestamp' in result['metadata'], 'Missing timestamp'\nassert 'tone_profile' in result['metadata'], 'Missing tone profile'\n\n# Check Twitter thread\nassert len(result['twitter']['thread']) >= 3, 'Twitter thread too short'\nassert all(len(t) <= 280 for t in result['twitter']['thread']), 'Twitter tweet over 280 chars'\n\n# Check LinkedIn post\nassert 1200 <= len(result['linkedin']['post']) <= 3000, 'LinkedIn post wrong length'\n\n# Check email\nassert 300 <= result['email_newsletter']['word_count'] <= 500, 'Email word count wrong'\n\n# Check Instagram\nassert 125 <= result['instagram']['word_count'] <= 150, 'Instagram word count wrong'\nassert len(result['instagram']['hashtags']) >= 5, 'Instagram needs more hashtags'\n\nprint('Integration test passed!')\n\"\n\n# Verify workflow.md references match actual tools\ngrep -E 'scrape_content|analyze_tone|generate_twitter|generate_linkedin|generate_email|generate_instagram|merge_output' workflow.md\n\n# Verify CLAUDE.md documents all subagents\ngrep -E 'content-scraper-specialist|tone-analyzer-specialist|platform-writer-specialist|output-formatter-specialist' CLAUDE.md\n\n# Verify GitHub Actions YAML is valid\npython -c \"import yaml; yaml.safe_load(open('.github/workflows/repurpose.yml'))\"\n\n# Expected: All checks pass. Integration test produces valid final JSON.\n```\n\n---\n\n## Final Validation Checklist\n- [ ] All tools pass Level 1 (syntax, imports, structure)\n- [ ] All tools pass Level 2 (unit tests with sample data)\n- [ ] Pipeline passes Level 3 (integration test end-to-end)\n- [ ] workflow.md has failure modes and fallbacks for every step\n- [ ] CLAUDE.md documents all tools, subagents, MCPs, and secrets\n- [ ] .github/workflows/repurpose.yml has timeout-minutes (15) and failure notifications\n- [ ] .env.example lists FIRECRAWL_API_KEY and ANTHROPIC_API_KEY\n- [ ] .gitignore excludes .env, __pycache__/, temp/, credentials\n- [ ] README.md covers all three execution paths (CLI, Actions, Agent HQ)\n- [ ] No hardcoded secrets anywhere in the codebase\n- [ ] Subagent files (.claude/agents/*.md) have valid YAML frontmatter and specific system prompts\n- [ ] requirements.txt lists firecrawl-py, anthropic, httpx, beautifulsoup4, jsonschema, python-dateutil\n- [ ] Agent Teams setup documented in CLAUDE.md with token cost notes (4x parallel = same cost, faster time)\n- [ ] Sequential fallback documented for Agent Teams failure\n\n---\n\n## Anti-Patterns to Avoid\n- Do not hardcode FIRECRAWL_API_KEY or ANTHROPIC_API_KEY — use GitHub Secrets or .env\n- Do not use `git add -A` or `git add .` — stage only outputs/*.json files\n- Do not skip Level 1 validation — syntax errors caught early save hours of debugging\n- Do not catch bare `except:` — catch specific exceptions (RateLimitError, JSONDecodeError, etc.)\n- Do not build tools that require interactive input — all inputs via CLI args or env vars\n- Do not depend solely on Firecrawl MCP without HTTP fallback — rate limits are real\n- Do not use Agent Teams for fewer than 3 tasks — overhead not justified\n- Do not ignore character limits — validate Twitter (280), LinkedIn (3000), Instagram (2200)\n- Do not commit temp/ directory to Git — add to .gitignore\n- Do not ignore failing platform generations — log and report partial failures\n- Do not skip tone analysis to save tokens — tone matching is a core feature\n- Do not generate content without including source URL attribution\n\n---\n\n## Confidence Score: 9/10\n\n**Score rationale:**\n- [Workflow clarity]: Very high confidence. The scrape > analyze > generate > merge pipeline is well-defined with clear inputs/outputs at each stage. Confidence: **high**\n- [Technical feasibility]: Very high confidence. All required APIs (Firecrawl, Anthropic) are well-documented and widely used. Character limit enforcement is straightforward validation. Confidence: **high**\n- [Agent Teams suitability]: Very high confidence. 4 independent content generation tasks with identical inputs and no dependencies is the ideal Agent Teams use case. 4x speedup with no complexity trade-off. Confidence: **high**\n- [MCP/fallback strategy]: High confidence. Firecrawl is primary with direct HTTP fallback. Anthropic API is stable. No single point of failure. Confidence: **high**\n- [Validation strategy]: High confidence. Three-level validation with specific checks per level. Integration test covers the full pipeline. Confidence: **high**\n- [Edge cases handled]: Medium-high confidence. Rate limiting, content extraction failures, and partial generation failures all have documented fallbacks. Minor uncertainty: how to handle non-English content (not specified). Confidence: **medium-high**\n\n**Ambiguity flags** (areas requiring clarification before building):\n- [ ] None — all requirements are clear and actionable.\n\n**Minor enhancement opportunities** (not blockers):\n- Could add support for custom platform templates (e.g., user-provided tweet format)\n- Could add analytics tracking (character count distribution, hashtag performance)\n- Could add content preview/approval step before commit (GitHub Issue workflow)\n- Could add multi-language support (detect source language, generate in same language)\n\n---\n\n## Factory Build Prompt\n\nWhen this PRP is ready, execute the build with:\n\n```\n/execute-prp PRPs/content-repurposer.md\n```\n\nThis will trigger `factory/workflow.md` with the above requirements as structured input.\n"}}],"api":"anthropic-messages","provider":"anthropic","model":"claude-sonnet-4-5-20250929","usage":{"input":13,"output":13138,"cacheRead":52663,"cacheWrite":0,"totalTokens":65814,"cost":{"input":0.000039,"output":0.19707,"cacheRead":0.015798899999999998,"cacheWrite":0,"total":0.2129079}},"stopReason":"toolUse","timestamp":1770816080733}}
